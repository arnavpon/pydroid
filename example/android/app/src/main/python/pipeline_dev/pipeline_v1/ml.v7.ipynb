{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.signal import resample\n",
    "from sdtw import SoftDTW\n",
    "from sdtw.distance import SquaredEuclidean\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [18, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from truth import IeeeGroundTruth\n",
    "from wavelet import apply_wavelet\n",
    "from peaks import get_peaks_v2\n",
    "from signal_pross import (\n",
    "    normalize_signal,\n",
    "    detrend_w_poly,\n",
    "    normalize_amplitude_to_1,\n",
    "    n_moving_avg,\n",
    "    min_max_scale,\n",
    "    bandpass,\n",
    "    get_hr,\n",
    "    get_hrv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFactory:\n",
    "\n",
    "    def __init__(self, split_size, loss_type = 'mse', gamma = 1.0, mse_weight = None, dtw_weight = None):\n",
    "        \n",
    "        if loss_type not in ['mse', 'dtw', 'combined']:\n",
    "            raise ValueError(f'Loss type [{loss_type}] not supported')\n",
    "        \n",
    "        self.split_size = split_size\n",
    "        self.gamma = gamma\n",
    "        self.mse_weight = mse_weight\n",
    "        self.dtw_weight = dtw_weight\n",
    "\n",
    "        if loss_type == 'mse':\n",
    "            self.loss_function = self.mse_loss\n",
    "        elif loss_type == 'dtw':\n",
    "            self.loss_function = self.soft_dtw_loss\n",
    "        elif loss_type == 'combined':\n",
    "            self.loss_function = self.combined_loss\n",
    "        \n",
    "    def __call__(self, y_pred, data):\n",
    "        return self.loss_function(y_pred, data)\n",
    "\n",
    "    def get_func(self):\n",
    "        return self.loss_function\n",
    "\n",
    "    def mse_loss(self, y_pred, data):\n",
    "        \n",
    "        y_true = data.get_label()\n",
    "        num_batches = int(len(y_pred) / self.split_size)\n",
    "        errs = np.zeros_like(y_true)\n",
    "\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            y_true_curr = y_true[i * self.split_size: (i + 1) * self.split_size]\n",
    "            y_pred_curr = y_pred[i * self.split_size: (i + 1) * self.split_size]\n",
    "            \n",
    "            err = y_true_curr - y_pred_curr\n",
    "            errs[i * self.split_size: (i + 1) * self.split_size] = err\n",
    "\n",
    "        grad = -2 * errs\n",
    "        hess = 2 * np.ones_like(y_true)\n",
    "        return grad, hess\n",
    "\n",
    "    # def soft_dtw_loss(self, y_pred, data):\n",
    "\n",
    "    #     y_true = data.get_label()\n",
    "    #     num_batches = int(len(y_pred) / self.split_size)\n",
    "    #     grads = np.zeros_like(y_true)\n",
    "    #     hesses = np.zeros_like(y_true)\n",
    "\n",
    "    #     for i in range(num_batches):\n",
    "\n",
    "    #         y_true_curr = y_true[i * self.split_size: (i + 1) * self.split_size]\n",
    "    #         y_pred_curr = y_pred[i * self.split_size: (i + 1) * self.split_size]\n",
    "            \n",
    "    #         grad_curr, hess_curr = self.soft_dtw_loss_helper(y_true_curr, y_pred_curr)\n",
    "    #         grad_curr = grad_curr.flatten()\n",
    "    #         hess_curr = hess_curr.flatten()\n",
    "\n",
    "    #         grads[i * self.split_size: (i + 1) * self.split_size] = grad_curr\n",
    "    #         hesses[i * self.split_size: (i + 1) * self.split_size] = hess_curr\n",
    "\n",
    "    #     return grads, hesses\n",
    "    def soft_dtw_loss(self, y_pred, data):\n",
    "\n",
    "        def batch_loss_helper(i, y_true, y_pred, split_size):\n",
    "            \n",
    "            y_true_curr = y_true[i * split_size: (i + 1) * split_size]\n",
    "            y_pred_curr = y_pred[i * split_size: (i + 1) * split_size]\n",
    "\n",
    "            grad_curr, hess_curr = self.soft_dtw_loss_helper(y_true_curr, y_pred_curr)\n",
    "            grad_curr = grad_curr.flatten()\n",
    "            hess_curr = hess_curr.flatten()\n",
    "\n",
    "            return grad_curr, hess_curr\n",
    "\n",
    "        y_true = data.get_label()\n",
    "        num_batches = int(len(y_pred) / self.split_size)\n",
    "        grads = np.zeros_like(y_true)\n",
    "        hesses = np.zeros_like(y_true)\n",
    "\n",
    "        results = Parallel(n_jobs = -1)(\n",
    "            delayed(batch_loss_helper)(\n",
    "                i, y_true, y_pred, self.split_size\n",
    "            ) for i in range(num_batches)\n",
    "        )\n",
    "\n",
    "        for i, (grad_curr, hess_curr) in enumerate(results):\n",
    "            grads[i * self.split_size: (i + 1) * self.split_size] = grad_curr\n",
    "            hesses[i * self.split_size: (i + 1) * self.split_size] = hess_curr\n",
    "\n",
    "        return grads, hesses\n",
    "    \n",
    "    def soft_dtw_loss_helper(self, y_true, y_pred):\n",
    "        x = y_true.reshape(-1, 1)\n",
    "        y = y_pred.reshape(-1, 1)\n",
    "        D = SquaredEuclidean(x, y)\n",
    "        sdtw = SoftDTW(D, gamma = self.gamma)\n",
    "        sdtw.compute()\n",
    "        E = sdtw.grad()\n",
    "        G = D.jacobian_product(E)\n",
    "        return G, np.ones(len(G))\n",
    "    \n",
    "    def combined_loss(self, y_pred, data):\n",
    "\n",
    "        if self.mse_weight is None or self.dtw_weight is None:\n",
    "            raise ValueError('mse_weight and dtw_weight must be set before calling combined_loss')\n",
    "\n",
    "        mse_grads, mse_hesses = self.mse_loss(y_pred, data)\n",
    "        dtw_grads, dtw_hesses = self.soft_dtw_loss(y_pred, data)\n",
    "\n",
    "        combined_grad = self.mse_weight * mse_grads + self.dtw_weight * dtw_grads\n",
    "        combined_hess = self.mse_weight * mse_hesses + self.dtw_weight * dtw_hesses\n",
    "\n",
    "        return combined_grad, combined_hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "class LonePineGBM:\n",
    "    \n",
    "    def __init__(self, truths, label_col = 'bvp', subject_col = 'subject',\n",
    "                # model customization\n",
    "                model_type = 'gbdt', random_state = None, loss_type = 'mse', excluded_subject = None,\n",
    "                # hyperparameters\n",
    "                n_estimators = 100, split_size = 1280, learning_rate = 0.1, test_size = 0.3, early_stopping_rounds = 50,\n",
    "                mse_weight = None, dtw_weight = None, data_beg = 1000, data_end = 10000, batches = 1, finetune = True,\n",
    "                min_bandpass_freq = 0.67, max_bandpass_freq = 3.0, bandpass_order = 4,\n",
    "                predicted_peaks_prominence = 0.22, true_peaks_prominence = 0.15,\n",
    "                # hyperparams from LightGBM docs\n",
    "                max_depth = 7, num_leaves = 75, max_bin = 255,\n",
    "                num_feats_per_channel = 3, skip_amount = 15):\n",
    "        \n",
    "        if model_type not in ['gbdt', 'rf']:\n",
    "            raise ValueError(f'Model type [{model_type}] not supported')\n",
    "        \n",
    "        self.label_col = label_col\n",
    "        self.subject_col = subject_col\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.random_state = random_state\n",
    "        self.excluded_subject = excluded_subject\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.split_size = split_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.test_size = test_size\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self.data_beg = data_beg\n",
    "        self.data_end = data_end\n",
    "        self.finetune = finetune\n",
    "\n",
    "        self.min_bandpass_freq = min_bandpass_freq\n",
    "        self.max_bandpass_freq = max_bandpass_freq\n",
    "        self.bandpass_order = bandpass_order\n",
    "        self.predicted_peaks_prominence = predicted_peaks_prominence\n",
    "        self.true_peaks_prominence = true_peaks_prominence\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.num_leaves = num_leaves\n",
    "        self.max_bin = max_bin\n",
    "\n",
    "        self.num_feats_per_channel = num_feats_per_channel\n",
    "        self.skip_amount = skip_amount\n",
    "\n",
    "        self.gbm = None\n",
    "        self.training_loss = None\n",
    "        self.test_loss = None\n",
    "\n",
    "        self.given_data = self.prepare_dataset_from_subjects(truths, data_beg = data_beg, data_end = data_end)\n",
    "        self.features = list(self.given_data.drop(columns = [self.label_col, self.subject_col]).columns)\n",
    "        if self.excluded_subject is not None:\n",
    "            self.given_data = self.given_data[self.given_data[self.subject_col] != self.excluded_subject]\n",
    "\n",
    "        if self.random_state is not None:\n",
    "            random.seed(self.random_state)\n",
    "        splits = self.split_data()\n",
    "        self.train_split_indices = random.sample(range(len(splits)), int(len(splits) * (1 - self.test_size)))\n",
    "        self.train_splits = [splits[i] for i in self.train_split_indices]\n",
    "        self.test_splits = [splits[i] for i in range(len(splits)) if i not in self.train_split_indices]\n",
    "        \n",
    "        self.train_data = []\n",
    "        self.train_data_just_data = []\n",
    "        batch_size = len(self.train_splits) // batches\n",
    "        print(f'Rows per batch: {batch_size * self.split_size}')\n",
    "        for batch_num in range(batches):\n",
    "            batch_split_idxs = random.sample(range(len(self.train_splits)), batch_size)\n",
    "            batch_splits = [self.train_splits[i] for i in batch_split_idxs]\n",
    "            self.train_splits = [self.train_splits[i] for i in range(len(self.train_splits)) if i not in batch_split_idxs]\n",
    "\n",
    "            train_indices = [idx for split in batch_splits for idx in split]\n",
    "            training_rows = self.given_data.iloc[train_indices].drop(columns = [self.subject_col])\n",
    "            train_X = training_rows.drop(columns = [self.label_col]).to_numpy()\n",
    "            train_y = training_rows[self.label_col].to_numpy()\n",
    "\n",
    "            # batch_data = lgb.Dataset(train_X, train_y, free_raw_data = False)\n",
    "            batch_data = xgb.DMatrix(train_X, train_y)\n",
    "            self.train_data.append(batch_data)\n",
    "            self.train_data_just_data.append(train_X)\n",
    "\n",
    "        test_indices = [idx for split in self.test_splits for idx in split]\n",
    "        test_rows = self.given_data.iloc[test_indices].drop(columns = [self.subject_col])\n",
    "        test_X = test_rows.drop(columns = [self.label_col]).to_numpy()\n",
    "        test_y = test_rows[self.label_col].to_numpy()\n",
    "        # self.test_data = lgb.Dataset(test_X, test_y, free_raw_data = False)\n",
    "        self.test_data = xgb.DMatrix(test_X, test_y)\n",
    "        self.test_X = test_X\n",
    "        self.test_y = test_y\n",
    "\n",
    "        self.loss = LossFactory(self.split_size, loss_type = loss_type, mse_weight = mse_weight, dtw_weight = dtw_weight)\n",
    "    \n",
    "    def split_data(self, to_exclude = None):\n",
    "        \n",
    "        data_in_use = self.given_data if to_exclude is None else self.given_data[~self.given_data[self.subject_col].isin(to_exclude)]\n",
    "\n",
    "        subject_indices = data_in_use.groupby(self.subject_col).indices\n",
    "        splits = []\n",
    "        for _, indices in subject_indices.items():\n",
    "            \n",
    "            n_splits = len(indices) // self.split_size\n",
    "            if n_splits > 0:\n",
    "\n",
    "                subject_splits = []\n",
    "                for i in range(n_splits):\n",
    "                    split_start = i * self.split_size\n",
    "                    split_end = (i + 1) * self.split_size\n",
    "                    subject_split = indices[split_start: split_end]\n",
    "                    subject_splits.append(subject_split)\n",
    "                \n",
    "                splits.extend(subject_splits)\n",
    "        \n",
    "        return splits\n",
    "\n",
    "    def fit(self):\n",
    "        t1 = datetime.today()\n",
    "        \n",
    "        self.params = {\n",
    "            'metric': 'None',\n",
    "            'verbosity': -1,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'objective': 'regression',\n",
    "            'boosting': self.model_type,\n",
    "            'max_depth': self.max_depth,\n",
    "            'num_leaves': self.num_leaves,\n",
    "            'max_bin': self.max_bin,\n",
    "        }\n",
    "    \n",
    "        if self.model_type == 'rf':\n",
    "            self.params['bagging_freq'] = 1\n",
    "            self.params['bagging_fraction'] = 0.8\n",
    "\n",
    "\n",
    "        training_loss_key = 'hr_err'\n",
    "        feval = self.hr_error_eval_metric\n",
    "        print('loss is loss')\n",
    "        \n",
    "        training_meta = {}\n",
    "\n",
    "        for train_data in self.train_data:\n",
    "            \n",
    "            if self.model_type == 'gbdt':\n",
    "                self.gbm = lgb.train(\n",
    "                    self.params,\n",
    "                    train_data,\n",
    "                    valid_sets = [train_data, self.test_data],\n",
    "                    valid_names=['train', 'test'],\n",
    "                    fobj = self.loss,\n",
    "                    num_boost_round = self.n_estimators,\n",
    "                    feval=feval,\n",
    "                    callbacks=[\n",
    "                        early_stopping(stopping_rounds = self.early_stopping_rounds),\n",
    "                        log_evaluation(period=5)\n",
    "                    ],\n",
    "                    evals_result = training_meta,\n",
    "                    init_model = self.gbm\n",
    "                )\n",
    "            else:\n",
    "                self.gbm = lgb.train(\n",
    "                    self.params,\n",
    "                    train_data,\n",
    "                    valid_sets = [train_data, self.test_data],\n",
    "                    valid_names=['train', 'test'],\n",
    "                    num_boost_round = self.n_estimators,\n",
    "                    feval=feval,\n",
    "                    callbacks=[\n",
    "                        early_stopping(stopping_rounds = self.early_stopping_rounds),\n",
    "                        log_evaluation(period=5)\n",
    "                    ],\n",
    "                    evals_result = training_meta,\n",
    "                )\n",
    "\n",
    "            mse, hr_err, hr_err_sq = self.eval()\n",
    "            print(f'Before fine-tuning: MSE = {mse}, HR error = {hr_err}, HR error (squared) = {hr_err_sq}')\n",
    "\n",
    "            if self.model_type == 'gbdt' and self.finetune:\n",
    "                \n",
    "                print('\\n\\nFine-tuning...')\n",
    "                gbm_copy = copy.deepcopy(self.gbm)\n",
    "                pred = gbm_copy.predict(train_data.get_data())\n",
    "                \n",
    "                # new_targ = train_data.get_label() - pred\n",
    "                new_targ = np.ones(len(pred))\n",
    "                nsplits = len(pred) // self.split_size\n",
    "                labels = train_data.get_label()\n",
    "                for i in range(nsplits):\n",
    "                    pred_curr = pred[i * self.split_size: (i + 1) * self.split_size]\n",
    "                    label_curr = labels[i * self.split_size: (i + 1) * self.split_size]\n",
    "                    hr_err = self.get_hr_error(pred_curr, label_curr, square = True)\n",
    "                    new_targ[i * self.split_size: (i + 1) * self.split_size] = hr_err\n",
    "                \n",
    "                new_train_data = lgb.Dataset(train_data.get_data(), label = new_targ)\n",
    "\n",
    "                self.gbm = lgb.train(\n",
    "                    self.params,\n",
    "                    new_train_data,\n",
    "                    valid_sets = [new_train_data, self.test_data],\n",
    "                    valid_names=['train', 'test'],\n",
    "                    fobj = self.loss,\n",
    "                    num_boost_round = self.n_estimators // 2,\n",
    "                    feval=feval,\n",
    "                    callbacks=[\n",
    "                        early_stopping(stopping_rounds = self.early_stopping_rounds // 2),\n",
    "                        log_evaluation(period=5)\n",
    "                    ],\n",
    "                    evals_result = training_meta,\n",
    "                    init_model = gbm_copy\n",
    "                )\n",
    "\n",
    "            \n",
    "\n",
    "        self.training_loss = training_meta['train'][training_loss_key]\n",
    "        self.test_loss = training_meta['test'][training_loss_key]\n",
    "        print(f'Finished training in {datetime.today() - t1}')\n",
    "    \n",
    "    def fit_xgb(self):\n",
    "        t1 = datetime.today()\n",
    "\n",
    "        self.params = {\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'booster': 'gbtree',\n",
    "            'max_depth': self.max_depth,\n",
    "            'num_leaves': self.num_leaves,\n",
    "            'max_bin': self.max_bin,\n",
    "        }\n",
    "\n",
    "        feval = self.hr_error_eval_metric_xgb\n",
    "\n",
    "        for train_data, train_data_just_data in zip(self.train_data, self.train_data_just_data):\n",
    "\n",
    "            self.gbm = xgb.train(\n",
    "                self.params,\n",
    "                train_data,\n",
    "                num_boost_round=self.n_estimators,\n",
    "                early_stopping_rounds=self.early_stopping_rounds,\n",
    "                feval=feval,\n",
    "                verbose_eval=5,\n",
    "                evals=[(train_data, 'train'), (self.test_data, 'test')],\n",
    "                xgb_model=self.gbm,\n",
    "                obj = self.loss.get_func()\n",
    "            )\n",
    "\n",
    "            # mse, hr_err, hr_err_sq = self.eval()\n",
    "            # print(f'Before fine-tuning: MSE = {mse}, HR error = {hr_err}, HR error (squared) = {hr_err_sq}')\n",
    "\n",
    "            if self.finetune:\n",
    "\n",
    "                print('\\n\\nFine-tuning...')\n",
    "                gbm_copy = self.gbm.copy()\n",
    "                pred = gbm_copy.predict(train_data)\n",
    "\n",
    "                new_targ = np.ones(len(pred))\n",
    "                nsplits = len(pred) // self.split_size\n",
    "                labels = train_data.get_label()\n",
    "                for i in range(nsplits):\n",
    "                    pred_curr = pred[i * self.split_size: (i + 1) * self.split_size]\n",
    "                    label_curr = labels[i * self.split_size: (i + 1) * self.split_size]\n",
    "                    hr_err = self.get_hr_error(pred_curr, label_curr, square=True)\n",
    "                    new_targ[i * self.split_size: (i + 1) * self.split_size] = hr_err\n",
    "\n",
    "                new_train_data = xgb.DMatrix(train_data_just_data, label=new_targ)\n",
    "\n",
    "                self.gbm = xgb.train(\n",
    "                    self.params,\n",
    "                    new_train_data,\n",
    "                    num_boost_round=self.n_estimators // 2,\n",
    "                    early_stopping_rounds=self.early_stopping_rounds // 2,\n",
    "                    feval=feval,\n",
    "                    verbose_eval=5,\n",
    "                    evals=[(new_train_data, 'train'), (self.test_data, 'test')],\n",
    "                    xgb_model=gbm_copy,\n",
    "                    obj = self.loss.get_func()\n",
    "                )\n",
    "\n",
    "        print(f'Finished training in {datetime.today() - t1}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.gbm.predict(X)\n",
    "    \n",
    "    def save(self, model_file = 'lonePineGBM.xgb'):\n",
    "\n",
    "        # new_params = copy.deepcopy(self.params)\n",
    "        # new_params['learning_rate'] = 0.0000001\n",
    "        # new_gbm = lgb.train(self.params, self.train_data[0], num_boost_round=1, init_model = self.gbm)\n",
    "\n",
    "        # # import onnxmltools\n",
    "        # # from onnxconverter_common.data_types import FloatTensorType\n",
    "\n",
    "        # # initial_types = [('input', FloatTensorType([None, new_gbm.num_feature()]))]\n",
    "        # # onnx_model = onnxmltools.convert_lightgbm(new_gbm, initial_types=initial_types)\n",
    "        # # onnxmltools.utils.save_model(onnx_model, model_file)\n",
    "\n",
    "        # import onnxmltools\n",
    "        # from onnxconverter_common.data_types import FloatTensorType\n",
    "        # onnx_model = onnxmltools.convert_lightgbm(new_gbm, initial_types=[('input', FloatTensorType([None, new_gbm.num_feature()]))])\n",
    "\n",
    "        # # Save as protobuf\n",
    "        # onnxmltools.utils.save_model(onnx_model, model_file)\n",
    "\n",
    "        self.gbm.save_model(model_file)\n",
    "\n",
    "    \n",
    "    def load_from_file(self, model_file):\n",
    "        self.gbm = lgb.model_from_string(model_file)\n",
    "\n",
    "    def eval(self):\n",
    "        \n",
    "        test_X = self.test_data.get_data()\n",
    "        test_y = self.test_data.get_label()\n",
    "        nsplits = int(len(test_X) / self.split_size)\n",
    "        errs = []\n",
    "        mses = np.zeros(len(test_X))\n",
    "        \n",
    "        for i in range(nsplits):\n",
    "\n",
    "            curr_pred = self.predict(test_X[i * self.split_size: (i + 1) * self.split_size, :])\n",
    "            curr_true = test_y[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true, curr_pred = self.process_signal(curr_true, curr_pred, smoothing_window = 5, use_bandpass = True)\n",
    "            \n",
    "            mses[i * self.split_size: (i + 1) * self.split_size] = curr_true - curr_pred\n",
    "            hr_err = self.get_hr_error(curr_true, curr_pred, square = False)\n",
    "            errs.append(hr_err)\n",
    "        \n",
    "        return np.mean(np.square(mses)), np.mean(errs), np.mean(np.square(errs))\n",
    "\n",
    "    def xgb_eval(self):\n",
    "        \n",
    "        nsplits = int(len(self.test_X) / self.split_size)\n",
    "        errs = []\n",
    "        mses = np.zeros(len(self.test_X))\n",
    "        \n",
    "        for i in range(nsplits):\n",
    "\n",
    "            curr_pred = self.predict(xgb.DMatrix(self.test_X[i * self.split_size: (i + 1) * self.split_size, :]))\n",
    "            curr_true = self.test_y[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true, curr_pred = self.process_signal(curr_true, curr_pred, smoothing_window = 5, use_bandpass = True)\n",
    "            \n",
    "            mses[i * self.split_size: (i + 1) * self.split_size] = curr_true - curr_pred\n",
    "            hr_err = self.get_hr_error(curr_true, curr_pred, square = False)\n",
    "            errs.append(hr_err)\n",
    "        \n",
    "        return np.mean(np.square(mses)), np.mean(errs), np.mean(np.square(errs))\n",
    "    \n",
    "    def validate(self):\n",
    "\n",
    "        test_X = self.test_data.get_data()\n",
    "        test_y = self.test_data.get_label()\n",
    "        nsplits = int(len(test_X) / self.split_size)\n",
    "        \n",
    "        errors = []\n",
    "        for i in range(nsplits):\n",
    "\n",
    "            curr_pred = self.predict(test_X[i * self.split_size: (i + 1) * self.split_size, :])\n",
    "            curr_true = test_y[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true, curr_pred = self.process_signal(curr_true, curr_pred, smoothing_window = 5, use_bandpass = True)\n",
    "            \n",
    "            mse = np.mean(np.square(curr_true - curr_pred))\n",
    "            hr_err = self.get_hr_error(curr_true, curr_pred, square = False)\n",
    "            hrv_err = self.get_hrv_error(curr_true, curr_pred, square = False)\n",
    "            peaks_err = self.get_peaks_error(curr_true, curr_pred, square = False)\n",
    "            errors.append({\n",
    "                'mse': mse,\n",
    "                'hr_err': hr_err,\n",
    "                'hrv_err': hrv_err,\n",
    "                'peaks_err': peaks_err\n",
    "            })\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def xgb_validate(self):\n",
    "\n",
    "        test_X = self.test_X\n",
    "        test_y = self.test_y\n",
    "        nsplits = int(len(test_X) / self.split_size)\n",
    "        \n",
    "        errors = []\n",
    "        for i in range(nsplits):\n",
    "            curr_X = xgb.DMatrix(test_X[i * self.split_size: (i + 1) * self.split_size, :])\n",
    "            curr_pred = self.predict(curr_X)\n",
    "            curr_true = test_y[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true, curr_pred = self.process_signal(curr_true, curr_pred, smoothing_window = 5, use_bandpass = True)\n",
    "            \n",
    "            mse = np.mean(np.square(curr_true - curr_pred))\n",
    "            hr_err = self.get_hr_error(curr_true, curr_pred, square = False)\n",
    "            hrv_err = self.get_hrv_error(curr_true, curr_pred, square = False)\n",
    "            peaks_err = self.get_peaks_error(curr_true, curr_pred, square = False)\n",
    "            errors.append({\n",
    "                'mse': mse,\n",
    "                'hr_err': hr_err,\n",
    "                'hrv_err': hrv_err,\n",
    "                'peaks_err': peaks_err\n",
    "            })\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def plot_loss(self):\n",
    "        if self.training_loss is not None and self.test_loss is not None:\n",
    "            training_loss_normed = min_max_scale(self.training_loss)\n",
    "            test_loss_normed = min_max_scale(self.test_loss)\n",
    "            plt.plot(training_loss_normed, label = 'training loss')\n",
    "            plt.plot(test_loss_normed, label = 'test loss')\n",
    "            plt.legend()\n",
    "        \n",
    "    def get_model_stats(self):\n",
    "\n",
    "        model_info = self.gbm.dump_model()\n",
    "        tree_depths = []\n",
    "\n",
    "        for tree_info in model_info['tree_info']:\n",
    "            tree_structure = tree_info['tree_structure']\n",
    "            \n",
    "            # Recursive function to compute the depth of a tree\n",
    "            def calculate_depth(node, current_depth=0):\n",
    "                if 'leaf_value' in node:\n",
    "                    return current_depth\n",
    "                else:\n",
    "                    left_depth = calculate_depth(node['left_child'], current_depth + 1)\n",
    "                    right_depth = calculate_depth(node['right_child'], current_depth + 1)\n",
    "                    return max(left_depth, right_depth)\n",
    "\n",
    "            tree_depth = calculate_depth(tree_structure)\n",
    "            tree_depths.append(tree_depth)\n",
    "        \n",
    "\n",
    "        print(f'Best test loss: {min(self.test_loss)}\\n')\n",
    "        print('Tree depth stats:')\n",
    "        print('Min tree depth:', min(tree_depths))\n",
    "        print('Max tree depth:', max(tree_depths))\n",
    "        print('Avg tree depth:', np.mean(tree_depths))\n",
    "        print('\\nFeature importances:')\n",
    "        display(self.get_feature_importances())\n",
    "    \n",
    "    def get_feature_importances(self):\n",
    "        importances = self.gbm.feature_importance(importance_type='gain')\n",
    "        feature_importances = pd.DataFrame({'feature': self.features, 'importance': importances})\n",
    "        feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "        return feature_importances\n",
    "    \n",
    "    def hr_error_eval_metric(self, y_pred, eval_data):\n",
    "        y_true = eval_data.get_label()\n",
    "        nsplits = int(len(y_pred) / self.split_size)\n",
    "        hr_err = []\n",
    "        for i in range(nsplits):\n",
    "            curr_pred = y_pred[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true = y_true[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true, curr_pred = self.process_signal(curr_true, curr_pred, smoothing_window = 10, use_bandpass = True)\n",
    "            hr_err.append(self.get_hr_error(curr_true, curr_pred, square = False))\n",
    "        return 'hr_err', np.mean(hr_err), False\n",
    "    \n",
    "    def hr_error_eval_metric_xgb(self, y_pred, eval_data):\n",
    "        y_true = eval_data.get_label()\n",
    "        nsplits = int(len(y_pred) / self.split_size)\n",
    "        hr_err = []\n",
    "        for i in range(nsplits):\n",
    "            curr_pred = y_pred[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true = y_true[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true, curr_pred = self.process_signal(curr_true, curr_pred, smoothing_window = 10, use_bandpass = True)\n",
    "            hr_err.append(self.get_hr_error(curr_true, curr_pred, square = False))\n",
    "        return 'hr_err', np.mean(hr_err)\n",
    "    \n",
    "    def get_hr_error(self, y_true, y_pred, square = True):\n",
    "\n",
    "        true_peaks, _ = self.get_true_peaks(y_true)\n",
    "        pred_peaks, _ = self.get_predicted_peaks(y_pred)\n",
    "\n",
    "        if len(true_peaks) >= 2:\n",
    "            true_ibis = np.diff(true_peaks) / 64\n",
    "            true_hr = 60 / np.mean(true_ibis)\n",
    "        else:\n",
    "            true_hr = 0\n",
    "\n",
    "        if len(pred_peaks) >= 2:\n",
    "            pred_ibis = np.diff(pred_peaks) / 64\n",
    "            pred_hr = 60 / np.mean(pred_ibis)\n",
    "        else:\n",
    "            pred_hr = 0\n",
    "        \n",
    "        if square:\n",
    "            return np.power(true_hr - pred_hr, 2)\n",
    "        return abs(true_hr - pred_hr)\n",
    "    \n",
    "    def get_peaks_error(self, y_true, y_pred, square = True):\n",
    "        true_peaks, _ = self.get_true_peaks(y_true)\n",
    "        pred_peaks, _ = self.get_predicted_peaks(y_pred)\n",
    "        if square:\n",
    "            return np.power(len(true_peaks) - len(pred_peaks), 2)\n",
    "        return abs(len(true_peaks) - len(pred_peaks))\n",
    "    \n",
    "    def get_hrv_error(self, y_true, y_pred, square = True):\n",
    "\n",
    "        true_peaks, _ = self.get_true_peaks(y_true)\n",
    "        pred_peaks, _ = self.get_predicted_peaks(y_pred)\n",
    "\n",
    "        if len(true_peaks) >= 2:\n",
    "            true_ibis = np.diff(true_peaks) / 64\n",
    "            true_hrv = get_hrv(true_ibis)\n",
    "        else:\n",
    "            true_hrv = 0\n",
    "\n",
    "        if len(pred_peaks) >= 2:\n",
    "            pred_ibis = np.diff(pred_peaks) / 64\n",
    "            pred_hrv = get_hrv(pred_ibis)\n",
    "        else:\n",
    "            pred_hrv = 0\n",
    "        \n",
    "        if square:\n",
    "            return np.power(true_hrv - pred_hrv, 2)\n",
    "        return abs(true_hrv - pred_hrv)\n",
    "    \n",
    "    def process_signal(self, y_true, y_pred, smoothing_window = 10, use_bandpass = False):\n",
    "    \n",
    "        orig_len = len(y_pred)\n",
    "        y_pred = n_moving_avg(y_pred, smoothing_window)\n",
    "        y_pred = resample(y_pred, orig_len)\n",
    "        if use_bandpass:\n",
    "            y_pred = bandpass(y_pred, 64, [self.min_bandpass_freq, self.max_bandpass_freq], self.bandpass_order)\n",
    "        y_pred = min_max_scale(y_pred)\n",
    "        \n",
    "        y_true = n_moving_avg(y_true, 20)\n",
    "        y_true = resample(y_true, orig_len)\n",
    "        if use_bandpass:\n",
    "            y_true = bandpass(y_true, 64, [self.min_bandpass_freq, self.max_bandpass_freq], self.bandpass_order)\n",
    "        y_true = min_max_scale(y_true)\n",
    "        \n",
    "        return y_true, y_pred\n",
    "    \n",
    "    def get_predicted_peaks(self, signal):\n",
    "        return get_peaks_v2(signal, 64, 3.0, -1, prominence = self.predicted_peaks_prominence, with_min_dist = True, with_valleys = False)\n",
    "    def get_true_peaks(self, signal):\n",
    "        return get_peaks_v2(signal, 64, 3.0, -1, prominence = self.true_peaks_prominence, with_min_dist = True, with_valleys = False)\n",
    "\n",
    "    def prepare_dataset_from_subjects(self, truths, data_beg = 1000, data_end = 2000):\n",
    "        data_arr = []\n",
    "        for i in range(len(truths)):    \n",
    "            truth = truths[i]\n",
    "            data = truth.prepare_data_for_ml(self.num_feats_per_channel, self.skip_amount)\n",
    "            data = data.iloc[data_beg: data_end, :]\n",
    "            data['subject'] = i + 1\n",
    "            data_arr.append(data)\n",
    "        return pd.concat(data_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectwise_kfold(truths, model_type = 'gbdt', random_state = None, loss_type = 'mse',\n",
    "                    n_estimators = 100, split_size = 1280, learning_rate = 0.1, test_size = 0.3, early_stopping_rounds = 50,\n",
    "                    mse_weight = None, dtw_weight = None, data_beg = 1000, data_end = 10000, batches = 1, finetune = True, \n",
    "                    min_bandpass_freq = 0.67, max_bandpass_freq = 3.0, bandpass_order = 4,\n",
    "                    predicted_peaks_prominence = 0.22, true_peaks_prominence = 0.15,\n",
    "                    max_depth = 7, num_leaves = 75, max_bin = 255, num_feats_per_channel = 3, skip_amount = 15,\n",
    "                    rounds_per_model = 1, collect = False):\n",
    "        \n",
    "        models = {}\n",
    "        for subj_idx in range(len(truths)):\n",
    "            models[subj_idx + 1] = []\n",
    "            for i in range(rounds_per_model):\n",
    "                \n",
    "                print(f'\\n\\nTraining excluding subject {subj_idx + 1}...\\n')\n",
    "                mod = LonePineGBM(\n",
    "                    truths = truths,\n",
    "                    model_type = model_type,\n",
    "                    random_state = random_state,\n",
    "                    loss_type = loss_type,\n",
    "                    n_estimators = n_estimators,\n",
    "                    split_size = split_size,\n",
    "                    learning_rate = learning_rate,\n",
    "                    test_size = test_size,\n",
    "                    early_stopping_rounds = early_stopping_rounds,\n",
    "                    mse_weight = mse_weight,\n",
    "                    dtw_weight = dtw_weight,\n",
    "                    data_beg = data_beg,\n",
    "                    data_end = data_end,\n",
    "                    batches = batches,\n",
    "                    finetune = finetune,\n",
    "                    min_bandpass_freq = min_bandpass_freq,\n",
    "                    max_bandpass_freq = max_bandpass_freq,\n",
    "                    bandpass_order = bandpass_order,\n",
    "                    predicted_peaks_prominence = predicted_peaks_prominence,\n",
    "                    true_peaks_prominence = true_peaks_prominence,\n",
    "                    max_depth = max_depth,\n",
    "                    num_leaves = num_leaves,\n",
    "                    max_bin = max_bin,\n",
    "                    num_feats_per_channel = num_feats_per_channel,\n",
    "                    skip_amount = skip_amount,\n",
    "                    excluded_subject = subj_idx + 1\n",
    "                )\n",
    "                mod.fit_xgb()\n",
    "                models[subj_idx + 1].append(mod)\n",
    "        \n",
    "        model_performances = {}\n",
    "        for subj_idx in models:\n",
    "            model_performances[subj_idx] = []\n",
    "            for i in range(rounds_per_model):\n",
    "                mod = models[subj_idx][i]\n",
    "                \n",
    "                if collect:\n",
    "                    model_performances[subj_idx].append(mod.xgb_validate())\n",
    "                else:\n",
    "                    model_performances[subj_idx].append(mod.xgb_eval())\n",
    "\n",
    "                \n",
    "        if collect:\n",
    "            return model_performances\n",
    "\n",
    "        mean_hr_score = np.mean([model_performances[subj_idx][i][1] for subj_idx in model_performances for i in range(rounds_per_model)])\n",
    "        return mean_hr_score, models, model_performances\n",
    "    \n",
    "\n",
    "class LonePineOptimizer:\n",
    "\n",
    "    def __init__(self, truths):\n",
    "        self.truths = truths\n",
    "    \n",
    "    def objective(self, n_estimators = 100, split_size = 1280, learning_rate = 0.1, test_size = 0.3, early_stopping_rounds = 50,\n",
    "                    mse_weight = None, dtw_weight = None, data_beg = 1000, data_end = 2000, batches = 1, finetune = True, \n",
    "                    min_bandpass_freq = 0.67, max_bandpass_freq = 3.0, bandpass_order = 4,\n",
    "                    predicted_peaks_prominence = 0.22, true_peaks_prominence = 0.15,\n",
    "                    max_depth = 7, num_leaves = 75, max_bin = 255, num_feats_per_channel = 3, skip_amount = 15):\n",
    "\n",
    "        try:\n",
    "            hr_score, _, _ = subjectwise_kfold(\n",
    "                self.truths,\n",
    "                model_type = 'gbdt',\n",
    "                random_state = None,\n",
    "                loss_type = 'combined',\n",
    "                n_estimators = n_estimators,\n",
    "                split_size = split_size,\n",
    "                learning_rate = learning_rate,\n",
    "                early_stopping_rounds = 50,\n",
    "                mse_weight = mse_weight,\n",
    "                dtw_weight = dtw_weight,\n",
    "                data_beg = data_beg,\n",
    "                data_end = data_end,\n",
    "                batches = batches,\n",
    "                finetune = finetune,\n",
    "                min_bandpass_freq = min_bandpass_freq,\n",
    "                max_bandpass_freq = max_bandpass_freq,\n",
    "                bandpass_order = bandpass_order,\n",
    "                predicted_peaks_prominence = predicted_peaks_prominence,\n",
    "                true_peaks_prominence = true_peaks_prominence,\n",
    "                max_depth = max_depth,\n",
    "                num_leaves = num_leaves,\n",
    "                max_bin = max_bin,\n",
    "                num_feats_per_channel = num_feats_per_channel,\n",
    "                skip_amount = skip_amount,\n",
    "            )\n",
    "            return hr_score\n",
    "        except:\n",
    "            return 1000\n",
    "    \n",
    "    def optimize(self, n_calls = 50):\n",
    "\n",
    "        space = [\n",
    "            Integer(50, 300, name = \"n_estimators\"),\n",
    "            Integer(640, 1280, name = \"split_size\"),\n",
    "            Real(0.002, 0.5, name = \"learning_rate\"),\n",
    "            Integer(10, 100, name = \"early_stopping_rounds\"),\n",
    "            Real(0.0, 1.0, name = \"mse_weight\"),\n",
    "            Real(0.0, 1.0, name = \"dtw_weight\"),\n",
    "            Integer(1000, 4000, name = \"data_beg\"),\n",
    "            Integer(6000, 10000, name = \"data_end\"),\n",
    "            Integer(1, 8, name = \"batches\"),\n",
    "            Real(0.4, 1.0, name = \"min_bandpass_freq\"),\n",
    "            Real(2.5, 4.0, name = \"max_bandpass_freq\"),\n",
    "            Integer(2, 6, name = \"bandpass_order\"),\n",
    "            Real(0.1, 0.75, name = \"predicted_peaks_prominence\"),\n",
    "            Real(0.1, 0.5, name = \"true_peaks_prominence\"),\n",
    "            Integer(3, 10, name = \"max_depth\"),\n",
    "            Integer(30, 140, name = \"num_leaves\"),\n",
    "            Integer(100, 300, name = \"max_bin\"),\n",
    "            Integer(3, 10, name = \"num_feats_per_channel\"),\n",
    "            Integer(5, 25, name = \"skip_amount\"),\n",
    "        ]\n",
    "\n",
    "        @use_named_args(space)\n",
    "        def wrapped_objective(**params):\n",
    "            return self.objective(**params)\n",
    "        \n",
    "        result = gp_minimize(\n",
    "            wrapped_objective, space, n_calls=n_calls, random_state=42, verbose=1\n",
    "        )\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = LonePineOptimizer(truths)\n",
    "result = optimizer.optimize(n_calls = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[198,\n",
       " 652,\n",
       " 0.13420232759443168,\n",
       " 65,\n",
       " 0.6077211141678971,\n",
       " 0.7044252128526859,\n",
       " 3747,\n",
       " 6000,\n",
       " 1,\n",
       " 0.8352220930795105,\n",
       " 2.8301370404787782,\n",
       " 6,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 5,\n",
       " 30,\n",
       " 300,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# space = [\n",
    "#             Integer(50, 300, name = \"n_estimators\"),\n",
    "#             Integer(640, 1280, name = \"split_size\"),\n",
    "#             Real(0.002, 0.5, name = \"learning_rate\"),\n",
    "#             Integer(10, 100, name = \"early_stopping_rounds\"),\n",
    "#             Real(0.0, 1.0, name = \"mse_weight\"),\n",
    "#             Real(0.0, 1.0, name = \"dtw_weight\"),\n",
    "#             Integer(1000, 4000, name = \"data_beg\"),\n",
    "#             Integer(6000, 10000, name = \"data_end\"),\n",
    "#             Integer(1, 8, name = \"batches\"),\n",
    "#             Real(0.4, 1.0, name = \"min_bandpass_freq\"),\n",
    "#             Real(2.5, 4.0, name = \"max_bandpass_freq\"),\n",
    "#             Integer(2, 6, name = \"bandpass_order\"),\n",
    "#             Real(0.1, 0.75, name = \"predicted_peaks_prominence\"),\n",
    "#             Real(0.1, 0.5, name = \"true_peaks_prominence\"),\n",
    "#             Integer(3, 10, name = \"max_depth\"),\n",
    "#             Integer(30, 140, name = \"num_leaves\"),\n",
    "#             Integer(100, 300, name = \"max_bin\"),\n",
    "#             Integer(3, 10, name = \"num_feats_per_channel\"),\n",
    "#             Integer(5, 25, name = \"skip_amount\"),\n",
    "#         ]\n",
    "\n",
    "optimization_res2 = [198,  # estimators\n",
    " 652,  # split size\n",
    " 0.13420232759443168,  learning rate\n",
    " 65,  # eary stopping rounds\n",
    " 0.6077211141678971, # mse weight\n",
    " 0.7044252128526859, # dtw weight\n",
    " 3747, # data beg\n",
    " 6000, # data end\n",
    " 1, # batches\n",
    " 0.8352220930795105, # min bandpass freq\n",
    " 2.8301370404787782, # max bandpass freq\n",
    " 6, # bandpass order\n",
    " 0.1, # predicted peaks prominence\n",
    " 0.1, # true peaks prominence\n",
    " 5, # max depth\n",
    " 30, # num leaves\n",
    " 300, # max bin\n",
    " 5, # num feats per channel\n",
    " 5] # skip amount\n",
    "\n",
    "optimization_res = [188,\n",
    " 993,\n",
    " 0.22844583483861589,\n",
    " 16,\n",
    " 0.913467044583398,\n",
    " 1.0,\n",
    " 3909,\n",
    " 7474,\n",
    " 5,\n",
    " 0.9421772128909808,\n",
    " 3.6351090994830813,\n",
    " 4,\n",
    " 0.17087564911262462,\n",
    " 0.322741927274642,\n",
    " 6,\n",
    " 34,\n",
    " 235,\n",
    " 8,\n",
    " 12]\n",
    "\n",
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean([model_performances[sub][i][1] for sub in model_performances for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "truths = []\n",
    "for subject in range(1, 8):\n",
    "\n",
    "    truth = IeeeGroundTruth(subject, 1, directory = 'channel_data3')\n",
    "    truth.align_rgb_bvp()\n",
    "    truth.fill_nans()\n",
    "    truth.process_rgb(\n",
    "        minmax = False,\n",
    "        use_wavelet = True,\n",
    "        use_bandpass = False\n",
    "    )\n",
    "    truth.process_bvp()\n",
    "    truths.append(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training excluding subject 1...\n",
      "\n",
      "Rows per batch: 4800\n",
      "[08:41:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.77048\ttrain-hr_err:6.31178\ttest-rmse:0.81159\ttest-hr_err:15.26486\n",
      "[5]\ttrain-rmse:0.80290\ttrain-hr_err:7.66678\ttest-rmse:0.83865\ttest-hr_err:15.33343\n",
      "[10]\ttrain-rmse:0.83699\ttrain-hr_err:8.50060\ttest-rmse:0.86887\ttest-hr_err:14.37104\n",
      "[15]\ttrain-rmse:0.87304\ttrain-hr_err:7.25496\ttest-rmse:0.90148\ttest-hr_err:14.71634\n",
      "[20]\ttrain-rmse:0.91109\ttrain-hr_err:8.03393\ttest-rmse:0.93420\ttest-hr_err:16.31446\n",
      "[24]\ttrain-rmse:0.94311\ttrain-hr_err:9.35810\ttest-rmse:0.96189\ttest-hr_err:17.37810\n",
      "\n",
      "\n",
      "Fine-tuning...\n",
      "[08:41:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:2915.62012\ttrain-hr_err:45.84414\ttest-rmse:25.57810\ttest-hr_err:25.69176\n",
      "[5]\ttrain-rmse:3054.48511\ttrain-hr_err:45.02798\ttest-rmse:160.39296\ttest-hr_err:25.69176\n",
      "[7]\ttrain-rmse:3112.09717\ttrain-hr_err:45.02798\ttest-rmse:216.20805\ttest-hr_err:25.69176\n",
      "[08:41:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:213.82295\ttrain-hr_err:19.92814\ttest-rmse:289.55679\ttest-hr_err:19.62604\n",
      "[5]\ttrain-rmse:265.43774\ttrain-hr_err:23.58515\ttest-rmse:763.92444\ttest-hr_err:23.47830\n",
      "[10]\ttrain-rmse:366.79990\ttrain-hr_err:48.15604\ttest-rmse:1325.81152\ttest-hr_err:24.09598\n",
      "[15]\ttrain-rmse:491.69092\ttrain-hr_err:55.86909\ttest-rmse:1913.30640\ttest-hr_err:21.94862\n",
      "[16]\ttrain-rmse:518.39722\ttrain-hr_err:55.62739\ttest-rmse:2033.18384\ttest-hr_err:21.94862\n",
      "\n",
      "\n",
      "Fine-tuning...\n",
      "[08:41:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:6452.70752\ttrain-hr_err:55.99546\ttest-rmse:2166.00757\ttest-hr_err:21.16371\n",
      "[5]\ttrain-rmse:6766.94629\ttrain-hr_err:54.84547\ttest-rmse:2263.65063\ttest-hr_err:25.88873\n",
      "[8]\ttrain-rmse:6963.39404\ttrain-hr_err:57.95126\ttest-rmse:2351.79663\ttest-hr_err:24.28359\n",
      "[08:41:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:4047.50366\ttrain-hr_err:30.26653\ttest-rmse:2373.76147\ttest-hr_err:24.28359\n",
      "[5]\ttrain-rmse:4250.52881\ttrain-hr_err:30.24667\ttest-rmse:2487.54004\ttest-hr_err:22.76249\n",
      "[10]\ttrain-rmse:4463.95117\ttrain-hr_err:30.24667\ttest-rmse:2608.10059\ttest-hr_err:23.10523\n",
      "[15]\ttrain-rmse:4688.29541\ttrain-hr_err:30.24667\ttest-rmse:2735.70801\ttest-hr_err:23.12629\n",
      "[20]\ttrain-rmse:4924.09619\ttrain-hr_err:30.24232\ttest-rmse:2870.63623\ttest-hr_err:22.66956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rg/h0swnf611kvgnhnj7mb2m6300000gn/T/ipykernel_1759/4282250085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_bandpass_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bandpass_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandpass_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mpredicted_peaks_prominence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_peaks_prominence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mnum_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m235\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_feats_per_channel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_amount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rg/h0swnf611kvgnhnj7mb2m6300000gn/T/ipykernel_1759/3755164656.py\u001b[0m in \u001b[0;36msubjectwise_kfold\u001b[0;34m(truths, model_type, random_state, loss_type, n_estimators, split_size, learning_rate, test_size, early_stopping_rounds, mse_weight, dtw_weight, data_beg, data_end, batches, finetune, min_bandpass_freq, max_bandpass_freq, bandpass_order, predicted_peaks_prominence, true_peaks_prominence, max_depth, num_leaves, max_bin, num_feats_per_channel, skip_amount, rounds_per_model, collect)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mexcluded_subject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubj_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 )\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_xgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubj_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rg/h0swnf611kvgnhnj7mb2m6300000gn/T/ipykernel_1759/3156343904.py\u001b[0m in \u001b[0;36mfit_xgb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgbm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             )\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    194\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/callback.py\u001b[0m in \u001b[0;36mafter_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dataset name should not contain `-`'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# into datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;31m# split up `test-error:0.1234`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36meval_set\u001b[0;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m                 feval_ret = feval(self.predict(dmat, training=False,\n\u001b[0;32m-> 1753\u001b[0;31m                                                output_margin=True), dmat)\n\u001b[0m\u001b[1;32m   1754\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeval_ret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rg/h0swnf611kvgnhnj7mb2m6300000gn/T/ipykernel_1759/3156343904.py\u001b[0m in \u001b[0;36mhr_error_eval_metric_xgb\u001b[0;34m(self, y_pred, eval_data)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mcurr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mcurr_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mcurr_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bandpass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0mhr_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hr_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'hr_err'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rg/h0swnf611kvgnhnj7mb2m6300000gn/T/ipykernel_1759/3156343904.py\u001b[0m in \u001b[0;36mprocess_signal\u001b[0;34m(self, y_true, y_pred, smoothing_window, use_bandpass)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0morig_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_moving_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_bandpass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/indiv_projects/school/masters/pydroid/example/android/app/src/main/python/pipeline_dev/pipeline_v1/signal_pross.py\u001b[0m in \u001b[0;36mn_moving_avg\u001b[0;34m(signal, window)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         result.append(\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "options = [\n",
    "    {\n",
    "        'minmax': False,\n",
    "        'use_wavelet': True,\n",
    "        'use_bandpass': False\n",
    "    }\n",
    "]\n",
    "rpm = 1\n",
    "\n",
    "scores = []\n",
    "\n",
    "for option in options:\n",
    "    \n",
    "    truths = []\n",
    "    for subject in range(1, 8):\n",
    "\n",
    "        truth = IeeeGroundTruth(subject, 1, directory = 'channel_data3')\n",
    "        truth.align_rgb_bvp()\n",
    "        truth.fill_nans()\n",
    "        truth.process_rgb(\n",
    "            minmax = option['minmax'],\n",
    "            use_wavelet = option['use_wavelet'],\n",
    "            use_bandpass = option['use_bandpass']\n",
    "        )\n",
    "        truth.process_bvp()\n",
    "        truths.append(truth)\n",
    "\n",
    "\n",
    "    res = subjectwise_kfold(\n",
    "        truths, model_type = 'gbdt', random_state = None, loss_type = 'combined', rounds_per_model = rpm,\n",
    "        n_estimators = 198, split_size = 960, learning_rate = 0.1,\n",
    "        early_stopping_rounds = 65, mse_weight = 0.61, dtw_weight = 0.7, data_beg = 3747, data_end = 7587,\n",
    "        batches = 1, min_bandpass_freq = 0.7, max_bandpass_freq = 4.0, bandpass_order = 6,\n",
    "        predicted_peaks_prominence = 0.1, true_peaks_prominence = 0.1, max_depth = 5,\n",
    "        num_leaves = 30, max_bin = 300, num_feats_per_channel = 5, skip_amount = 5, collect = True\n",
    "    )\n",
    "\n",
    "    scores.append(res)\n",
    "\n",
    "    optimization_res2 = [198,  # estimators\n",
    " 652,  # split size\n",
    " 0.13420232759443168,  learning rate\n",
    " 65,  # eary stopping rounds\n",
    " 0.6077211141678971, # mse weight\n",
    " 0.7044252128526859, # dtw weight\n",
    " 3747, # data beg\n",
    " 6000, # data end\n",
    " 1, # batches\n",
    " 0.8352220930795105, # min bandpass freq\n",
    " 2.8301370404787782, # max bandpass freq\n",
    " 6, # bandpass order\n",
    " 0.1, # predicted peaks prominence\n",
    " 0.1, # true peaks prominence\n",
    " 5, # max depth\n",
    " 30, # num leaves\n",
    " 300, # max bin\n",
    " 5, # num feats per channel\n",
    " 5] # skip amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_bandpass_freq = 0.9421772128909808\n",
    "# max_bandpass_freq = 3.6351090994830813\n",
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = scores[0]\n",
    "for sub in res:\n",
    "    print(f'Subject {sub}:')\n",
    "    sub_errs = {'hr': [], 'hrv': [], 'peaks': []}\n",
    "    for subres in res[sub]:\n",
    "        hr = round(np.mean([d['hr_err'] for d in subres]), 2)\n",
    "        hrv = round(np.mean([d['hrv_err'] for d in subres]), 2)\n",
    "        peaks = round(np.mean([d['peaks_err'] for d in subres]), 2)\n",
    "        sub_errs['hr'].append(hr)\n",
    "        sub_errs['hrv'].append(hrv)\n",
    "        sub_errs['peaks'].append(peaks)\n",
    "\n",
    "        print(f'\\t- HR: {hr}; HRV: {hrv}; Peaks: {peaks}')\n",
    "    \n",
    "    print(f'\\tMean HR: {round(np.mean(sub_errs[\"hr\"]), 2)}; Mean HRV: {round(np.mean(sub_errs[\"hrv\"]), 2)}; Mean Peaks: {round(np.mean(sub_errs[\"peaks\"]), 2)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject = 7\n",
    "test_subject_truth = truths[test_subject - 1]\n",
    "import xgboost as xgb\n",
    "mod = LonePineGBM(\n",
    "    truths, model_type = 'gbdt', random_state = None, loss_type = 'combined',\n",
    "    n_estimators = 188, split_size = 960, learning_rate = 0.01,\n",
    "    early_stopping_rounds = 16, mse_weight = 0.2, dtw_weight = 0.8, data_beg = 8000, data_end = 10180,\n",
    "    batches = 5, min_bandpass_freq = 0.7, max_bandpass_freq = 4.0, bandpass_order = 4,\n",
    "    predicted_peaks_prominence = 0.17087564911262462, true_peaks_prominence = 0.322741927274642, max_depth = 6,\n",
    "    num_leaves = 34, max_bin = 235, num_feats_per_channel = 8, skip_amount = 12, finetune = True\n",
    ")\n",
    "mod.fit_xgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, hr_err, hr_err_sq = mod.xgb_eval()\n",
    "\n",
    "print(f'\\n\\nMSE: {mse}')\n",
    "print(f'HR error: {hr_err}')\n",
    "print(f'HR error squared: {hr_err_sq}\\n\\n')\n",
    "\n",
    "mod.get_model_stats()\n",
    "mod.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg = 3000\n",
    "end = beg + 960\n",
    "\n",
    "model = mod\n",
    "\n",
    "data = truths[2].prepare_data_for_ml(8, 12)\n",
    "x = xgb.DMatrix(data.drop(columns = ['bvp']).to_numpy())\n",
    "y = data['bvp'].to_numpy()\n",
    "\n",
    "def measure_code_block():\n",
    "    t = datetime.today()\n",
    "    targ = y[beg: end]\n",
    "    pred = model.predict(x)\n",
    "    pred = pred[beg: end]\n",
    "    targ, pred = mod.process_signal(targ, pred, use_bandpass=True)\n",
    "    print(f'Elapsed time: {datetime.today() - t}')\n",
    "    return targ, pred\n",
    "\n",
    "targ, pred = measure_code_block()\n",
    "\n",
    "pred_peaks, _ = mod.get_predicted_peaks(pred)\n",
    "true_peaks, _ = mod.get_true_peaks(targ)\n",
    "\n",
    "plt.plot(targ)\n",
    "plt.plot(pred)\n",
    "plt.scatter(pred_peaks, pred[pred_peaks], c='r')\n",
    "plt.scatter(true_peaks, targ[true_peaks], c='g')\n",
    "\n",
    "pred_ibis = np.diff(pred_peaks) / 64\n",
    "true_ibis = np.diff(true_peaks) / 64\n",
    "pred_hr = get_hr(pred_ibis)\n",
    "true_hr = get_hr(true_ibis)\n",
    "print(f'True HR: {true_hr}; Pred HR: {pred_hr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def normalized_lomb_periodogram(time, ibis, frequency_range):\n",
    "    \"\"\"\n",
    "    Calculate the normalized Lomb periodogram of detrended IBIs.\n",
    "\n",
    "    Args:\n",
    "    time (numpy array): The time values of the IBIs\n",
    "    ibis (numpy array): The detrended IBIs\n",
    "    frequency_range (tuple): A tuple with the minimum and maximum frequencies to consider (min_freq, max_freq)\n",
    "\n",
    "    Returns:\n",
    "    (numpy array, numpy array): The frequencies and corresponding Lomb periodogram values\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the Lomb periodogram\n",
    "    angular_frequencies = np.linspace(frequency_range[0] * 2 * np.pi, frequency_range[1] * 2 * np.pi, len(time))\n",
    "    periodogram = scipy.signal.lombscargle(time, ibis, angular_frequencies, normalize=True)\n",
    "\n",
    "    # Convert angular frequencies to regular frequencies\n",
    "    frequencies = angular_frequencies / (2 * np.pi)\n",
    "\n",
    "    return frequencies, periodogram\n",
    "\n",
    "def plot_normalized_lomb_periodogram(time, ibis, frequency_range):\n",
    "    # Calculate the Lomb periodogram using the normalized_lomb_periodogram function\n",
    "    frequencies, periodogram = normalized_lomb_periodogram(time, ibis, frequency_range)\n",
    "\n",
    "    # Create a plot using matplotlib\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(frequencies, periodogram)\n",
    "\n",
    "    # Label the axes and add a title\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Power\")\n",
    "    plt.title(\"Normalized Lomb Periodogram of Detrended IBIs\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "true_time = np.cumsum(true_ibis)\n",
    "pred_time = np.cumsum(pred_ibis)\n",
    "\n",
    "plot_normalized_lomb_periodogram(true_time, true_ibis, (0.7, 4.0))\n",
    "plot_normalized_lomb_periodogram(pred_time, pred_ibis, (0.7, 4.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between two 1D NumPy arrays.\n",
    "\n",
    "    Args:\n",
    "    x (numpy array): The first 1D NumPy array\n",
    "    y (numpy array): The second 1D NumPy array\n",
    "\n",
    "    Returns:\n",
    "    float: The Pearson correlation coefficient\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the correlation matrix using numpy.corrcoef\n",
    "    corr_matrix = np.corrcoef(x, y)\n",
    "\n",
    "    # Extract the correlation coefficient (off-diagonal element)\n",
    "    correlation_coefficient = corr_matrix[0, 1]\n",
    "\n",
    "    return correlation_coefficient\n",
    "\n",
    "# Example usage:\n",
    "# x = np.array([...])  # Your first 1D NumPy array\n",
    "# y = np.array([...])  # Your second 1D NumPy array\n",
    "# correlation = pearson_correlation(x, y)\n",
    "print(pred.shape)\n",
    "pearson_correlation(targ, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_work_def import  MtlNetwork_body2\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir = 'replicate_pickle_data'\n",
    "num_classes = 85\n",
    "num_features = 100*100*40\n",
    "batch_size = 16\n",
    "total_training_batches = 2500\n",
    "\n",
    "subject_data = {}\n",
    "\n",
    "subs = ['sub1', 'sub2', 'sub3', 'sub4', 'sub5', 'sub7']\n",
    "for subject in subs:\n",
    "\n",
    "    # load subject data\n",
    "    with open(f'{ddir}/{subject}/data_align.pkl', 'rb') as f:\n",
    "        data_align = pickle.load(f)\n",
    "    f.close()\n",
    "    with open(f'{ddir}/{subject}/ppgnp_align.pkl', 'rb') as f:\n",
    "        ppgnp_align = pickle.load(f)\n",
    "    f.close()\n",
    "    with open(f'{ddir}/{subject}/trainX.pkl', 'rb') as f:\n",
    "        trainX = pickle.load(f)\n",
    "    f.close()\n",
    "    with open(f'{ddir}/{subject}/trainY.pkl', 'rb') as f:\n",
    "        trainY = pickle.load(f)\n",
    "    f.close()\n",
    "    with open(f'{ddir}/{subject}/pulR.pkl', 'rb') as f:\n",
    "        pulR = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    subject_data[subject] = {}\n",
    "    subject_data[subject]['data_align'] = data_align\n",
    "    subject_data[subject]['ppgnp_align'] = ppgnp_align\n",
    "    subject_data[subject]['trainX'] = trainX[0: total_training_batches]\n",
    "    subject_data[subject]['trainY'] = trainY[0: total_training_batches]\n",
    "    subject_data[subject]['pulR'] = pulR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:44<00:00, 17.40s/it]\n"
     ]
    }
   ],
   "source": [
    "def final_data_prep(trainX, trainY):\n",
    "\n",
    "    trainX = np.array(trainX, dtype = np.float32)\n",
    "    trainY = np.array(trainY, dtype = np.float32)\n",
    "\n",
    "    trainY = trainY - trainY.min(axis = 1)[:, np.newaxis]\n",
    "    trainY = (trainY/(trainY.max(axis = 1)[:, np.newaxis]+ 10**-5))*2-1\n",
    "    trainX = (trainX-trainX.min())\n",
    "    trainX = trainX/ trainX.max()\n",
    "\n",
    "    trX, teX, trY, teY = train_test_split(trainX , trainY, \n",
    "                                        test_size = .1, random_state = 42)\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((trX, trY))\n",
    "    train_data = train_data.repeat().shuffle(buffer_size=100,\n",
    "                                            seed= 8).batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return {\n",
    "        'train_data': train_data,\n",
    "        'testX': teX,\n",
    "        'testY': teY\n",
    "    }\n",
    "\n",
    "training_data = {}\n",
    "for subject in tqdm(subject_data):\n",
    "    training_data[subject] = final_data_prep(subject_data[subject]['trainX'], subject_data[subject]['trainY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RootMeanSquareLoss(x,y):\n",
    "    \n",
    "    # pdb.set_trace()  \n",
    "    loss = tf.keras.losses.MSE(y_true = y, y_pred =x)  # initial one\n",
    "    #return tf.reduce_mean(loss)  # some other shape similarity\n",
    "     \n",
    "    loss2 = tf.reduce_mean((tf.math.abs(tf.math.sign(y))-tf.math.sign(tf.math.multiply(x,y))),axis = -1)\n",
    "    # print(loss2.shape)\n",
    "    \n",
    "    # print(tf.reduce_mean(loss), tf.reduce_mean(loss2))\n",
    "    return loss + 0.5*loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(opt, mod, x, y):\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "        \n",
    "        pred = mod(x, training=True)\n",
    "        loss = RootMeanSquareLoss(y, pred)\n",
    "\n",
    "    # Gradients for the shared body\n",
    "    tvars = mod.trainable_variables\n",
    "    gradients = g.gradient(loss, tvars)\n",
    "    opt.apply_gradients(zip(gradients, tvars))\n",
    "\n",
    "    del g  # Delete the GradientTape object to free up resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "def Val_loss (mod, testX, testY):\n",
    "    pred = mod(testX, training = False)\n",
    "    loss = RootMeanSquareLoss(testY, pred)\n",
    "    val_loss.append(tf.reduce_mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mtl_nn(mod, training_data, training_steps_per_subject = 1000):\n",
    "    \n",
    "    opt = tf.optimizers.SGD(0.0005)\n",
    "    min_val_loss = float('inf')\n",
    "\n",
    "    subject_iterators = {subject: iter(training_data[subject]['train_data']) for subject in training_data}\n",
    "\n",
    "    t1 = datetime.today()\n",
    "\n",
    "    for step in range(1, training_steps_per_subject + 1):\n",
    "        for subject in training_data:\n",
    "            \n",
    "            teX = training_data[subject]['testX']\n",
    "            teY = training_data[subject]['testY']\n",
    "\n",
    "            batch_x, batch_y = next(subject_iterators[subject])\n",
    "\n",
    "            # Run optimization for the current subject's data and head\n",
    "            run_optimization(opt, mod, batch_x, batch_y)\n",
    "        \n",
    "            if step % 10 == 0:\n",
    "                \n",
    "                pred = mod(batch_x, training=True)\n",
    "                loss = RootMeanSquareLoss(batch_y, pred)\n",
    "                train_loss.append(tf.reduce_mean(loss))\n",
    "\n",
    "                tp = np.random.randint(len(teX)-16)\n",
    "                Val_loss(mod, teX[tp+0:tp+16], teY[tp+0:tp+16])\n",
    "                current_val_loss = val_loss[-1]\n",
    "                print(f'Step: {step}, Subject: {subject}; Training Loss: {tf.reduce_mean(train_loss[-1])}, Val Loss: {current_val_loss}')\n",
    "                \n",
    "                # Save the model weights if the current validation loss is lower than the previous minimum validation loss\n",
    "                if current_val_loss < min_val_loss:\n",
    "                    min_val_loss = current_val_loss\n",
    "                    if not os.path.exists(\"inprocess_weights\"):\n",
    "                        os.mkdir(\"inprocess_weights\")\n",
    "\n",
    "                    print(f'Saving model with validation loss: {min_val_loss}\\n')\n",
    "                    mod.save_weights(\"inprocess_weights/shared_body\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10, Subject: sub1; Training Loss: 0.8419879674911499, Val Loss: 0.8882651925086975\n",
      "Saving model with validation loss: 0.8882651925086975\n",
      "\n",
      "Step: 10, Subject: sub2; Training Loss: 0.8147581815719604, Val Loss: 0.9089601039886475\n",
      "Step: 10, Subject: sub3; Training Loss: 0.7709710597991943, Val Loss: 0.9080119132995605\n",
      "Step: 10, Subject: sub4; Training Loss: 0.8473868370056152, Val Loss: 0.9087417125701904\n",
      "Step: 10, Subject: sub5; Training Loss: 0.7645107507705688, Val Loss: 0.7904037833213806\n",
      "Saving model with validation loss: 0.7904037833213806\n",
      "\n",
      "Step: 10, Subject: sub7; Training Loss: 0.648466944694519, Val Loss: 0.8710561990737915\n",
      "Step: 20, Subject: sub1; Training Loss: 0.8585892915725708, Val Loss: 0.9707973003387451\n",
      "Step: 20, Subject: sub2; Training Loss: 0.7584059238433838, Val Loss: 0.9322973489761353\n",
      "Step: 20, Subject: sub3; Training Loss: 0.8060441017150879, Val Loss: 0.9230567216873169\n",
      "Step: 20, Subject: sub4; Training Loss: 0.7638152837753296, Val Loss: 0.9496170282363892\n",
      "Step: 20, Subject: sub5; Training Loss: 0.7174891233444214, Val Loss: 0.8946541547775269\n",
      "Step: 20, Subject: sub7; Training Loss: 0.6012579202651978, Val Loss: 0.8617964386940002\n",
      "Step: 30, Subject: sub1; Training Loss: 0.7733513712882996, Val Loss: 0.897029459476471\n",
      "Step: 30, Subject: sub2; Training Loss: 0.8022935390472412, Val Loss: 0.9565337300300598\n",
      "Step: 30, Subject: sub3; Training Loss: 0.8093891143798828, Val Loss: 0.8724783658981323\n",
      "Step: 30, Subject: sub4; Training Loss: 0.8312758207321167, Val Loss: 0.923041820526123\n",
      "Step: 30, Subject: sub5; Training Loss: 0.6653526425361633, Val Loss: 0.8472957611083984\n",
      "Step: 30, Subject: sub7; Training Loss: 0.5863304138183594, Val Loss: 0.8520975112915039\n",
      "Step: 40, Subject: sub1; Training Loss: 0.784393846988678, Val Loss: 0.9243497848510742\n",
      "Step: 40, Subject: sub2; Training Loss: 0.7880802154541016, Val Loss: 0.933312177658081\n",
      "Step: 40, Subject: sub3; Training Loss: 0.7722623944282532, Val Loss: 0.9038235545158386\n",
      "Step: 40, Subject: sub4; Training Loss: 0.8089139461517334, Val Loss: 0.9303690195083618\n",
      "Step: 40, Subject: sub5; Training Loss: 0.6433905363082886, Val Loss: 0.8291001319885254\n",
      "Step: 40, Subject: sub7; Training Loss: 0.6724891662597656, Val Loss: 0.8772231936454773\n",
      "Step: 50, Subject: sub1; Training Loss: 0.8277311325073242, Val Loss: 0.9199672937393188\n",
      "Step: 50, Subject: sub2; Training Loss: 0.7641010284423828, Val Loss: 0.9137226343154907\n",
      "Step: 50, Subject: sub3; Training Loss: 0.7687304019927979, Val Loss: 0.8410236835479736\n",
      "Step: 50, Subject: sub4; Training Loss: 0.840129554271698, Val Loss: 0.9221531748771667\n",
      "Step: 50, Subject: sub5; Training Loss: 0.6705297231674194, Val Loss: 0.8100976943969727\n",
      "Step: 50, Subject: sub7; Training Loss: 0.6012235283851624, Val Loss: 0.8125340342521667\n",
      "Step: 60, Subject: sub1; Training Loss: 0.8248456120491028, Val Loss: 0.850462794303894\n",
      "Step: 60, Subject: sub2; Training Loss: 0.7222113609313965, Val Loss: 0.9337060451507568\n",
      "Step: 60, Subject: sub3; Training Loss: 0.7706429362297058, Val Loss: 0.8270895481109619\n",
      "Step: 60, Subject: sub4; Training Loss: 0.7535438537597656, Val Loss: 0.9301587343215942\n",
      "Step: 60, Subject: sub5; Training Loss: 0.7061288952827454, Val Loss: 0.7986722588539124\n",
      "Step: 60, Subject: sub7; Training Loss: 0.5962556004524231, Val Loss: 0.792542576789856\n",
      "Step: 70, Subject: sub1; Training Loss: 0.7903562784194946, Val Loss: 0.8304630517959595\n",
      "Step: 70, Subject: sub2; Training Loss: 0.7814277410507202, Val Loss: 0.9109214544296265\n",
      "Step: 70, Subject: sub3; Training Loss: 0.7681806087493896, Val Loss: 0.8279138803482056\n",
      "Step: 70, Subject: sub4; Training Loss: 0.8227922916412354, Val Loss: 0.8822053074836731\n",
      "Step: 70, Subject: sub5; Training Loss: 0.668953537940979, Val Loss: 0.7356764078140259\n",
      "Saving model with validation loss: 0.7356764078140259\n",
      "\n",
      "Step: 70, Subject: sub7; Training Loss: 0.5774282813072205, Val Loss: 0.7988631129264832\n",
      "Step: 80, Subject: sub1; Training Loss: 0.7821169495582581, Val Loss: 0.8371338844299316\n",
      "Step: 80, Subject: sub2; Training Loss: 0.6408812403678894, Val Loss: 0.835414469242096\n",
      "Step: 80, Subject: sub3; Training Loss: 0.7402474880218506, Val Loss: 0.7937148809432983\n",
      "Step: 80, Subject: sub4; Training Loss: 0.843925952911377, Val Loss: 0.9092514514923096\n",
      "Step: 80, Subject: sub5; Training Loss: 0.6664286255836487, Val Loss: 0.6884995698928833\n",
      "Saving model with validation loss: 0.6884995698928833\n",
      "\n",
      "Step: 80, Subject: sub7; Training Loss: 0.5883859395980835, Val Loss: 0.7524970769882202\n",
      "Step: 90, Subject: sub1; Training Loss: 0.8029963970184326, Val Loss: 0.7960222959518433\n",
      "Step: 90, Subject: sub2; Training Loss: 0.6675148010253906, Val Loss: 0.8575624227523804\n",
      "Step: 90, Subject: sub3; Training Loss: 0.7263318300247192, Val Loss: 0.8657619953155518\n",
      "Step: 90, Subject: sub4; Training Loss: 0.785190224647522, Val Loss: 0.8568041920661926\n",
      "Step: 90, Subject: sub5; Training Loss: 0.6516273021697998, Val Loss: 0.7542881369590759\n",
      "Step: 90, Subject: sub7; Training Loss: 0.5721441507339478, Val Loss: 0.7178774476051331\n",
      "Step: 100, Subject: sub1; Training Loss: 0.7730873823165894, Val Loss: 0.7923474311828613\n",
      "Step: 100, Subject: sub2; Training Loss: 0.7329984903335571, Val Loss: 0.805027961730957\n",
      "Step: 100, Subject: sub3; Training Loss: 0.7901851534843445, Val Loss: 0.7832781076431274\n",
      "Step: 100, Subject: sub4; Training Loss: 0.8065330982208252, Val Loss: 0.8855748176574707\n",
      "Step: 100, Subject: sub5; Training Loss: 0.6532655358314514, Val Loss: 0.7976922988891602\n",
      "Step: 100, Subject: sub7; Training Loss: 0.607380747795105, Val Loss: 0.7958605289459229\n",
      "Step: 110, Subject: sub1; Training Loss: 0.7553356885910034, Val Loss: 0.8324688673019409\n",
      "Step: 110, Subject: sub2; Training Loss: 0.7270669341087341, Val Loss: 0.9328246116638184\n",
      "Step: 110, Subject: sub3; Training Loss: 0.8563655614852905, Val Loss: 0.9001843929290771\n",
      "Step: 110, Subject: sub4; Training Loss: 0.7452816963195801, Val Loss: 0.9069836139678955\n",
      "Step: 110, Subject: sub5; Training Loss: 0.6666470170021057, Val Loss: 0.7537262439727783\n",
      "Step: 110, Subject: sub7; Training Loss: 0.6000634431838989, Val Loss: 0.7833739519119263\n",
      "Step: 120, Subject: sub1; Training Loss: 0.7831648588180542, Val Loss: 0.8064658641815186\n",
      "Step: 120, Subject: sub2; Training Loss: 0.7909942269325256, Val Loss: 0.8300653696060181\n",
      "Step: 120, Subject: sub3; Training Loss: 0.7250024080276489, Val Loss: 0.8399685621261597\n",
      "Step: 120, Subject: sub4; Training Loss: 0.8075979948043823, Val Loss: 0.8485417366027832\n",
      "Step: 120, Subject: sub5; Training Loss: 0.6762220859527588, Val Loss: 0.7350925207138062\n",
      "Step: 120, Subject: sub7; Training Loss: 0.6244106292724609, Val Loss: 0.7103327512741089\n",
      "Step: 130, Subject: sub1; Training Loss: 0.7937930822372437, Val Loss: 0.7933961153030396\n",
      "Step: 130, Subject: sub2; Training Loss: 0.7070351243019104, Val Loss: 0.8377083539962769\n",
      "Step: 130, Subject: sub3; Training Loss: 0.7070902585983276, Val Loss: 0.8338186740875244\n",
      "Step: 130, Subject: sub4; Training Loss: 0.7599010467529297, Val Loss: 0.9333425760269165\n",
      "Step: 130, Subject: sub5; Training Loss: 0.6793419718742371, Val Loss: 0.766903281211853\n",
      "Step: 130, Subject: sub7; Training Loss: 0.5999516844749451, Val Loss: 0.7470974326133728\n",
      "Step: 140, Subject: sub1; Training Loss: 0.8021961450576782, Val Loss: 0.8417787551879883\n",
      "Step: 140, Subject: sub2; Training Loss: 0.7279238700866699, Val Loss: 0.8979268670082092\n",
      "Step: 140, Subject: sub3; Training Loss: 0.7253389954566956, Val Loss: 0.8266379833221436\n",
      "Step: 140, Subject: sub4; Training Loss: 0.8184018135070801, Val Loss: 0.8824668526649475\n",
      "Step: 140, Subject: sub5; Training Loss: 0.6756008863449097, Val Loss: 0.7070515751838684\n",
      "Step: 140, Subject: sub7; Training Loss: 0.6177387237548828, Val Loss: 0.7328971028327942\n",
      "Step: 150, Subject: sub1; Training Loss: 0.8068124055862427, Val Loss: 0.7919854521751404\n",
      "Step: 150, Subject: sub2; Training Loss: 0.7401216626167297, Val Loss: 0.840453028678894\n",
      "Step: 150, Subject: sub3; Training Loss: 0.7880709171295166, Val Loss: 0.8291099071502686\n",
      "Step: 150, Subject: sub4; Training Loss: 0.7752431035041809, Val Loss: 0.8693503737449646\n",
      "Step: 150, Subject: sub5; Training Loss: 0.6649143695831299, Val Loss: 0.7768862843513489\n",
      "Step: 150, Subject: sub7; Training Loss: 0.5592269897460938, Val Loss: 0.7615799903869629\n",
      "Step: 160, Subject: sub1; Training Loss: 0.7440464496612549, Val Loss: 0.767109751701355\n",
      "Step: 160, Subject: sub2; Training Loss: 0.7230573892593384, Val Loss: 0.7900349497795105\n",
      "Step: 160, Subject: sub3; Training Loss: 0.7527852058410645, Val Loss: 0.78340083360672\n",
      "Step: 160, Subject: sub4; Training Loss: 0.8041160106658936, Val Loss: 0.8383837938308716\n",
      "Step: 160, Subject: sub5; Training Loss: 0.6599467396736145, Val Loss: 0.7023906707763672\n",
      "Step: 160, Subject: sub7; Training Loss: 0.5587964653968811, Val Loss: 0.7542226314544678\n",
      "Step: 170, Subject: sub1; Training Loss: 0.777072012424469, Val Loss: 0.8282278776168823\n",
      "Step: 170, Subject: sub2; Training Loss: 0.7163851261138916, Val Loss: 0.8387433290481567\n",
      "Step: 170, Subject: sub3; Training Loss: 0.7713719606399536, Val Loss: 0.8315587043762207\n",
      "Step: 170, Subject: sub4; Training Loss: 0.8171144127845764, Val Loss: 0.8295564651489258\n",
      "Step: 170, Subject: sub5; Training Loss: 0.6409311294555664, Val Loss: 0.7050340175628662\n",
      "Step: 170, Subject: sub7; Training Loss: 0.6398007273674011, Val Loss: 0.7150120735168457\n",
      "Step: 180, Subject: sub1; Training Loss: 0.786811113357544, Val Loss: 0.8542954921722412\n",
      "Step: 180, Subject: sub2; Training Loss: 0.7126621603965759, Val Loss: 0.8733232021331787\n",
      "Step: 180, Subject: sub3; Training Loss: 0.742369532585144, Val Loss: 0.8739287257194519\n",
      "Step: 180, Subject: sub4; Training Loss: 0.7838469743728638, Val Loss: 0.8455842733383179\n",
      "Step: 180, Subject: sub5; Training Loss: 0.6380164623260498, Val Loss: 0.7737723588943481\n",
      "Step: 180, Subject: sub7; Training Loss: 0.5898918509483337, Val Loss: 0.7798172235488892\n",
      "Step: 190, Subject: sub1; Training Loss: 0.7739681601524353, Val Loss: 0.7623217701911926\n",
      "Step: 190, Subject: sub2; Training Loss: 0.7222367525100708, Val Loss: 0.8255628347396851\n",
      "Step: 190, Subject: sub3; Training Loss: 0.7113916277885437, Val Loss: 0.791803240776062\n",
      "Step: 190, Subject: sub4; Training Loss: 0.7898690104484558, Val Loss: 0.8814413547515869\n",
      "Step: 190, Subject: sub5; Training Loss: 0.6447980403900146, Val Loss: 0.7689555883407593\n",
      "Step: 190, Subject: sub7; Training Loss: 0.5812940001487732, Val Loss: 0.7278722524642944\n",
      "Step: 200, Subject: sub1; Training Loss: 0.8016083240509033, Val Loss: 0.8755073547363281\n",
      "Step: 200, Subject: sub2; Training Loss: 0.7243005633354187, Val Loss: 0.8204936981201172\n",
      "Step: 200, Subject: sub3; Training Loss: 0.7215307950973511, Val Loss: 0.8339492082595825\n",
      "Step: 200, Subject: sub4; Training Loss: 0.7621644139289856, Val Loss: 0.8616656064987183\n",
      "Step: 200, Subject: sub5; Training Loss: 0.6564934253692627, Val Loss: 0.7577581405639648\n",
      "Step: 200, Subject: sub7; Training Loss: 0.5764064192771912, Val Loss: 0.807953953742981\n",
      "Step: 210, Subject: sub1; Training Loss: 0.825717031955719, Val Loss: 0.7864886522293091\n",
      "Step: 210, Subject: sub2; Training Loss: 0.6654372811317444, Val Loss: 0.8948886394500732\n",
      "Step: 210, Subject: sub3; Training Loss: 0.780504584312439, Val Loss: 0.8067253828048706\n",
      "Step: 210, Subject: sub4; Training Loss: 0.7868558168411255, Val Loss: 0.8850785493850708\n",
      "Step: 210, Subject: sub5; Training Loss: 0.6757922172546387, Val Loss: 0.6956865191459656\n",
      "Step: 210, Subject: sub7; Training Loss: 0.6301053762435913, Val Loss: 0.7559804916381836\n",
      "Step: 220, Subject: sub1; Training Loss: 0.7978712320327759, Val Loss: 0.8213111162185669\n",
      "Step: 220, Subject: sub2; Training Loss: 0.736424446105957, Val Loss: 0.8176083564758301\n",
      "Step: 220, Subject: sub3; Training Loss: 0.768341600894928, Val Loss: 0.8122833371162415\n",
      "Step: 220, Subject: sub4; Training Loss: 0.7422595024108887, Val Loss: 0.8933533430099487\n",
      "Step: 220, Subject: sub5; Training Loss: 0.6222994923591614, Val Loss: 0.6988397836685181\n",
      "Step: 220, Subject: sub7; Training Loss: 0.5759127140045166, Val Loss: 0.717848539352417\n",
      "Step: 230, Subject: sub1; Training Loss: 0.8027430176734924, Val Loss: 0.785086989402771\n",
      "Step: 230, Subject: sub2; Training Loss: 0.6675581932067871, Val Loss: 0.8668129444122314\n",
      "Step: 230, Subject: sub3; Training Loss: 0.7473840713500977, Val Loss: 0.864315927028656\n",
      "Step: 230, Subject: sub4; Training Loss: 0.7563703060150146, Val Loss: 0.8442646265029907\n",
      "Step: 230, Subject: sub5; Training Loss: 0.6340121030807495, Val Loss: 0.7542698383331299\n",
      "Step: 230, Subject: sub7; Training Loss: 0.621382474899292, Val Loss: 0.7462427020072937\n",
      "Step: 240, Subject: sub1; Training Loss: 0.7637563943862915, Val Loss: 0.8644517064094543\n",
      "Step: 240, Subject: sub2; Training Loss: 0.6759670972824097, Val Loss: 0.8377923965454102\n",
      "Step: 240, Subject: sub3; Training Loss: 0.7399537563323975, Val Loss: 0.858788013458252\n",
      "Step: 240, Subject: sub4; Training Loss: 0.7711999416351318, Val Loss: 0.857363760471344\n",
      "Step: 240, Subject: sub5; Training Loss: 0.6419847011566162, Val Loss: 0.7181435823440552\n",
      "Step: 240, Subject: sub7; Training Loss: 0.6206614971160889, Val Loss: 0.7454109191894531\n",
      "Step: 250, Subject: sub1; Training Loss: 0.7882322072982788, Val Loss: 0.8252328634262085\n",
      "Step: 250, Subject: sub2; Training Loss: 0.6963454484939575, Val Loss: 0.8234113454818726\n",
      "Step: 250, Subject: sub3; Training Loss: 0.7726130485534668, Val Loss: 0.8400981426239014\n",
      "Step: 250, Subject: sub4; Training Loss: 0.7330102920532227, Val Loss: 0.8757886290550232\n",
      "Step: 250, Subject: sub5; Training Loss: 0.6660127639770508, Val Loss: 0.7423815727233887\n",
      "Step: 250, Subject: sub7; Training Loss: 0.5856432914733887, Val Loss: 0.705432653427124\n",
      "Step: 260, Subject: sub1; Training Loss: 0.827406644821167, Val Loss: 0.8039751052856445\n",
      "Step: 260, Subject: sub2; Training Loss: 0.7056307792663574, Val Loss: 0.8834282755851746\n",
      "Step: 260, Subject: sub3; Training Loss: 0.7700320482254028, Val Loss: 0.8319100737571716\n",
      "Step: 260, Subject: sub4; Training Loss: 0.805974543094635, Val Loss: 0.8443741798400879\n",
      "Step: 260, Subject: sub5; Training Loss: 0.6889053583145142, Val Loss: 0.7331218719482422\n",
      "Step: 260, Subject: sub7; Training Loss: 0.5933634042739868, Val Loss: 0.7689507007598877\n",
      "Step: 270, Subject: sub1; Training Loss: 0.7206918001174927, Val Loss: 0.8053377866744995\n",
      "Step: 270, Subject: sub2; Training Loss: 0.7110127210617065, Val Loss: 0.8506976962089539\n",
      "Step: 270, Subject: sub3; Training Loss: 0.8740079402923584, Val Loss: 0.7644497156143188\n",
      "Step: 270, Subject: sub4; Training Loss: 0.7895336747169495, Val Loss: 0.8949638605117798\n",
      "Step: 270, Subject: sub5; Training Loss: 0.6571570634841919, Val Loss: 0.7185466289520264\n",
      "Step: 270, Subject: sub7; Training Loss: 0.6408612132072449, Val Loss: 0.7535606622695923\n",
      "Step: 280, Subject: sub1; Training Loss: 0.793028712272644, Val Loss: 0.8265780210494995\n",
      "Step: 280, Subject: sub2; Training Loss: 0.774909496307373, Val Loss: 0.8607687950134277\n",
      "Step: 280, Subject: sub3; Training Loss: 0.7130371332168579, Val Loss: 0.8296932578086853\n",
      "Step: 280, Subject: sub4; Training Loss: 0.782650351524353, Val Loss: 0.8632886409759521\n",
      "Step: 280, Subject: sub5; Training Loss: 0.6574146747589111, Val Loss: 0.7164819240570068\n",
      "Step: 280, Subject: sub7; Training Loss: 0.5939934253692627, Val Loss: 0.786246657371521\n",
      "Step: 290, Subject: sub1; Training Loss: 0.7928727865219116, Val Loss: 0.8244911432266235\n",
      "Step: 290, Subject: sub2; Training Loss: 0.6920065879821777, Val Loss: 0.8350859880447388\n",
      "Step: 290, Subject: sub3; Training Loss: 0.7377129793167114, Val Loss: 0.8140296339988708\n",
      "Step: 290, Subject: sub4; Training Loss: 0.7416343688964844, Val Loss: 0.8839340209960938\n",
      "Step: 290, Subject: sub5; Training Loss: 0.6647149324417114, Val Loss: 0.7678883075714111\n",
      "Step: 290, Subject: sub7; Training Loss: 0.6240300536155701, Val Loss: 0.6789608001708984\n",
      "Saving model with validation loss: 0.6789608001708984\n",
      "\n",
      "Step: 300, Subject: sub1; Training Loss: 0.8327894806861877, Val Loss: 0.7816181778907776\n",
      "Step: 300, Subject: sub2; Training Loss: 0.7760510444641113, Val Loss: 0.8507227897644043\n",
      "Step: 300, Subject: sub3; Training Loss: 0.6849263906478882, Val Loss: 0.8347704410552979\n",
      "Step: 300, Subject: sub4; Training Loss: 0.827763557434082, Val Loss: 0.8910658359527588\n",
      "Step: 300, Subject: sub5; Training Loss: 0.6552069783210754, Val Loss: 0.7665311098098755\n",
      "Step: 300, Subject: sub7; Training Loss: 0.6023794412612915, Val Loss: 0.7859069108963013\n",
      "Step: 310, Subject: sub1; Training Loss: 0.7630767822265625, Val Loss: 0.8614774942398071\n",
      "Step: 310, Subject: sub2; Training Loss: 0.6797170639038086, Val Loss: 0.9451115131378174\n",
      "Step: 310, Subject: sub3; Training Loss: 0.8249534368515015, Val Loss: 0.8398610353469849\n",
      "Step: 310, Subject: sub4; Training Loss: 0.7760443687438965, Val Loss: 0.8453386425971985\n",
      "Step: 310, Subject: sub5; Training Loss: 0.6798187494277954, Val Loss: 0.7793941497802734\n",
      "Step: 310, Subject: sub7; Training Loss: 0.6177387237548828, Val Loss: 0.7989570498466492\n",
      "Step: 320, Subject: sub1; Training Loss: 0.7835360169410706, Val Loss: 0.839931070804596\n",
      "Step: 320, Subject: sub2; Training Loss: 0.7869136333465576, Val Loss: 0.8268574476242065\n",
      "Step: 320, Subject: sub3; Training Loss: 0.8219863176345825, Val Loss: 0.8362395167350769\n",
      "Step: 320, Subject: sub4; Training Loss: 0.7971352338790894, Val Loss: 0.8718630075454712\n",
      "Step: 320, Subject: sub5; Training Loss: 0.696098268032074, Val Loss: 0.7751401662826538\n",
      "Step: 320, Subject: sub7; Training Loss: 0.6445215940475464, Val Loss: 0.7407181262969971\n",
      "Step: 330, Subject: sub1; Training Loss: 0.7783533334732056, Val Loss: 0.8270893692970276\n",
      "Step: 330, Subject: sub2; Training Loss: 0.796176016330719, Val Loss: 0.8223822712898254\n",
      "Step: 330, Subject: sub3; Training Loss: 0.7387872934341431, Val Loss: 0.8588280081748962\n",
      "Step: 330, Subject: sub4; Training Loss: 0.7864058017730713, Val Loss: 0.8401519656181335\n",
      "Step: 330, Subject: sub5; Training Loss: 0.6521328687667847, Val Loss: 0.6895980834960938\n",
      "Step: 330, Subject: sub7; Training Loss: 0.5994924306869507, Val Loss: 0.7296414375305176\n",
      "Step: 340, Subject: sub1; Training Loss: 0.8299726843833923, Val Loss: 0.857297956943512\n",
      "Step: 340, Subject: sub2; Training Loss: 0.7020319700241089, Val Loss: 0.8786987662315369\n",
      "Step: 340, Subject: sub3; Training Loss: 0.6900925636291504, Val Loss: 0.8443827033042908\n",
      "Step: 340, Subject: sub4; Training Loss: 0.8464727401733398, Val Loss: 0.8820407390594482\n",
      "Step: 340, Subject: sub5; Training Loss: 0.6617392301559448, Val Loss: 0.7287671566009521\n",
      "Step: 340, Subject: sub7; Training Loss: 0.5527833700180054, Val Loss: 0.7949666976928711\n",
      "Step: 350, Subject: sub1; Training Loss: 0.8066685795783997, Val Loss: 0.8283776640892029\n",
      "Step: 350, Subject: sub2; Training Loss: 0.7403829097747803, Val Loss: 0.8422670364379883\n",
      "Step: 350, Subject: sub3; Training Loss: 0.8036525845527649, Val Loss: 0.8041440844535828\n",
      "Step: 350, Subject: sub4; Training Loss: 0.7341755628585815, Val Loss: 0.855844259262085\n",
      "Step: 350, Subject: sub5; Training Loss: 0.6647790670394897, Val Loss: 0.696743905544281\n",
      "Step: 350, Subject: sub7; Training Loss: 0.6399730443954468, Val Loss: 0.7623998522758484\n",
      "Step: 360, Subject: sub1; Training Loss: 0.8037181496620178, Val Loss: 0.8054674863815308\n",
      "Step: 360, Subject: sub2; Training Loss: 0.7481212019920349, Val Loss: 0.8539841175079346\n",
      "Step: 360, Subject: sub3; Training Loss: 0.709755003452301, Val Loss: 0.826298713684082\n",
      "Step: 360, Subject: sub4; Training Loss: 0.7731688022613525, Val Loss: 0.9134904146194458\n",
      "Step: 360, Subject: sub5; Training Loss: 0.6542311310768127, Val Loss: 0.698357880115509\n",
      "Step: 360, Subject: sub7; Training Loss: 0.5715277791023254, Val Loss: 0.738306999206543\n",
      "Step: 370, Subject: sub1; Training Loss: 0.7939176559448242, Val Loss: 0.8241935968399048\n",
      "Step: 370, Subject: sub2; Training Loss: 0.7213782668113708, Val Loss: 0.8020590543746948\n",
      "Step: 370, Subject: sub3; Training Loss: 0.7335569858551025, Val Loss: 0.7794593572616577\n",
      "Step: 370, Subject: sub4; Training Loss: 0.8201503753662109, Val Loss: 0.8561296463012695\n",
      "Step: 370, Subject: sub5; Training Loss: 0.6353007555007935, Val Loss: 0.7447988390922546\n",
      "Step: 370, Subject: sub7; Training Loss: 0.6117715239524841, Val Loss: 0.7340124845504761\n",
      "Step: 380, Subject: sub1; Training Loss: 0.7952111959457397, Val Loss: 0.8346840143203735\n",
      "Step: 380, Subject: sub2; Training Loss: 0.7672742605209351, Val Loss: 0.8866962790489197\n",
      "Step: 380, Subject: sub3; Training Loss: 0.7605924606323242, Val Loss: 0.8310845494270325\n",
      "Step: 380, Subject: sub4; Training Loss: 0.7709892988204956, Val Loss: 0.8833580017089844\n",
      "Step: 380, Subject: sub5; Training Loss: 0.6788424253463745, Val Loss: 0.727785050868988\n",
      "Step: 380, Subject: sub7; Training Loss: 0.5704405307769775, Val Loss: 0.7842936515808105\n",
      "Step: 390, Subject: sub1; Training Loss: 0.7930409908294678, Val Loss: 0.7996574640274048\n",
      "Step: 390, Subject: sub2; Training Loss: 0.7397728562355042, Val Loss: 0.9002407193183899\n",
      "Step: 390, Subject: sub3; Training Loss: 0.7577749490737915, Val Loss: 0.8221904039382935\n",
      "Step: 390, Subject: sub4; Training Loss: 0.8006311655044556, Val Loss: 0.8529412746429443\n",
      "Step: 390, Subject: sub5; Training Loss: 0.6846510171890259, Val Loss: 0.7099182605743408\n",
      "Step: 390, Subject: sub7; Training Loss: 0.6128415465354919, Val Loss: 0.7743428945541382\n",
      "Step: 400, Subject: sub1; Training Loss: 0.8290730714797974, Val Loss: 0.8478214740753174\n",
      "Step: 400, Subject: sub2; Training Loss: 0.7169873714447021, Val Loss: 0.8892276287078857\n",
      "Step: 400, Subject: sub3; Training Loss: 0.786340594291687, Val Loss: 0.7854059934616089\n",
      "Step: 400, Subject: sub4; Training Loss: 0.817272424697876, Val Loss: 0.8849019408226013\n",
      "Step: 400, Subject: sub5; Training Loss: 0.6650228500366211, Val Loss: 0.6718540191650391\n",
      "Saving model with validation loss: 0.6718540191650391\n",
      "\n",
      "Step: 400, Subject: sub7; Training Loss: 0.619745135307312, Val Loss: 0.7579313516616821\n",
      "Step: 410, Subject: sub1; Training Loss: 0.7189131379127502, Val Loss: 0.7581514120101929\n",
      "Step: 410, Subject: sub2; Training Loss: 0.7528557777404785, Val Loss: 0.8990176916122437\n",
      "Step: 410, Subject: sub3; Training Loss: 0.7508490681648254, Val Loss: 0.8567606806755066\n",
      "Step: 410, Subject: sub4; Training Loss: 0.8005171418190002, Val Loss: 0.8583406209945679\n",
      "Step: 410, Subject: sub5; Training Loss: 0.689395546913147, Val Loss: 0.7187091112136841\n",
      "Step: 410, Subject: sub7; Training Loss: 0.5755093097686768, Val Loss: 0.7212089896202087\n",
      "Step: 420, Subject: sub1; Training Loss: 0.7516847848892212, Val Loss: 0.7918220162391663\n",
      "Step: 420, Subject: sub2; Training Loss: 0.704877495765686, Val Loss: 0.8434409499168396\n",
      "Step: 420, Subject: sub3; Training Loss: 0.7587838172912598, Val Loss: 0.8491889238357544\n",
      "Step: 420, Subject: sub4; Training Loss: 0.7786611318588257, Val Loss: 0.8607926964759827\n",
      "Step: 420, Subject: sub5; Training Loss: 0.6377134323120117, Val Loss: 0.7177648544311523\n",
      "Step: 420, Subject: sub7; Training Loss: 0.638092041015625, Val Loss: 0.7670694589614868\n",
      "Step: 430, Subject: sub1; Training Loss: 0.7768985033035278, Val Loss: 0.8049651980400085\n",
      "Step: 430, Subject: sub2; Training Loss: 0.7200113534927368, Val Loss: 0.8515169024467468\n",
      "Step: 430, Subject: sub3; Training Loss: 0.8062427043914795, Val Loss: 0.8525554537773132\n",
      "Step: 430, Subject: sub4; Training Loss: 0.7952178716659546, Val Loss: 0.8456404209136963\n",
      "Step: 430, Subject: sub5; Training Loss: 0.6740816831588745, Val Loss: 0.7829812169075012\n",
      "Step: 430, Subject: sub7; Training Loss: 0.5890409350395203, Val Loss: 0.7316650152206421\n",
      "Step: 440, Subject: sub1; Training Loss: 0.752501368522644, Val Loss: 0.8128786087036133\n",
      "Step: 440, Subject: sub2; Training Loss: 0.7338085174560547, Val Loss: 0.8871463537216187\n",
      "Step: 440, Subject: sub3; Training Loss: 0.7352966070175171, Val Loss: 0.8219401836395264\n",
      "Step: 440, Subject: sub4; Training Loss: 0.768768310546875, Val Loss: 0.873323917388916\n",
      "Step: 440, Subject: sub5; Training Loss: 0.6434631943702698, Val Loss: 0.7931679487228394\n",
      "Step: 440, Subject: sub7; Training Loss: 0.613891065120697, Val Loss: 0.7679471969604492\n",
      "Step: 450, Subject: sub1; Training Loss: 0.8127569556236267, Val Loss: 0.8666149377822876\n",
      "Step: 450, Subject: sub2; Training Loss: 0.6998031735420227, Val Loss: 0.8894377946853638\n",
      "Step: 450, Subject: sub3; Training Loss: 0.7436203956604004, Val Loss: 0.8656463623046875\n",
      "Step: 450, Subject: sub4; Training Loss: 0.827564537525177, Val Loss: 0.8327436447143555\n",
      "Step: 450, Subject: sub5; Training Loss: 0.6489356160163879, Val Loss: 0.688010573387146\n",
      "Step: 450, Subject: sub7; Training Loss: 0.5819209218025208, Val Loss: 0.6962663531303406\n",
      "Step: 460, Subject: sub1; Training Loss: 0.7758418321609497, Val Loss: 0.8097123503684998\n",
      "Step: 460, Subject: sub2; Training Loss: 0.7165321707725525, Val Loss: 0.857313334941864\n",
      "Step: 460, Subject: sub3; Training Loss: 0.7750794887542725, Val Loss: 0.8478331565856934\n",
      "Step: 460, Subject: sub4; Training Loss: 0.7447516918182373, Val Loss: 0.8257063031196594\n",
      "Step: 460, Subject: sub5; Training Loss: 0.6258125901222229, Val Loss: 0.7631680369377136\n",
      "Step: 460, Subject: sub7; Training Loss: 0.6157549023628235, Val Loss: 0.7500736117362976\n",
      "Step: 470, Subject: sub1; Training Loss: 0.7740543484687805, Val Loss: 0.7320578098297119\n",
      "Step: 470, Subject: sub2; Training Loss: 0.7071170210838318, Val Loss: 0.8509703874588013\n",
      "Step: 470, Subject: sub3; Training Loss: 0.8073015213012695, Val Loss: 0.8160057663917542\n",
      "Step: 470, Subject: sub4; Training Loss: 0.8404443264007568, Val Loss: 0.87758469581604\n",
      "Step: 470, Subject: sub5; Training Loss: 0.6574835777282715, Val Loss: 0.7219155430793762\n",
      "Step: 470, Subject: sub7; Training Loss: 0.5917458534240723, Val Loss: 0.7711645364761353\n",
      "Step: 480, Subject: sub1; Training Loss: 0.7741411328315735, Val Loss: 0.8533462285995483\n",
      "Step: 480, Subject: sub2; Training Loss: 0.715807318687439, Val Loss: 0.8233180046081543\n",
      "Step: 480, Subject: sub3; Training Loss: 0.7021753787994385, Val Loss: 0.858721137046814\n",
      "Step: 480, Subject: sub4; Training Loss: 0.7636936902999878, Val Loss: 0.8567083477973938\n",
      "Step: 480, Subject: sub5; Training Loss: 0.6812624931335449, Val Loss: 0.7863690853118896\n",
      "Step: 480, Subject: sub7; Training Loss: 0.571603536605835, Val Loss: 0.768330454826355\n",
      "Step: 490, Subject: sub1; Training Loss: 0.8020798563957214, Val Loss: 0.826997697353363\n",
      "Step: 490, Subject: sub2; Training Loss: 0.6804933547973633, Val Loss: 0.9058279395103455\n",
      "Step: 490, Subject: sub3; Training Loss: 0.799676775932312, Val Loss: 0.8297735452651978\n",
      "Step: 490, Subject: sub4; Training Loss: 0.8286123871803284, Val Loss: 0.8674572706222534\n",
      "Step: 490, Subject: sub5; Training Loss: 0.6405429244041443, Val Loss: 0.7042571902275085\n",
      "Step: 490, Subject: sub7; Training Loss: 0.6107286214828491, Val Loss: 0.755062997341156\n",
      "Step: 500, Subject: sub1; Training Loss: 0.7266253232955933, Val Loss: 0.7399246692657471\n",
      "Step: 500, Subject: sub2; Training Loss: 0.7194349765777588, Val Loss: 0.8485659956932068\n",
      "Step: 500, Subject: sub3; Training Loss: 0.700926661491394, Val Loss: 0.864872395992279\n",
      "Step: 500, Subject: sub4; Training Loss: 0.7832657694816589, Val Loss: 0.8455643057823181\n",
      "Step: 500, Subject: sub5; Training Loss: 0.6385641098022461, Val Loss: 0.7793174386024475\n",
      "Step: 500, Subject: sub7; Training Loss: 0.5714497566223145, Val Loss: 0.7569278478622437\n",
      "Step: 510, Subject: sub1; Training Loss: 0.8013464212417603, Val Loss: 0.7647758722305298\n",
      "Step: 510, Subject: sub2; Training Loss: 0.7171392440795898, Val Loss: 0.8275090456008911\n",
      "Step: 510, Subject: sub3; Training Loss: 0.8005462288856506, Val Loss: 0.8454705476760864\n",
      "Step: 510, Subject: sub4; Training Loss: 0.775791585445404, Val Loss: 0.8577384948730469\n",
      "Step: 510, Subject: sub5; Training Loss: 0.6576700210571289, Val Loss: 0.712767481803894\n",
      "Step: 510, Subject: sub7; Training Loss: 0.6003412008285522, Val Loss: 0.7532491087913513\n",
      "Step: 520, Subject: sub1; Training Loss: 0.7991331219673157, Val Loss: 0.8024259805679321\n",
      "Step: 520, Subject: sub2; Training Loss: 0.7527596950531006, Val Loss: 0.7938501834869385\n",
      "Step: 520, Subject: sub3; Training Loss: 0.7168687582015991, Val Loss: 0.7990329265594482\n",
      "Step: 520, Subject: sub4; Training Loss: 0.7729315757751465, Val Loss: 0.8607779145240784\n",
      "Step: 520, Subject: sub5; Training Loss: 0.6928743124008179, Val Loss: 0.7047221064567566\n",
      "Step: 520, Subject: sub7; Training Loss: 0.574836015701294, Val Loss: 0.6981425285339355\n",
      "Step: 530, Subject: sub1; Training Loss: 0.6947695016860962, Val Loss: 0.7560174465179443\n",
      "Step: 530, Subject: sub2; Training Loss: 0.6854543685913086, Val Loss: 0.8832652568817139\n",
      "Step: 530, Subject: sub3; Training Loss: 0.7833632230758667, Val Loss: 0.8255282640457153\n",
      "Step: 530, Subject: sub4; Training Loss: 0.8050034642219543, Val Loss: 0.8579339385032654\n",
      "Step: 530, Subject: sub5; Training Loss: 0.6609616875648499, Val Loss: 0.6754652261734009\n",
      "Step: 530, Subject: sub7; Training Loss: 0.6185513138771057, Val Loss: 0.7375975847244263\n",
      "Step: 540, Subject: sub1; Training Loss: 0.8022547960281372, Val Loss: 0.8359423279762268\n",
      "Step: 540, Subject: sub2; Training Loss: 0.6583176851272583, Val Loss: 0.7976797819137573\n",
      "Step: 540, Subject: sub3; Training Loss: 0.7456685900688171, Val Loss: 0.8491260409355164\n",
      "Step: 540, Subject: sub4; Training Loss: 0.7896040081977844, Val Loss: 0.8579679727554321\n",
      "Step: 540, Subject: sub5; Training Loss: 0.6718953251838684, Val Loss: 0.7157589197158813\n",
      "Step: 540, Subject: sub7; Training Loss: 0.5847669243812561, Val Loss: 0.7495867013931274\n",
      "Step: 550, Subject: sub1; Training Loss: 0.7414149045944214, Val Loss: 0.795150101184845\n",
      "Step: 550, Subject: sub2; Training Loss: 0.6836570501327515, Val Loss: 0.8714965581893921\n",
      "Step: 550, Subject: sub3; Training Loss: 0.7871954441070557, Val Loss: 0.8228709697723389\n",
      "Step: 550, Subject: sub4; Training Loss: 0.752379834651947, Val Loss: 0.8993977904319763\n",
      "Step: 550, Subject: sub5; Training Loss: 0.6522115468978882, Val Loss: 0.7495860457420349\n",
      "Step: 550, Subject: sub7; Training Loss: 0.5810838937759399, Val Loss: 0.710161566734314\n",
      "Step: 560, Subject: sub1; Training Loss: 0.7423442602157593, Val Loss: 0.8581335544586182\n",
      "Step: 560, Subject: sub2; Training Loss: 0.7514420747756958, Val Loss: 0.8672476410865784\n",
      "Step: 560, Subject: sub3; Training Loss: 0.7441240549087524, Val Loss: 0.7301074266433716\n",
      "Step: 560, Subject: sub4; Training Loss: 0.7743356823921204, Val Loss: 0.8999025821685791\n",
      "Step: 560, Subject: sub5; Training Loss: 0.687602698802948, Val Loss: 0.7397294044494629\n",
      "Step: 560, Subject: sub7; Training Loss: 0.6185728311538696, Val Loss: 0.7857081890106201\n",
      "Step: 570, Subject: sub1; Training Loss: 0.7623550891876221, Val Loss: 0.7758126258850098\n",
      "Step: 570, Subject: sub2; Training Loss: 0.7191915512084961, Val Loss: 0.8378966450691223\n",
      "Step: 570, Subject: sub3; Training Loss: 0.815129816532135, Val Loss: 0.8574754595756531\n",
      "Step: 570, Subject: sub4; Training Loss: 0.7893526554107666, Val Loss: 0.8312950134277344\n",
      "Step: 570, Subject: sub5; Training Loss: 0.6751143932342529, Val Loss: 0.7615838050842285\n",
      "Step: 570, Subject: sub7; Training Loss: 0.6320762038230896, Val Loss: 0.7503169178962708\n",
      "Step: 580, Subject: sub1; Training Loss: 0.8319196701049805, Val Loss: 0.8467065691947937\n",
      "Step: 580, Subject: sub2; Training Loss: 0.7138682007789612, Val Loss: 0.8032383322715759\n",
      "Step: 580, Subject: sub3; Training Loss: 0.8070359230041504, Val Loss: 0.8038989901542664\n",
      "Step: 580, Subject: sub4; Training Loss: 0.8200967907905579, Val Loss: 0.8329540491104126\n",
      "Step: 580, Subject: sub5; Training Loss: 0.6576250195503235, Val Loss: 0.757794201374054\n",
      "Step: 580, Subject: sub7; Training Loss: 0.6352752447128296, Val Loss: 0.735897421836853\n",
      "Step: 590, Subject: sub1; Training Loss: 0.785551905632019, Val Loss: 0.7640992403030396\n",
      "Step: 590, Subject: sub2; Training Loss: 0.7339556813240051, Val Loss: 0.8353378176689148\n",
      "Step: 590, Subject: sub3; Training Loss: 0.786763072013855, Val Loss: 0.8180882334709167\n",
      "Step: 590, Subject: sub4; Training Loss: 0.7655547261238098, Val Loss: 0.8544602990150452\n",
      "Step: 590, Subject: sub5; Training Loss: 0.6559954881668091, Val Loss: 0.7622238397598267\n",
      "Step: 590, Subject: sub7; Training Loss: 0.6692241430282593, Val Loss: 0.7011366486549377\n",
      "Step: 600, Subject: sub1; Training Loss: 0.7724606990814209, Val Loss: 0.779554545879364\n",
      "Step: 600, Subject: sub2; Training Loss: 0.724777102470398, Val Loss: 0.8692489862442017\n",
      "Step: 600, Subject: sub3; Training Loss: 0.7539093494415283, Val Loss: 0.8263424634933472\n",
      "Step: 600, Subject: sub4; Training Loss: 0.7543157339096069, Val Loss: 0.8273504972457886\n",
      "Step: 600, Subject: sub5; Training Loss: 0.6360276937484741, Val Loss: 0.7963458299636841\n",
      "Step: 600, Subject: sub7; Training Loss: 0.6012201309204102, Val Loss: 0.7385348081588745\n",
      "Step: 610, Subject: sub1; Training Loss: 0.8154556751251221, Val Loss: 0.8311416506767273\n",
      "Step: 610, Subject: sub2; Training Loss: 0.777244508266449, Val Loss: 0.8471753597259521\n",
      "Step: 610, Subject: sub3; Training Loss: 0.7581524848937988, Val Loss: 0.8423557877540588\n",
      "Step: 610, Subject: sub4; Training Loss: 0.7361472845077515, Val Loss: 0.860933244228363\n",
      "Step: 610, Subject: sub5; Training Loss: 0.6920149326324463, Val Loss: 0.7282474040985107\n",
      "Step: 610, Subject: sub7; Training Loss: 0.6198232769966125, Val Loss: 0.764867901802063\n",
      "Step: 620, Subject: sub1; Training Loss: 0.8077470064163208, Val Loss: 0.8019305467605591\n",
      "Step: 620, Subject: sub2; Training Loss: 0.6609411835670471, Val Loss: 0.8876094222068787\n",
      "Step: 620, Subject: sub3; Training Loss: 0.7921162247657776, Val Loss: 0.8247242569923401\n",
      "Step: 620, Subject: sub4; Training Loss: 0.7894571423530579, Val Loss: 0.840346097946167\n",
      "Step: 620, Subject: sub5; Training Loss: 0.6520146131515503, Val Loss: 0.737089991569519\n",
      "Step: 620, Subject: sub7; Training Loss: 0.5780818462371826, Val Loss: 0.7248169183731079\n",
      "Step: 630, Subject: sub1; Training Loss: 0.7640773057937622, Val Loss: 0.8165404796600342\n",
      "Step: 630, Subject: sub2; Training Loss: 0.7025598287582397, Val Loss: 0.8342492580413818\n",
      "Step: 630, Subject: sub3; Training Loss: 0.7009819149971008, Val Loss: 0.7664101123809814\n",
      "Step: 630, Subject: sub4; Training Loss: 0.7425875663757324, Val Loss: 0.8269926905632019\n",
      "Step: 630, Subject: sub5; Training Loss: 0.6871997117996216, Val Loss: 0.6896752119064331\n",
      "Step: 630, Subject: sub7; Training Loss: 0.6298395395278931, Val Loss: 0.7465560436248779\n",
      "Step: 640, Subject: sub1; Training Loss: 0.772151529788971, Val Loss: 0.7859933972358704\n",
      "Step: 640, Subject: sub2; Training Loss: 0.698881208896637, Val Loss: 0.8500646948814392\n",
      "Step: 640, Subject: sub3; Training Loss: 0.8039913177490234, Val Loss: 0.8145443797111511\n",
      "Step: 640, Subject: sub4; Training Loss: 0.7606743574142456, Val Loss: 0.8758491277694702\n",
      "Step: 640, Subject: sub5; Training Loss: 0.6543428897857666, Val Loss: 0.6716059446334839\n",
      "Saving model with validation loss: 0.6716059446334839\n",
      "\n",
      "Step: 640, Subject: sub7; Training Loss: 0.6708041429519653, Val Loss: 0.746712863445282\n",
      "Step: 650, Subject: sub1; Training Loss: 0.776374340057373, Val Loss: 0.859889030456543\n",
      "Step: 650, Subject: sub2; Training Loss: 0.7222229242324829, Val Loss: 0.8797958493232727\n",
      "Step: 650, Subject: sub3; Training Loss: 0.7071654796600342, Val Loss: 0.7783571481704712\n",
      "Step: 650, Subject: sub4; Training Loss: 0.7628030180931091, Val Loss: 0.8506484031677246\n",
      "Step: 650, Subject: sub5; Training Loss: 0.6790690422058105, Val Loss: 0.7606970071792603\n",
      "Step: 650, Subject: sub7; Training Loss: 0.6274731755256653, Val Loss: 0.7092418670654297\n",
      "Step: 660, Subject: sub1; Training Loss: 0.780749499797821, Val Loss: 0.775714099407196\n",
      "Step: 660, Subject: sub2; Training Loss: 0.7047551274299622, Val Loss: 0.8527601957321167\n",
      "Step: 660, Subject: sub3; Training Loss: 0.7326637506484985, Val Loss: 0.8092118501663208\n",
      "Step: 660, Subject: sub4; Training Loss: 0.7131584286689758, Val Loss: 0.8539059162139893\n",
      "Step: 660, Subject: sub5; Training Loss: 0.6575057506561279, Val Loss: 0.7548806071281433\n",
      "Step: 660, Subject: sub7; Training Loss: 0.6371446847915649, Val Loss: 0.7454213500022888\n",
      "Step: 670, Subject: sub1; Training Loss: 0.8019946217536926, Val Loss: 0.8430922627449036\n",
      "Step: 670, Subject: sub2; Training Loss: 0.6891255974769592, Val Loss: 0.842739462852478\n",
      "Step: 670, Subject: sub3; Training Loss: 0.7452030181884766, Val Loss: 0.8766108751296997\n",
      "Step: 670, Subject: sub4; Training Loss: 0.7429021596908569, Val Loss: 0.892889142036438\n",
      "Step: 670, Subject: sub5; Training Loss: 0.646443784236908, Val Loss: 0.780977725982666\n",
      "Step: 670, Subject: sub7; Training Loss: 0.5697214007377625, Val Loss: 0.7929701805114746\n",
      "Step: 680, Subject: sub1; Training Loss: 0.8462797999382019, Val Loss: 0.7812556028366089\n",
      "Step: 680, Subject: sub2; Training Loss: 0.7102612853050232, Val Loss: 0.8937650918960571\n",
      "Step: 680, Subject: sub3; Training Loss: 0.7595645189285278, Val Loss: 0.8299099206924438\n",
      "Step: 680, Subject: sub4; Training Loss: 0.7803174257278442, Val Loss: 0.9013808965682983\n",
      "Step: 680, Subject: sub5; Training Loss: 0.6542273759841919, Val Loss: 0.6914852261543274\n",
      "Step: 680, Subject: sub7; Training Loss: 0.5951485633850098, Val Loss: 0.7619240283966064\n",
      "Step: 690, Subject: sub1; Training Loss: 0.798866868019104, Val Loss: 0.7960034608840942\n",
      "Step: 690, Subject: sub2; Training Loss: 0.7678208351135254, Val Loss: 0.870599091053009\n",
      "Step: 690, Subject: sub3; Training Loss: 0.8181231021881104, Val Loss: 0.8034600019454956\n",
      "Step: 690, Subject: sub4; Training Loss: 0.7183465361595154, Val Loss: 0.8453369736671448\n",
      "Step: 690, Subject: sub5; Training Loss: 0.6131255030632019, Val Loss: 0.7561485767364502\n",
      "Step: 690, Subject: sub7; Training Loss: 0.6275702118873596, Val Loss: 0.741523802280426\n",
      "Step: 700, Subject: sub1; Training Loss: 0.7767612934112549, Val Loss: 0.8439688682556152\n",
      "Step: 700, Subject: sub2; Training Loss: 0.7008975744247437, Val Loss: 0.835609495639801\n",
      "Step: 700, Subject: sub3; Training Loss: 0.7694422006607056, Val Loss: 0.8473561406135559\n",
      "Step: 700, Subject: sub4; Training Loss: 0.7457494735717773, Val Loss: 0.8569912910461426\n",
      "Step: 700, Subject: sub5; Training Loss: 0.6570538282394409, Val Loss: 0.758538544178009\n",
      "Step: 700, Subject: sub7; Training Loss: 0.5647539496421814, Val Loss: 0.7725038528442383\n",
      "Step: 710, Subject: sub1; Training Loss: 0.7379599809646606, Val Loss: 0.8628278374671936\n",
      "Step: 710, Subject: sub2; Training Loss: 0.7577117085456848, Val Loss: 0.8469032645225525\n",
      "Step: 710, Subject: sub3; Training Loss: 0.7483896017074585, Val Loss: 0.8065276741981506\n",
      "Step: 710, Subject: sub4; Training Loss: 0.8165509104728699, Val Loss: 0.8981956243515015\n",
      "Step: 710, Subject: sub5; Training Loss: 0.6918193101882935, Val Loss: 0.7854691743850708\n",
      "Step: 710, Subject: sub7; Training Loss: 0.5583122968673706, Val Loss: 0.7374680042266846\n",
      "Step: 720, Subject: sub1; Training Loss: 0.7728926539421082, Val Loss: 0.8270974159240723\n",
      "Step: 720, Subject: sub2; Training Loss: 0.6830236315727234, Val Loss: 0.8561882376670837\n",
      "Step: 720, Subject: sub3; Training Loss: 0.7683404684066772, Val Loss: 0.8310623168945312\n",
      "Step: 720, Subject: sub4; Training Loss: 0.7298583388328552, Val Loss: 0.8825392723083496\n",
      "Step: 720, Subject: sub5; Training Loss: 0.6713792085647583, Val Loss: 0.7479284405708313\n",
      "Step: 720, Subject: sub7; Training Loss: 0.590792179107666, Val Loss: 0.766193687915802\n",
      "Step: 730, Subject: sub1; Training Loss: 0.7670219540596008, Val Loss: 0.7845606803894043\n",
      "Step: 730, Subject: sub2; Training Loss: 0.702648937702179, Val Loss: 0.8466660976409912\n",
      "Step: 730, Subject: sub3; Training Loss: 0.7718863487243652, Val Loss: 0.8375935554504395\n",
      "Step: 730, Subject: sub4; Training Loss: 0.7812946438789368, Val Loss: 0.8896007537841797\n",
      "Step: 730, Subject: sub5; Training Loss: 0.6618027687072754, Val Loss: 0.6858983039855957\n",
      "Step: 730, Subject: sub7; Training Loss: 0.6327801942825317, Val Loss: 0.7215559482574463\n",
      "Step: 740, Subject: sub1; Training Loss: 0.7437278628349304, Val Loss: 0.8494201898574829\n",
      "Step: 740, Subject: sub2; Training Loss: 0.7459173202514648, Val Loss: 0.8672288060188293\n",
      "Step: 740, Subject: sub3; Training Loss: 0.8287836909294128, Val Loss: 0.8998395204544067\n",
      "Step: 740, Subject: sub4; Training Loss: 0.757834792137146, Val Loss: 0.8644466996192932\n",
      "Step: 740, Subject: sub5; Training Loss: 0.6684281229972839, Val Loss: 0.7815206050872803\n",
      "Step: 740, Subject: sub7; Training Loss: 0.5830162763595581, Val Loss: 0.7691159248352051\n",
      "Step: 750, Subject: sub1; Training Loss: 0.8259211182594299, Val Loss: 0.918886661529541\n",
      "Step: 750, Subject: sub2; Training Loss: 0.6508218050003052, Val Loss: 0.8899011611938477\n",
      "Step: 750, Subject: sub3; Training Loss: 0.8025746941566467, Val Loss: 0.8306090831756592\n",
      "Step: 750, Subject: sub4; Training Loss: 0.7518002986907959, Val Loss: 0.8322579860687256\n",
      "Step: 750, Subject: sub5; Training Loss: 0.6263493895530701, Val Loss: 0.7339615821838379\n",
      "Step: 750, Subject: sub7; Training Loss: 0.6226806640625, Val Loss: 0.7460303902626038\n",
      "Step: 760, Subject: sub1; Training Loss: 0.8380834460258484, Val Loss: 0.7778829336166382\n",
      "Step: 760, Subject: sub2; Training Loss: 0.71483314037323, Val Loss: 0.9151577353477478\n",
      "Step: 760, Subject: sub3; Training Loss: 0.8332675099372864, Val Loss: 0.763157308101654\n",
      "Step: 760, Subject: sub4; Training Loss: 0.8003116846084595, Val Loss: 0.9025660157203674\n",
      "Step: 760, Subject: sub5; Training Loss: 0.6195344924926758, Val Loss: 0.716619610786438\n",
      "Step: 760, Subject: sub7; Training Loss: 0.5851211547851562, Val Loss: 0.7971099615097046\n",
      "Step: 770, Subject: sub1; Training Loss: 0.7786281108856201, Val Loss: 0.7980018854141235\n",
      "Step: 770, Subject: sub2; Training Loss: 0.697678804397583, Val Loss: 0.8443568348884583\n",
      "Step: 770, Subject: sub3; Training Loss: 0.8196576237678528, Val Loss: 0.841892421245575\n",
      "Step: 770, Subject: sub4; Training Loss: 0.7900412082672119, Val Loss: 0.8449611663818359\n",
      "Step: 770, Subject: sub5; Training Loss: 0.6799458265304565, Val Loss: 0.7481552958488464\n",
      "Step: 770, Subject: sub7; Training Loss: 0.6373578310012817, Val Loss: 0.750659167766571\n",
      "Step: 780, Subject: sub1; Training Loss: 0.799627959728241, Val Loss: 0.7637898325920105\n",
      "Step: 780, Subject: sub2; Training Loss: 0.738850474357605, Val Loss: 0.8377345204353333\n",
      "Step: 780, Subject: sub3; Training Loss: 0.7524216175079346, Val Loss: 0.8545452356338501\n",
      "Step: 780, Subject: sub4; Training Loss: 0.7197322845458984, Val Loss: 0.8320903778076172\n",
      "Step: 780, Subject: sub5; Training Loss: 0.6618133783340454, Val Loss: 0.7133704423904419\n",
      "Step: 780, Subject: sub7; Training Loss: 0.5860783457756042, Val Loss: 0.7368795871734619\n",
      "Step: 790, Subject: sub1; Training Loss: 0.8073759078979492, Val Loss: 0.7672743797302246\n",
      "Step: 790, Subject: sub2; Training Loss: 0.7623645067214966, Val Loss: 0.8556810617446899\n",
      "Step: 790, Subject: sub3; Training Loss: 0.7325080633163452, Val Loss: 0.8363603353500366\n",
      "Step: 790, Subject: sub4; Training Loss: 0.7317966222763062, Val Loss: 0.8582349419593811\n",
      "Step: 790, Subject: sub5; Training Loss: 0.6677131056785583, Val Loss: 0.751318097114563\n",
      "Step: 790, Subject: sub7; Training Loss: 0.656024694442749, Val Loss: 0.7368500828742981\n",
      "Step: 800, Subject: sub1; Training Loss: 0.7529383897781372, Val Loss: 0.8034883141517639\n",
      "Step: 800, Subject: sub2; Training Loss: 0.7172996997833252, Val Loss: 0.8474984169006348\n",
      "Step: 800, Subject: sub3; Training Loss: 0.7953288555145264, Val Loss: 0.8529036641120911\n",
      "Step: 800, Subject: sub4; Training Loss: 0.7548621892929077, Val Loss: 0.8902086615562439\n",
      "Step: 800, Subject: sub5; Training Loss: 0.6472774744033813, Val Loss: 0.7029480934143066\n",
      "Step: 800, Subject: sub7; Training Loss: 0.553006112575531, Val Loss: 0.777577817440033\n",
      "Step: 810, Subject: sub1; Training Loss: 0.7693381309509277, Val Loss: 0.8577666282653809\n",
      "Step: 810, Subject: sub2; Training Loss: 0.6755113005638123, Val Loss: 0.8620595335960388\n",
      "Step: 810, Subject: sub3; Training Loss: 0.7750988006591797, Val Loss: 0.8258860111236572\n",
      "Step: 810, Subject: sub4; Training Loss: 0.7864106297492981, Val Loss: 0.8581897020339966\n",
      "Step: 810, Subject: sub5; Training Loss: 0.6763862371444702, Val Loss: 0.7394064664840698\n",
      "Step: 810, Subject: sub7; Training Loss: 0.6048626899719238, Val Loss: 0.7937105894088745\n",
      "Step: 820, Subject: sub1; Training Loss: 0.7482328414916992, Val Loss: 0.8905308246612549\n",
      "Step: 820, Subject: sub2; Training Loss: 0.7522028088569641, Val Loss: 0.8763340711593628\n",
      "Step: 820, Subject: sub3; Training Loss: 0.8054083585739136, Val Loss: 0.8077017068862915\n",
      "Step: 820, Subject: sub4; Training Loss: 0.7154381275177002, Val Loss: 0.867168664932251\n",
      "Step: 820, Subject: sub5; Training Loss: 0.6523910760879517, Val Loss: 0.7527664303779602\n",
      "Step: 820, Subject: sub7; Training Loss: 0.5383180379867554, Val Loss: 0.7796635627746582\n",
      "Step: 830, Subject: sub1; Training Loss: 0.7285701036453247, Val Loss: 0.7944562435150146\n",
      "Step: 830, Subject: sub2; Training Loss: 0.7276391386985779, Val Loss: 0.8643884658813477\n",
      "Step: 830, Subject: sub3; Training Loss: 0.7447444200515747, Val Loss: 0.8036831021308899\n",
      "Step: 830, Subject: sub4; Training Loss: 0.7663909196853638, Val Loss: 0.8382012844085693\n",
      "Step: 830, Subject: sub5; Training Loss: 0.6065402626991272, Val Loss: 0.7469525337219238\n",
      "Step: 830, Subject: sub7; Training Loss: 0.6188822984695435, Val Loss: 0.7993524670600891\n",
      "Step: 840, Subject: sub1; Training Loss: 0.7624624967575073, Val Loss: 0.8515282869338989\n",
      "Step: 840, Subject: sub2; Training Loss: 0.6585845947265625, Val Loss: 0.8649715185165405\n",
      "Step: 840, Subject: sub3; Training Loss: 0.8351990580558777, Val Loss: 0.8115665912628174\n",
      "Step: 840, Subject: sub4; Training Loss: 0.7157754898071289, Val Loss: 0.8423366546630859\n",
      "Step: 840, Subject: sub5; Training Loss: 0.6357476115226746, Val Loss: 0.710609495639801\n",
      "Step: 840, Subject: sub7; Training Loss: 0.6093435287475586, Val Loss: 0.7357885241508484\n",
      "Step: 850, Subject: sub1; Training Loss: 0.8017182946205139, Val Loss: 0.8195902109146118\n",
      "Step: 850, Subject: sub2; Training Loss: 0.7156690359115601, Val Loss: 0.8580269813537598\n",
      "Step: 850, Subject: sub3; Training Loss: 0.6753708124160767, Val Loss: 0.8116642236709595\n",
      "Step: 850, Subject: sub4; Training Loss: 0.7016549110412598, Val Loss: 0.884778618812561\n",
      "Step: 850, Subject: sub5; Training Loss: 0.6516959071159363, Val Loss: 0.7423930764198303\n",
      "Step: 850, Subject: sub7; Training Loss: 0.635491132736206, Val Loss: 0.7176562547683716\n",
      "Step: 860, Subject: sub1; Training Loss: 0.7626867294311523, Val Loss: 0.833838939666748\n",
      "Step: 860, Subject: sub2; Training Loss: 0.7663333415985107, Val Loss: 0.8636808395385742\n",
      "Step: 860, Subject: sub3; Training Loss: 0.7534220814704895, Val Loss: 0.8176146745681763\n",
      "Step: 860, Subject: sub4; Training Loss: 0.7396233081817627, Val Loss: 0.8593530654907227\n",
      "Step: 860, Subject: sub5; Training Loss: 0.6227796077728271, Val Loss: 0.7190821170806885\n",
      "Step: 860, Subject: sub7; Training Loss: 0.5887326002120972, Val Loss: 0.7232747077941895\n",
      "Step: 870, Subject: sub1; Training Loss: 0.8107292652130127, Val Loss: 0.8155477046966553\n",
      "Step: 870, Subject: sub2; Training Loss: 0.7156091928482056, Val Loss: 0.8224946856498718\n",
      "Step: 870, Subject: sub3; Training Loss: 0.7683573961257935, Val Loss: 0.8590805530548096\n",
      "Step: 870, Subject: sub4; Training Loss: 0.7133331298828125, Val Loss: 0.8474878072738647\n",
      "Step: 870, Subject: sub5; Training Loss: 0.6279258728027344, Val Loss: 0.7601804733276367\n",
      "Step: 870, Subject: sub7; Training Loss: 0.6275556683540344, Val Loss: 0.7772372961044312\n",
      "Step: 880, Subject: sub1; Training Loss: 0.7633740305900574, Val Loss: 0.8386392593383789\n",
      "Step: 880, Subject: sub2; Training Loss: 0.7163975238800049, Val Loss: 0.9053667783737183\n",
      "Step: 880, Subject: sub3; Training Loss: 0.704021155834198, Val Loss: 0.8274058103561401\n",
      "Step: 880, Subject: sub4; Training Loss: 0.7442691922187805, Val Loss: 0.8372179269790649\n",
      "Step: 880, Subject: sub5; Training Loss: 0.6712594628334045, Val Loss: 0.7925996780395508\n",
      "Step: 880, Subject: sub7; Training Loss: 0.6094202995300293, Val Loss: 0.7699275016784668\n",
      "Step: 890, Subject: sub1; Training Loss: 0.8043041229248047, Val Loss: 0.8036329746246338\n",
      "Step: 890, Subject: sub2; Training Loss: 0.7112236022949219, Val Loss: 0.8842073082923889\n",
      "Step: 890, Subject: sub3; Training Loss: 0.6860557794570923, Val Loss: 0.8764517307281494\n",
      "Step: 890, Subject: sub4; Training Loss: 0.7983419895172119, Val Loss: 0.8572896122932434\n",
      "Step: 890, Subject: sub5; Training Loss: 0.6109122037887573, Val Loss: 0.7332714796066284\n",
      "Step: 890, Subject: sub7; Training Loss: 0.6114749312400818, Val Loss: 0.795226514339447\n",
      "Step: 900, Subject: sub1; Training Loss: 0.7525455951690674, Val Loss: 0.7960817813873291\n",
      "Step: 900, Subject: sub2; Training Loss: 0.71734619140625, Val Loss: 0.7848692536354065\n",
      "Step: 900, Subject: sub3; Training Loss: 0.7654311656951904, Val Loss: 0.761496901512146\n",
      "Step: 900, Subject: sub4; Training Loss: 0.7003505229949951, Val Loss: 0.8931350111961365\n",
      "Step: 900, Subject: sub5; Training Loss: 0.6405978202819824, Val Loss: 0.7473626732826233\n",
      "Step: 900, Subject: sub7; Training Loss: 0.5692756175994873, Val Loss: 0.7584084272384644\n",
      "Step: 910, Subject: sub1; Training Loss: 0.7671169638633728, Val Loss: 0.824084997177124\n",
      "Step: 910, Subject: sub2; Training Loss: 0.7375534772872925, Val Loss: 0.7953795790672302\n",
      "Step: 910, Subject: sub3; Training Loss: 0.7257031202316284, Val Loss: 0.8618404865264893\n",
      "Step: 910, Subject: sub4; Training Loss: 0.7725104093551636, Val Loss: 0.8536056280136108\n",
      "Step: 910, Subject: sub5; Training Loss: 0.6332917213439941, Val Loss: 0.7841795682907104\n",
      "Step: 910, Subject: sub7; Training Loss: 0.5703286528587341, Val Loss: 0.7903030514717102\n",
      "Step: 920, Subject: sub1; Training Loss: 0.7266273498535156, Val Loss: 0.802422285079956\n",
      "Step: 920, Subject: sub2; Training Loss: 0.6864772439002991, Val Loss: 0.8930588960647583\n",
      "Step: 920, Subject: sub3; Training Loss: 0.6866546869277954, Val Loss: 0.8615200519561768\n",
      "Step: 920, Subject: sub4; Training Loss: 0.7621617317199707, Val Loss: 0.85785973072052\n",
      "Step: 920, Subject: sub5; Training Loss: 0.6128208637237549, Val Loss: 0.7558565735816956\n",
      "Step: 920, Subject: sub7; Training Loss: 0.5988529920578003, Val Loss: 0.7403490543365479\n",
      "Step: 930, Subject: sub1; Training Loss: 0.8059237003326416, Val Loss: 0.7857728004455566\n",
      "Step: 930, Subject: sub2; Training Loss: 0.6861974000930786, Val Loss: 0.8607462644577026\n",
      "Step: 930, Subject: sub3; Training Loss: 0.7573503255844116, Val Loss: 0.8051789402961731\n",
      "Step: 930, Subject: sub4; Training Loss: 0.8145201206207275, Val Loss: 0.8796776533126831\n",
      "Step: 930, Subject: sub5; Training Loss: 0.6082087755203247, Val Loss: 0.6973759531974792\n",
      "Step: 930, Subject: sub7; Training Loss: 0.6052114367485046, Val Loss: 0.7371480464935303\n",
      "Step: 940, Subject: sub1; Training Loss: 0.7711224555969238, Val Loss: 0.7764415740966797\n",
      "Step: 940, Subject: sub2; Training Loss: 0.7070285081863403, Val Loss: 0.8421814441680908\n",
      "Step: 940, Subject: sub3; Training Loss: 0.7303867936134338, Val Loss: 0.8102052211761475\n",
      "Step: 940, Subject: sub4; Training Loss: 0.7331544756889343, Val Loss: 0.8533170223236084\n",
      "Step: 940, Subject: sub5; Training Loss: 0.6625181436538696, Val Loss: 0.6944975852966309\n",
      "Step: 940, Subject: sub7; Training Loss: 0.6147562265396118, Val Loss: 0.7225910425186157\n",
      "Step: 950, Subject: sub1; Training Loss: 0.7518410682678223, Val Loss: 0.840526819229126\n",
      "Step: 950, Subject: sub2; Training Loss: 0.7373231649398804, Val Loss: 0.8507118225097656\n",
      "Step: 950, Subject: sub3; Training Loss: 0.7160221338272095, Val Loss: 0.8925819396972656\n",
      "Step: 950, Subject: sub4; Training Loss: 0.7737619876861572, Val Loss: 0.869229793548584\n",
      "Step: 950, Subject: sub5; Training Loss: 0.6184363961219788, Val Loss: 0.7722265124320984\n",
      "Step: 950, Subject: sub7; Training Loss: 0.6139236092567444, Val Loss: 0.8068256974220276\n",
      "Step: 960, Subject: sub1; Training Loss: 0.7169119119644165, Val Loss: 0.7076884508132935\n",
      "Step: 960, Subject: sub2; Training Loss: 0.6969391107559204, Val Loss: 0.8830336332321167\n",
      "Step: 960, Subject: sub3; Training Loss: 0.6960452795028687, Val Loss: 0.7895148396492004\n",
      "Step: 960, Subject: sub4; Training Loss: 0.7982428073883057, Val Loss: 0.8647306561470032\n",
      "Step: 960, Subject: sub5; Training Loss: 0.670539379119873, Val Loss: 0.7139992117881775\n",
      "Step: 960, Subject: sub7; Training Loss: 0.6334465146064758, Val Loss: 0.7376319169998169\n",
      "Step: 970, Subject: sub1; Training Loss: 0.7748696804046631, Val Loss: 0.8920706510543823\n",
      "Step: 970, Subject: sub2; Training Loss: 0.7533074021339417, Val Loss: 0.8544622659683228\n",
      "Step: 970, Subject: sub3; Training Loss: 0.7826679944992065, Val Loss: 0.8227311372756958\n",
      "Step: 970, Subject: sub4; Training Loss: 0.7514176368713379, Val Loss: 0.840555727481842\n",
      "Step: 970, Subject: sub5; Training Loss: 0.6140061616897583, Val Loss: 0.6981392502784729\n",
      "Step: 970, Subject: sub7; Training Loss: 0.6045734286308289, Val Loss: 0.7647424936294556\n",
      "Step: 980, Subject: sub1; Training Loss: 0.8231409788131714, Val Loss: 0.7556796073913574\n",
      "Step: 980, Subject: sub2; Training Loss: 0.7580534815788269, Val Loss: 0.821707546710968\n",
      "Step: 980, Subject: sub3; Training Loss: 0.7130621671676636, Val Loss: 0.7744513154029846\n",
      "Step: 980, Subject: sub4; Training Loss: 0.7231440544128418, Val Loss: 0.8605910539627075\n",
      "Step: 980, Subject: sub5; Training Loss: 0.6340978145599365, Val Loss: 0.7459600567817688\n",
      "Step: 980, Subject: sub7; Training Loss: 0.5974792838096619, Val Loss: 0.7790063619613647\n",
      "Step: 990, Subject: sub1; Training Loss: 0.7915706634521484, Val Loss: 0.8335249423980713\n",
      "Step: 990, Subject: sub2; Training Loss: 0.6822458505630493, Val Loss: 0.8038519620895386\n",
      "Step: 990, Subject: sub3; Training Loss: 0.6889127492904663, Val Loss: 0.810080885887146\n",
      "Step: 990, Subject: sub4; Training Loss: 0.641623318195343, Val Loss: 0.8999335169792175\n",
      "Step: 990, Subject: sub5; Training Loss: 0.6523601412773132, Val Loss: 0.6721667647361755\n",
      "Step: 990, Subject: sub7; Training Loss: 0.5870584845542908, Val Loss: 0.7533602714538574\n",
      "Step: 1000, Subject: sub1; Training Loss: 0.7555879354476929, Val Loss: 0.7710902690887451\n",
      "Step: 1000, Subject: sub2; Training Loss: 0.7052074074745178, Val Loss: 0.8536200523376465\n",
      "Step: 1000, Subject: sub3; Training Loss: 0.7198742628097534, Val Loss: 0.8937021493911743\n",
      "Step: 1000, Subject: sub4; Training Loss: 0.718834400177002, Val Loss: 0.8521943688392639\n",
      "Step: 1000, Subject: sub5; Training Loss: 0.6554203033447266, Val Loss: 0.7689218521118164\n",
      "Step: 1000, Subject: sub7; Training Loss: 0.5500039458274841, Val Loss: 0.7270115613937378\n",
      "Step: 1010, Subject: sub1; Training Loss: 0.746642529964447, Val Loss: 0.8054649829864502\n",
      "Step: 1010, Subject: sub2; Training Loss: 0.7311868667602539, Val Loss: 0.8426336050033569\n",
      "Step: 1010, Subject: sub3; Training Loss: 0.7749338150024414, Val Loss: 0.8207285404205322\n",
      "Step: 1010, Subject: sub4; Training Loss: 0.7482445240020752, Val Loss: 0.8654666543006897\n",
      "Step: 1010, Subject: sub5; Training Loss: 0.6432168483734131, Val Loss: 0.784565806388855\n",
      "Step: 1010, Subject: sub7; Training Loss: 0.6262364387512207, Val Loss: 0.7492477893829346\n",
      "Step: 1020, Subject: sub1; Training Loss: 0.7779045104980469, Val Loss: 0.8083908557891846\n",
      "Step: 1020, Subject: sub2; Training Loss: 0.6208212971687317, Val Loss: 0.8192898035049438\n",
      "Step: 1020, Subject: sub3; Training Loss: 0.8007379174232483, Val Loss: 0.7964233160018921\n",
      "Step: 1020, Subject: sub4; Training Loss: 0.7839351892471313, Val Loss: 0.8536123037338257\n",
      "Step: 1020, Subject: sub5; Training Loss: 0.6696668267250061, Val Loss: 0.6819392442703247\n",
      "Step: 1020, Subject: sub7; Training Loss: 0.5912359952926636, Val Loss: 0.7777791023254395\n",
      "Step: 1030, Subject: sub1; Training Loss: 0.8141705393791199, Val Loss: 0.7577047348022461\n",
      "Step: 1030, Subject: sub2; Training Loss: 0.6966971158981323, Val Loss: 0.8283525705337524\n",
      "Step: 1030, Subject: sub3; Training Loss: 0.7739448547363281, Val Loss: 0.8250594735145569\n",
      "Step: 1030, Subject: sub4; Training Loss: 0.7533381581306458, Val Loss: 0.8470255732536316\n",
      "Step: 1030, Subject: sub5; Training Loss: 0.6099315881729126, Val Loss: 0.7362885475158691\n",
      "Step: 1030, Subject: sub7; Training Loss: 0.5860447287559509, Val Loss: 0.7809724807739258\n",
      "Step: 1040, Subject: sub1; Training Loss: 0.7799996733665466, Val Loss: 0.7546259164810181\n",
      "Step: 1040, Subject: sub2; Training Loss: 0.7210593223571777, Val Loss: 0.8186417818069458\n",
      "Step: 1040, Subject: sub3; Training Loss: 0.7480538487434387, Val Loss: 0.8504461646080017\n",
      "Step: 1040, Subject: sub4; Training Loss: 0.7285991311073303, Val Loss: 0.9218155145645142\n",
      "Step: 1040, Subject: sub5; Training Loss: 0.6636925339698792, Val Loss: 0.7339886426925659\n",
      "Step: 1040, Subject: sub7; Training Loss: 0.575325608253479, Val Loss: 0.7707824110984802\n",
      "Step: 1050, Subject: sub1; Training Loss: 0.7873435020446777, Val Loss: 0.8195866346359253\n",
      "Step: 1050, Subject: sub2; Training Loss: 0.6860885620117188, Val Loss: 0.8577414751052856\n",
      "Step: 1050, Subject: sub3; Training Loss: 0.7371989488601685, Val Loss: 0.7423001527786255\n",
      "Step: 1050, Subject: sub4; Training Loss: 0.7051313519477844, Val Loss: 0.8297698497772217\n",
      "Step: 1050, Subject: sub5; Training Loss: 0.6236896514892578, Val Loss: 0.6787183284759521\n",
      "Step: 1050, Subject: sub7; Training Loss: 0.5596444606781006, Val Loss: 0.7292275428771973\n",
      "Step: 1060, Subject: sub1; Training Loss: 0.7383235096931458, Val Loss: 0.8207590579986572\n",
      "Step: 1060, Subject: sub2; Training Loss: 0.7133610248565674, Val Loss: 0.8488274812698364\n",
      "Step: 1060, Subject: sub3; Training Loss: 0.7024078369140625, Val Loss: 0.8385295271873474\n",
      "Step: 1060, Subject: sub4; Training Loss: 0.6769975423812866, Val Loss: 0.8576937913894653\n",
      "Step: 1060, Subject: sub5; Training Loss: 0.6308798789978027, Val Loss: 0.7471442222595215\n",
      "Step: 1060, Subject: sub7; Training Loss: 0.6615984439849854, Val Loss: 0.7905409336090088\n",
      "Step: 1070, Subject: sub1; Training Loss: 0.7799054384231567, Val Loss: 0.7691906690597534\n",
      "Step: 1070, Subject: sub2; Training Loss: 0.7602144479751587, Val Loss: 0.7764424085617065\n",
      "Step: 1070, Subject: sub3; Training Loss: 0.7456790208816528, Val Loss: 0.8536003828048706\n",
      "Step: 1070, Subject: sub4; Training Loss: 0.773261308670044, Val Loss: 0.8403157591819763\n",
      "Step: 1070, Subject: sub5; Training Loss: 0.6366446018218994, Val Loss: 0.7554875612258911\n",
      "Step: 1070, Subject: sub7; Training Loss: 0.5468502640724182, Val Loss: 0.7501873970031738\n",
      "Step: 1080, Subject: sub1; Training Loss: 0.7278395891189575, Val Loss: 0.806230366230011\n",
      "Step: 1080, Subject: sub2; Training Loss: 0.6828551292419434, Val Loss: 0.8582892417907715\n",
      "Step: 1080, Subject: sub3; Training Loss: 0.8447030186653137, Val Loss: 0.8064900636672974\n",
      "Step: 1080, Subject: sub4; Training Loss: 0.7132515907287598, Val Loss: 0.8556915521621704\n",
      "Step: 1080, Subject: sub5; Training Loss: 0.6188647150993347, Val Loss: 0.6742377877235413\n",
      "Step: 1080, Subject: sub7; Training Loss: 0.6046161651611328, Val Loss: 0.7160372138023376\n",
      "Step: 1090, Subject: sub1; Training Loss: 0.7882795929908752, Val Loss: 0.7962746024131775\n",
      "Step: 1090, Subject: sub2; Training Loss: 0.671497106552124, Val Loss: 0.8544037342071533\n",
      "Step: 1090, Subject: sub3; Training Loss: 0.7319687008857727, Val Loss: 0.8463359475135803\n",
      "Step: 1090, Subject: sub4; Training Loss: 0.7279921770095825, Val Loss: 0.8453989028930664\n",
      "Step: 1090, Subject: sub5; Training Loss: 0.6321996450424194, Val Loss: 0.7740607261657715\n",
      "Step: 1090, Subject: sub7; Training Loss: 0.5639933943748474, Val Loss: 0.7709976434707642\n",
      "Step: 1100, Subject: sub1; Training Loss: 0.7839771509170532, Val Loss: 0.893283486366272\n",
      "Step: 1100, Subject: sub2; Training Loss: 0.7028771638870239, Val Loss: 0.8344899415969849\n",
      "Step: 1100, Subject: sub3; Training Loss: 0.7639384269714355, Val Loss: 0.8041949272155762\n",
      "Step: 1100, Subject: sub4; Training Loss: 0.7061427235603333, Val Loss: 0.8240131139755249\n",
      "Step: 1100, Subject: sub5; Training Loss: 0.6640899181365967, Val Loss: 0.7226155996322632\n",
      "Step: 1100, Subject: sub7; Training Loss: 0.5505553483963013, Val Loss: 0.7785006761550903\n",
      "Step: 1110, Subject: sub1; Training Loss: 0.7840757369995117, Val Loss: 0.8243428468704224\n",
      "Step: 1110, Subject: sub2; Training Loss: 0.6928556561470032, Val Loss: 0.8160259127616882\n",
      "Step: 1110, Subject: sub3; Training Loss: 0.6800777912139893, Val Loss: 0.8686395287513733\n",
      "Step: 1110, Subject: sub4; Training Loss: 0.730945348739624, Val Loss: 0.84818434715271\n",
      "Step: 1110, Subject: sub5; Training Loss: 0.6360926628112793, Val Loss: 0.7175980806350708\n",
      "Step: 1110, Subject: sub7; Training Loss: 0.5972410440444946, Val Loss: 0.7939261198043823\n",
      "Step: 1120, Subject: sub1; Training Loss: 0.7352463006973267, Val Loss: 0.8520848751068115\n",
      "Step: 1120, Subject: sub2; Training Loss: 0.6973978281021118, Val Loss: 0.8655687570571899\n",
      "Step: 1120, Subject: sub3; Training Loss: 0.7602857947349548, Val Loss: 0.8208224773406982\n",
      "Step: 1120, Subject: sub4; Training Loss: 0.6697598099708557, Val Loss: 0.8218235373497009\n",
      "Step: 1120, Subject: sub5; Training Loss: 0.6393595933914185, Val Loss: 0.704205334186554\n",
      "Step: 1120, Subject: sub7; Training Loss: 0.5921038389205933, Val Loss: 0.7684628963470459\n",
      "Step: 1130, Subject: sub1; Training Loss: 0.8448443412780762, Val Loss: 0.8102276921272278\n",
      "Step: 1130, Subject: sub2; Training Loss: 0.6537919044494629, Val Loss: 0.8440964221954346\n",
      "Step: 1130, Subject: sub3; Training Loss: 0.7393732070922852, Val Loss: 0.8543167114257812\n",
      "Step: 1130, Subject: sub4; Training Loss: 0.6737569570541382, Val Loss: 0.8811228275299072\n",
      "Step: 1130, Subject: sub5; Training Loss: 0.6340018510818481, Val Loss: 0.7399348616600037\n",
      "Step: 1130, Subject: sub7; Training Loss: 0.5585888028144836, Val Loss: 0.7907814979553223\n",
      "Step: 1140, Subject: sub1; Training Loss: 0.7220258712768555, Val Loss: 0.7984439730644226\n",
      "Step: 1140, Subject: sub2; Training Loss: 0.7081180214881897, Val Loss: 0.8713977336883545\n",
      "Step: 1140, Subject: sub3; Training Loss: 0.7343525290489197, Val Loss: 0.796905517578125\n",
      "Step: 1140, Subject: sub4; Training Loss: 0.724561333656311, Val Loss: 0.8486330509185791\n",
      "Step: 1140, Subject: sub5; Training Loss: 0.6145232915878296, Val Loss: 0.7358100414276123\n",
      "Step: 1140, Subject: sub7; Training Loss: 0.5797331929206848, Val Loss: 0.7217052578926086\n",
      "Step: 1150, Subject: sub1; Training Loss: 0.799523115158081, Val Loss: 0.8637936115264893\n",
      "Step: 1150, Subject: sub2; Training Loss: 0.7368583083152771, Val Loss: 0.8721151947975159\n",
      "Step: 1150, Subject: sub3; Training Loss: 0.6821519136428833, Val Loss: 0.861202597618103\n",
      "Step: 1150, Subject: sub4; Training Loss: 0.7460827231407166, Val Loss: 0.8527179956436157\n",
      "Step: 1150, Subject: sub5; Training Loss: 0.6274138689041138, Val Loss: 0.7968121767044067\n",
      "Step: 1150, Subject: sub7; Training Loss: 0.5999799370765686, Val Loss: 0.7757529020309448\n",
      "Step: 1160, Subject: sub1; Training Loss: 0.821747899055481, Val Loss: 0.8494493961334229\n",
      "Step: 1160, Subject: sub2; Training Loss: 0.6491923332214355, Val Loss: 0.889224648475647\n",
      "Step: 1160, Subject: sub3; Training Loss: 0.7046236395835876, Val Loss: 0.8154038190841675\n",
      "Step: 1160, Subject: sub4; Training Loss: 0.7549506425857544, Val Loss: 0.870477557182312\n",
      "Step: 1160, Subject: sub5; Training Loss: 0.6500926613807678, Val Loss: 0.7108304500579834\n",
      "Step: 1160, Subject: sub7; Training Loss: 0.6140997409820557, Val Loss: 0.7764542698860168\n",
      "Step: 1170, Subject: sub1; Training Loss: 0.8023394346237183, Val Loss: 0.7553614377975464\n",
      "Step: 1170, Subject: sub2; Training Loss: 0.7129266262054443, Val Loss: 0.7752236723899841\n",
      "Step: 1170, Subject: sub3; Training Loss: 0.7311924695968628, Val Loss: 0.8296258449554443\n",
      "Step: 1170, Subject: sub4; Training Loss: 0.6745900511741638, Val Loss: 0.8347392678260803\n",
      "Step: 1170, Subject: sub5; Training Loss: 0.6240077018737793, Val Loss: 0.6793124079704285\n",
      "Step: 1170, Subject: sub7; Training Loss: 0.6563628911972046, Val Loss: 0.7679077386856079\n",
      "Step: 1180, Subject: sub1; Training Loss: 0.7721776962280273, Val Loss: 0.8467463850975037\n",
      "Step: 1180, Subject: sub2; Training Loss: 0.7350496053695679, Val Loss: 0.8287872076034546\n",
      "Step: 1180, Subject: sub3; Training Loss: 0.7144098281860352, Val Loss: 0.7651466727256775\n",
      "Step: 1180, Subject: sub4; Training Loss: 0.6939568519592285, Val Loss: 0.8302310109138489\n",
      "Step: 1180, Subject: sub5; Training Loss: 0.6016489267349243, Val Loss: 0.7535570859909058\n",
      "Step: 1180, Subject: sub7; Training Loss: 0.5522977709770203, Val Loss: 0.7723375558853149\n",
      "Step: 1190, Subject: sub1; Training Loss: 0.765669584274292, Val Loss: 0.8157023191452026\n",
      "Step: 1190, Subject: sub2; Training Loss: 0.6321812272071838, Val Loss: 0.8606171607971191\n",
      "Step: 1190, Subject: sub3; Training Loss: 0.7487000226974487, Val Loss: 0.8383429646492004\n",
      "Step: 1190, Subject: sub4; Training Loss: 0.7193087339401245, Val Loss: 0.8382577300071716\n",
      "Step: 1190, Subject: sub5; Training Loss: 0.5835279822349548, Val Loss: 0.7425216436386108\n",
      "Step: 1190, Subject: sub7; Training Loss: 0.5843478441238403, Val Loss: 0.7387053370475769\n",
      "Step: 1200, Subject: sub1; Training Loss: 0.7717387676239014, Val Loss: 0.8270058035850525\n",
      "Step: 1200, Subject: sub2; Training Loss: 0.7268058061599731, Val Loss: 0.8457961082458496\n",
      "Step: 1200, Subject: sub3; Training Loss: 0.7015707492828369, Val Loss: 0.8057027459144592\n",
      "Step: 1200, Subject: sub4; Training Loss: 0.7155950665473938, Val Loss: 0.862379789352417\n",
      "Step: 1200, Subject: sub5; Training Loss: 0.616307258605957, Val Loss: 0.7761646509170532\n",
      "Step: 1200, Subject: sub7; Training Loss: 0.5691816806793213, Val Loss: 0.772369921207428\n",
      "Step: 1210, Subject: sub1; Training Loss: 0.6869627833366394, Val Loss: 0.7525108456611633\n",
      "Step: 1210, Subject: sub2; Training Loss: 0.6948994398117065, Val Loss: 0.8204552531242371\n",
      "Step: 1210, Subject: sub3; Training Loss: 0.7698280811309814, Val Loss: 0.8420686721801758\n",
      "Step: 1210, Subject: sub4; Training Loss: 0.7516059875488281, Val Loss: 0.8917761445045471\n",
      "Step: 1210, Subject: sub5; Training Loss: 0.6936043500900269, Val Loss: 0.7533690333366394\n",
      "Step: 1210, Subject: sub7; Training Loss: 0.6174773573875427, Val Loss: 0.7428094148635864\n",
      "Step: 1220, Subject: sub1; Training Loss: 0.7583250999450684, Val Loss: 0.808259904384613\n",
      "Step: 1220, Subject: sub2; Training Loss: 0.7044823169708252, Val Loss: 0.933198094367981\n",
      "Step: 1220, Subject: sub3; Training Loss: 0.7161837816238403, Val Loss: 0.8313618898391724\n",
      "Step: 1220, Subject: sub4; Training Loss: 0.6536014080047607, Val Loss: 0.8829039931297302\n",
      "Step: 1220, Subject: sub5; Training Loss: 0.6525292992591858, Val Loss: 0.743118941783905\n",
      "Step: 1220, Subject: sub7; Training Loss: 0.6151114106178284, Val Loss: 0.8080459833145142\n",
      "Step: 1230, Subject: sub1; Training Loss: 0.765373945236206, Val Loss: 0.8315783739089966\n",
      "Step: 1230, Subject: sub2; Training Loss: 0.6929951906204224, Val Loss: 0.8959535360336304\n",
      "Step: 1230, Subject: sub3; Training Loss: 0.7728550434112549, Val Loss: 0.7847873568534851\n",
      "Step: 1230, Subject: sub4; Training Loss: 0.7354048490524292, Val Loss: 0.8547974824905396\n",
      "Step: 1230, Subject: sub5; Training Loss: 0.6174754500389099, Val Loss: 0.6871259808540344\n",
      "Step: 1230, Subject: sub7; Training Loss: 0.6016380786895752, Val Loss: 0.8410184979438782\n",
      "Step: 1240, Subject: sub1; Training Loss: 0.8022822141647339, Val Loss: 0.8282380104064941\n",
      "Step: 1240, Subject: sub2; Training Loss: 0.7496979832649231, Val Loss: 0.8550996780395508\n",
      "Step: 1240, Subject: sub3; Training Loss: 0.7597652673721313, Val Loss: 0.8356260061264038\n",
      "Step: 1240, Subject: sub4; Training Loss: 0.6489466428756714, Val Loss: 0.8785169124603271\n",
      "Step: 1240, Subject: sub5; Training Loss: 0.6121110916137695, Val Loss: 0.6847238540649414\n",
      "Step: 1240, Subject: sub7; Training Loss: 0.5824654698371887, Val Loss: 0.7533227205276489\n",
      "Step: 1250, Subject: sub1; Training Loss: 0.7725021243095398, Val Loss: 0.8261663317680359\n",
      "Step: 1250, Subject: sub2; Training Loss: 0.7207273840904236, Val Loss: 0.8591188192367554\n",
      "Step: 1250, Subject: sub3; Training Loss: 0.7169232368469238, Val Loss: 0.8255869150161743\n",
      "Step: 1250, Subject: sub4; Training Loss: 0.6811442375183105, Val Loss: 0.8589571714401245\n",
      "Step: 1250, Subject: sub5; Training Loss: 0.5999146699905396, Val Loss: 0.7130864858627319\n",
      "Step: 1250, Subject: sub7; Training Loss: 0.6025928258895874, Val Loss: 0.7035875916481018\n",
      "Step: 1260, Subject: sub1; Training Loss: 0.7642433643341064, Val Loss: 0.7814664840698242\n",
      "Step: 1260, Subject: sub2; Training Loss: 0.7148858308792114, Val Loss: 0.8801895380020142\n",
      "Step: 1260, Subject: sub3; Training Loss: 0.7169207334518433, Val Loss: 0.8676745891571045\n",
      "Step: 1260, Subject: sub4; Training Loss: 0.7408024072647095, Val Loss: 0.8677211999893188\n",
      "Step: 1260, Subject: sub5; Training Loss: 0.6314387917518616, Val Loss: 0.7021868228912354\n",
      "Step: 1260, Subject: sub7; Training Loss: 0.6482406854629517, Val Loss: 0.6626601219177246\n",
      "Saving model with validation loss: 0.6626601219177246\n",
      "\n",
      "Step: 1270, Subject: sub1; Training Loss: 0.7979417443275452, Val Loss: 0.8146320581436157\n",
      "Step: 1270, Subject: sub2; Training Loss: 0.7233830690383911, Val Loss: 0.8176053166389465\n",
      "Step: 1270, Subject: sub3; Training Loss: 0.864708662033081, Val Loss: 0.8516465425491333\n",
      "Step: 1270, Subject: sub4; Training Loss: 0.7098147869110107, Val Loss: 0.9027856588363647\n",
      "Step: 1270, Subject: sub5; Training Loss: 0.6648048758506775, Val Loss: 0.7714745998382568\n",
      "Step: 1270, Subject: sub7; Training Loss: 0.570229709148407, Val Loss: 0.7522715330123901\n",
      "Step: 1280, Subject: sub1; Training Loss: 0.7517472505569458, Val Loss: 0.7433651089668274\n",
      "Step: 1280, Subject: sub2; Training Loss: 0.7043663263320923, Val Loss: 0.8422102928161621\n",
      "Step: 1280, Subject: sub3; Training Loss: 0.7453876733779907, Val Loss: 0.8146024942398071\n",
      "Step: 1280, Subject: sub4; Training Loss: 0.6781255006790161, Val Loss: 0.8615875840187073\n",
      "Step: 1280, Subject: sub5; Training Loss: 0.619513988494873, Val Loss: 0.7474989295005798\n",
      "Step: 1280, Subject: sub7; Training Loss: 0.5796536207199097, Val Loss: 0.8176518678665161\n",
      "Step: 1290, Subject: sub1; Training Loss: 0.767281174659729, Val Loss: 0.8338513374328613\n",
      "Step: 1290, Subject: sub2; Training Loss: 0.6676748991012573, Val Loss: 0.9739817380905151\n",
      "Step: 1290, Subject: sub3; Training Loss: 0.7350247502326965, Val Loss: 0.8048720359802246\n",
      "Step: 1290, Subject: sub4; Training Loss: 0.672855019569397, Val Loss: 0.906240701675415\n",
      "Step: 1290, Subject: sub5; Training Loss: 0.6124770641326904, Val Loss: 0.8688431978225708\n",
      "Step: 1290, Subject: sub7; Training Loss: 0.5787652134895325, Val Loss: 0.8594498038291931\n",
      "Step: 1300, Subject: sub1; Training Loss: 0.7727239727973938, Val Loss: 0.786090075969696\n",
      "Step: 1300, Subject: sub2; Training Loss: 0.6825365424156189, Val Loss: 0.8259323835372925\n",
      "Step: 1300, Subject: sub3; Training Loss: 0.764408528804779, Val Loss: 0.8109892010688782\n",
      "Step: 1300, Subject: sub4; Training Loss: 0.650252103805542, Val Loss: 0.8674902319908142\n",
      "Step: 1300, Subject: sub5; Training Loss: 0.6345010995864868, Val Loss: 0.7527583837509155\n",
      "Step: 1300, Subject: sub7; Training Loss: 0.6304938793182373, Val Loss: 0.7510768175125122\n",
      "Step: 1310, Subject: sub1; Training Loss: 0.7626934051513672, Val Loss: 0.802447497844696\n",
      "Step: 1310, Subject: sub2; Training Loss: 0.7023146152496338, Val Loss: 0.8467249870300293\n",
      "Step: 1310, Subject: sub3; Training Loss: 0.6719998717308044, Val Loss: 0.8201228976249695\n",
      "Step: 1310, Subject: sub4; Training Loss: 0.7283984422683716, Val Loss: 0.8795289993286133\n",
      "Step: 1310, Subject: sub5; Training Loss: 0.6501007676124573, Val Loss: 0.73146653175354\n",
      "Step: 1310, Subject: sub7; Training Loss: 0.5334078669548035, Val Loss: 0.8337180614471436\n",
      "Step: 1320, Subject: sub1; Training Loss: 0.766384482383728, Val Loss: 0.829393744468689\n",
      "Step: 1320, Subject: sub2; Training Loss: 0.6834356784820557, Val Loss: 0.8335086107254028\n",
      "Step: 1320, Subject: sub3; Training Loss: 0.723336398601532, Val Loss: 0.8098108172416687\n",
      "Step: 1320, Subject: sub4; Training Loss: 0.6836912631988525, Val Loss: 0.8454795479774475\n",
      "Step: 1320, Subject: sub5; Training Loss: 0.607228696346283, Val Loss: 0.7229342460632324\n",
      "Step: 1320, Subject: sub7; Training Loss: 0.5649441480636597, Val Loss: 0.7482762932777405\n",
      "Step: 1330, Subject: sub1; Training Loss: 0.7226241827011108, Val Loss: 0.7406514883041382\n",
      "Step: 1330, Subject: sub2; Training Loss: 0.6565456986427307, Val Loss: 0.7860555052757263\n",
      "Step: 1330, Subject: sub3; Training Loss: 0.7470406293869019, Val Loss: 0.8693685531616211\n",
      "Step: 1330, Subject: sub4; Training Loss: 0.6954873204231262, Val Loss: 0.8685784935951233\n",
      "Step: 1330, Subject: sub5; Training Loss: 0.6319147348403931, Val Loss: 0.7719898223876953\n",
      "Step: 1330, Subject: sub7; Training Loss: 0.5954225063323975, Val Loss: 0.7647385597229004\n",
      "Step: 1340, Subject: sub1; Training Loss: 0.7493603229522705, Val Loss: 0.8485599756240845\n",
      "Step: 1340, Subject: sub2; Training Loss: 0.6652860045433044, Val Loss: 0.848029613494873\n",
      "Step: 1340, Subject: sub3; Training Loss: 0.70455402135849, Val Loss: 0.8844813108444214\n",
      "Step: 1340, Subject: sub4; Training Loss: 0.625508189201355, Val Loss: 0.8342305421829224\n",
      "Step: 1340, Subject: sub5; Training Loss: 0.5581923127174377, Val Loss: 0.6965811252593994\n",
      "Step: 1340, Subject: sub7; Training Loss: 0.6038773059844971, Val Loss: 0.7664766311645508\n",
      "Step: 1350, Subject: sub1; Training Loss: 0.7522128820419312, Val Loss: 0.809265673160553\n",
      "Step: 1350, Subject: sub2; Training Loss: 0.6513510942459106, Val Loss: 0.8250016570091248\n",
      "Step: 1350, Subject: sub3; Training Loss: 0.703231692314148, Val Loss: 0.8818225264549255\n",
      "Step: 1350, Subject: sub4; Training Loss: 0.7652595043182373, Val Loss: 0.8467962145805359\n",
      "Step: 1350, Subject: sub5; Training Loss: 0.5920130014419556, Val Loss: 0.814781904220581\n",
      "Step: 1350, Subject: sub7; Training Loss: 0.5613495707511902, Val Loss: 0.893344521522522\n",
      "Step: 1360, Subject: sub1; Training Loss: 0.7566653490066528, Val Loss: 0.8648720383644104\n",
      "Step: 1360, Subject: sub2; Training Loss: 0.6914663314819336, Val Loss: 0.8618564009666443\n",
      "Step: 1360, Subject: sub3; Training Loss: 0.6776227951049805, Val Loss: 0.86995530128479\n",
      "Step: 1360, Subject: sub4; Training Loss: 0.6773281097412109, Val Loss: 0.8189635872840881\n",
      "Step: 1360, Subject: sub5; Training Loss: 0.6126379370689392, Val Loss: 0.824565052986145\n",
      "Step: 1360, Subject: sub7; Training Loss: 0.6102869510650635, Val Loss: 0.818617582321167\n",
      "Step: 1370, Subject: sub1; Training Loss: 0.7571574449539185, Val Loss: 0.9089391827583313\n",
      "Step: 1370, Subject: sub2; Training Loss: 0.5849732160568237, Val Loss: 0.8025038242340088\n",
      "Step: 1370, Subject: sub3; Training Loss: 0.7863926291465759, Val Loss: 0.8589047193527222\n",
      "Step: 1370, Subject: sub4; Training Loss: 0.6715296506881714, Val Loss: 0.8856307864189148\n",
      "Step: 1370, Subject: sub5; Training Loss: 0.6400180459022522, Val Loss: 0.7705036401748657\n",
      "Step: 1370, Subject: sub7; Training Loss: 0.612288236618042, Val Loss: 0.7069025039672852\n",
      "Step: 1380, Subject: sub1; Training Loss: 0.8094077706336975, Val Loss: 0.8337149620056152\n",
      "Step: 1380, Subject: sub2; Training Loss: 0.7056694030761719, Val Loss: 0.8436558246612549\n",
      "Step: 1380, Subject: sub3; Training Loss: 0.7864224314689636, Val Loss: 0.824273943901062\n",
      "Step: 1380, Subject: sub4; Training Loss: 0.6624466180801392, Val Loss: 0.8294579982757568\n",
      "Step: 1380, Subject: sub5; Training Loss: 0.616333544254303, Val Loss: 0.7322609424591064\n",
      "Step: 1380, Subject: sub7; Training Loss: 0.5876471996307373, Val Loss: 0.8192121386528015\n",
      "Step: 1390, Subject: sub1; Training Loss: 0.7914266586303711, Val Loss: 0.8456723690032959\n",
      "Step: 1390, Subject: sub2; Training Loss: 0.6474646329879761, Val Loss: 0.8691747784614563\n",
      "Step: 1390, Subject: sub3; Training Loss: 0.806072473526001, Val Loss: 0.861290693283081\n",
      "Step: 1390, Subject: sub4; Training Loss: 0.7040544152259827, Val Loss: 0.8590422868728638\n",
      "Step: 1390, Subject: sub5; Training Loss: 0.584305465221405, Val Loss: 0.7411872148513794\n",
      "Step: 1390, Subject: sub7; Training Loss: 0.5986908674240112, Val Loss: 0.788669228553772\n",
      "Step: 1400, Subject: sub1; Training Loss: 0.7329579591751099, Val Loss: 0.8204166293144226\n",
      "Step: 1400, Subject: sub2; Training Loss: 0.6251433491706848, Val Loss: 0.790264904499054\n",
      "Step: 1400, Subject: sub3; Training Loss: 0.7265896797180176, Val Loss: 0.8813652992248535\n",
      "Step: 1400, Subject: sub4; Training Loss: 0.6088100671768188, Val Loss: 0.8435141444206238\n",
      "Step: 1400, Subject: sub5; Training Loss: 0.6123508214950562, Val Loss: 0.857412576675415\n",
      "Step: 1400, Subject: sub7; Training Loss: 0.6127864122390747, Val Loss: 0.914086103439331\n",
      "Step: 1410, Subject: sub1; Training Loss: 0.8477029800415039, Val Loss: 0.8246326446533203\n",
      "Step: 1410, Subject: sub2; Training Loss: 0.585564374923706, Val Loss: 0.850426971912384\n",
      "Step: 1410, Subject: sub3; Training Loss: 0.7877604961395264, Val Loss: 0.8578143119812012\n",
      "Step: 1410, Subject: sub4; Training Loss: 0.6554419994354248, Val Loss: 0.907546877861023\n",
      "Step: 1410, Subject: sub5; Training Loss: 0.6140478849411011, Val Loss: 0.7729020714759827\n",
      "Step: 1410, Subject: sub7; Training Loss: 0.5887405872344971, Val Loss: 0.8061825037002563\n",
      "Step: 1420, Subject: sub1; Training Loss: 0.7506186962127686, Val Loss: 0.8220993280410767\n",
      "Step: 1420, Subject: sub2; Training Loss: 0.614648163318634, Val Loss: 0.8411489725112915\n",
      "Step: 1420, Subject: sub3; Training Loss: 0.741226315498352, Val Loss: 0.8113237619400024\n",
      "Step: 1420, Subject: sub4; Training Loss: 0.6082414388656616, Val Loss: 0.8925255537033081\n",
      "Step: 1420, Subject: sub5; Training Loss: 0.5722440481185913, Val Loss: 0.7698135375976562\n",
      "Step: 1420, Subject: sub7; Training Loss: 0.6000456809997559, Val Loss: 0.7736585140228271\n",
      "Step: 1430, Subject: sub1; Training Loss: 0.7785731554031372, Val Loss: 0.8638670444488525\n",
      "Step: 1430, Subject: sub2; Training Loss: 0.6555418968200684, Val Loss: 0.8957108855247498\n",
      "Step: 1430, Subject: sub3; Training Loss: 0.8097552061080933, Val Loss: 0.8229992985725403\n",
      "Step: 1430, Subject: sub4; Training Loss: 0.6274999976158142, Val Loss: 0.850050687789917\n",
      "Step: 1430, Subject: sub5; Training Loss: 0.5988506078720093, Val Loss: 0.7783569097518921\n",
      "Step: 1430, Subject: sub7; Training Loss: 0.5885826945304871, Val Loss: 0.8474928140640259\n",
      "Step: 1440, Subject: sub1; Training Loss: 0.7047882080078125, Val Loss: 0.8328598737716675\n",
      "Step: 1440, Subject: sub2; Training Loss: 0.7225185632705688, Val Loss: 0.835067629814148\n",
      "Step: 1440, Subject: sub3; Training Loss: 0.7446045875549316, Val Loss: 0.8608080148696899\n",
      "Step: 1440, Subject: sub4; Training Loss: 0.6353046894073486, Val Loss: 0.8407617211341858\n",
      "Step: 1440, Subject: sub5; Training Loss: 0.5885318517684937, Val Loss: 0.8207376003265381\n",
      "Step: 1440, Subject: sub7; Training Loss: 0.5627169609069824, Val Loss: 0.78958660364151\n",
      "Step: 1450, Subject: sub1; Training Loss: 0.7931653261184692, Val Loss: 0.7842835783958435\n",
      "Step: 1450, Subject: sub2; Training Loss: 0.683486819267273, Val Loss: 0.8579254150390625\n",
      "Step: 1450, Subject: sub3; Training Loss: 0.7497295141220093, Val Loss: 0.8375277519226074\n",
      "Step: 1450, Subject: sub4; Training Loss: 0.6346843838691711, Val Loss: 0.8357728719711304\n",
      "Step: 1450, Subject: sub5; Training Loss: 0.6103072166442871, Val Loss: 0.7563243508338928\n",
      "Step: 1450, Subject: sub7; Training Loss: 0.5653021335601807, Val Loss: 0.7608999013900757\n",
      "Step: 1460, Subject: sub1; Training Loss: 0.7883337736129761, Val Loss: 0.8329280614852905\n",
      "Step: 1460, Subject: sub2; Training Loss: 0.6539964079856873, Val Loss: 0.8481743335723877\n",
      "Step: 1460, Subject: sub3; Training Loss: 0.7287619113922119, Val Loss: 0.8693166971206665\n",
      "Step: 1460, Subject: sub4; Training Loss: 0.6755173206329346, Val Loss: 0.8947256803512573\n",
      "Step: 1460, Subject: sub5; Training Loss: 0.626354992389679, Val Loss: 0.7525973320007324\n",
      "Step: 1460, Subject: sub7; Training Loss: 0.5621688365936279, Val Loss: 0.863882303237915\n",
      "Step: 1470, Subject: sub1; Training Loss: 0.7888573408126831, Val Loss: 0.8662478923797607\n",
      "Step: 1470, Subject: sub2; Training Loss: 0.6398446559906006, Val Loss: 0.8095738887786865\n",
      "Step: 1470, Subject: sub3; Training Loss: 0.7144467234611511, Val Loss: 0.834683895111084\n",
      "Step: 1470, Subject: sub4; Training Loss: 0.6099357008934021, Val Loss: 0.8656991124153137\n",
      "Step: 1470, Subject: sub5; Training Loss: 0.5741785764694214, Val Loss: 0.7804818153381348\n",
      "Step: 1470, Subject: sub7; Training Loss: 0.5566373467445374, Val Loss: 0.7265821695327759\n",
      "Step: 1480, Subject: sub1; Training Loss: 0.7079850435256958, Val Loss: 0.7272603511810303\n",
      "Step: 1480, Subject: sub2; Training Loss: 0.6656061410903931, Val Loss: 0.8698259592056274\n",
      "Step: 1480, Subject: sub3; Training Loss: 0.6903275847434998, Val Loss: 0.864372968673706\n",
      "Step: 1480, Subject: sub4; Training Loss: 0.5861654877662659, Val Loss: 0.8622603416442871\n",
      "Step: 1480, Subject: sub5; Training Loss: 0.528315544128418, Val Loss: 0.7810815572738647\n",
      "Step: 1480, Subject: sub7; Training Loss: 0.6352744102478027, Val Loss: 0.7510042190551758\n",
      "Step: 1490, Subject: sub1; Training Loss: 0.7518694400787354, Val Loss: 0.8598190546035767\n",
      "Step: 1490, Subject: sub2; Training Loss: 0.6266837120056152, Val Loss: 0.8123442530632019\n",
      "Step: 1490, Subject: sub3; Training Loss: 0.7245864868164062, Val Loss: 0.8398136496543884\n",
      "Step: 1490, Subject: sub4; Training Loss: 0.5355080366134644, Val Loss: 0.8548948764801025\n",
      "Step: 1490, Subject: sub5; Training Loss: 0.5588598251342773, Val Loss: 0.6951842308044434\n",
      "Step: 1490, Subject: sub7; Training Loss: 0.5697386860847473, Val Loss: 0.7753034830093384\n",
      "Step: 1500, Subject: sub1; Training Loss: 0.7700159549713135, Val Loss: 0.7992467880249023\n",
      "Step: 1500, Subject: sub2; Training Loss: 0.6607186794281006, Val Loss: 0.8154083490371704\n",
      "Step: 1500, Subject: sub3; Training Loss: 0.752633810043335, Val Loss: 0.9181420803070068\n",
      "Step: 1500, Subject: sub4; Training Loss: 0.6079047918319702, Val Loss: 0.8375539779663086\n",
      "Step: 1500, Subject: sub5; Training Loss: 0.6000500917434692, Val Loss: 0.7722405195236206\n",
      "Step: 1500, Subject: sub7; Training Loss: 0.5349053144454956, Val Loss: 0.8132632970809937\n",
      "Step: 1510, Subject: sub1; Training Loss: 0.7084335088729858, Val Loss: 0.8832095861434937\n",
      "Step: 1510, Subject: sub2; Training Loss: 0.6130903959274292, Val Loss: 0.8796409964561462\n",
      "Step: 1510, Subject: sub3; Training Loss: 0.7622131705284119, Val Loss: 0.8706586360931396\n",
      "Step: 1510, Subject: sub4; Training Loss: 0.6795817613601685, Val Loss: 0.8384417295455933\n",
      "Step: 1510, Subject: sub5; Training Loss: 0.5566409230232239, Val Loss: 0.6806662678718567\n",
      "Step: 1510, Subject: sub7; Training Loss: 0.5881341695785522, Val Loss: 0.861492395401001\n",
      "Step: 1520, Subject: sub1; Training Loss: 0.7323890924453735, Val Loss: 0.7711912393569946\n",
      "Step: 1520, Subject: sub2; Training Loss: 0.6945347189903259, Val Loss: 0.8691748380661011\n",
      "Step: 1520, Subject: sub3; Training Loss: 0.6689391136169434, Val Loss: 0.8522940278053284\n",
      "Step: 1520, Subject: sub4; Training Loss: 0.6431344151496887, Val Loss: 0.8779554963111877\n",
      "Step: 1520, Subject: sub5; Training Loss: 0.5669048428535461, Val Loss: 0.7195864319801331\n",
      "Step: 1520, Subject: sub7; Training Loss: 0.6182733774185181, Val Loss: 0.7587485909461975\n",
      "Step: 1530, Subject: sub1; Training Loss: 0.7515156269073486, Val Loss: 0.8329810500144958\n",
      "Step: 1530, Subject: sub2; Training Loss: 0.5759402513504028, Val Loss: 0.8225376605987549\n",
      "Step: 1530, Subject: sub3; Training Loss: 0.685938835144043, Val Loss: 0.8545047044754028\n",
      "Step: 1530, Subject: sub4; Training Loss: 0.5626915693283081, Val Loss: 0.8280421495437622\n",
      "Step: 1530, Subject: sub5; Training Loss: 0.5437784194946289, Val Loss: 0.735744833946228\n",
      "Step: 1530, Subject: sub7; Training Loss: 0.6103436946868896, Val Loss: 0.9204456806182861\n",
      "Step: 1540, Subject: sub1; Training Loss: 0.7470875978469849, Val Loss: 0.808640718460083\n",
      "Step: 1540, Subject: sub2; Training Loss: 0.5495091080665588, Val Loss: 0.8454872965812683\n",
      "Step: 1540, Subject: sub3; Training Loss: 0.7341110706329346, Val Loss: 0.807957112789154\n",
      "Step: 1540, Subject: sub4; Training Loss: 0.6139534711837769, Val Loss: 0.871108889579773\n",
      "Step: 1540, Subject: sub5; Training Loss: 0.5942394733428955, Val Loss: 0.7591059803962708\n",
      "Step: 1540, Subject: sub7; Training Loss: 0.5751620531082153, Val Loss: 0.8342968225479126\n",
      "Step: 1550, Subject: sub1; Training Loss: 0.7206616401672363, Val Loss: 0.7947525978088379\n",
      "Step: 1550, Subject: sub2; Training Loss: 0.6016232967376709, Val Loss: 0.8642479181289673\n",
      "Step: 1550, Subject: sub3; Training Loss: 0.7625904679298401, Val Loss: 1.0791866779327393\n",
      "Step: 1550, Subject: sub4; Training Loss: 0.567771852016449, Val Loss: 0.8598267436027527\n",
      "Step: 1550, Subject: sub5; Training Loss: 0.5599076151847839, Val Loss: 0.6696904897689819\n",
      "Step: 1550, Subject: sub7; Training Loss: 0.6006750464439392, Val Loss: 0.7720966339111328\n",
      "Step: 1560, Subject: sub1; Training Loss: 0.7604421377182007, Val Loss: 0.9011603593826294\n",
      "Step: 1560, Subject: sub2; Training Loss: 0.5279635190963745, Val Loss: 0.9397904872894287\n",
      "Step: 1560, Subject: sub3; Training Loss: 0.7033143043518066, Val Loss: 0.8233246803283691\n",
      "Step: 1560, Subject: sub4; Training Loss: 0.6003708839416504, Val Loss: 0.7860018014907837\n",
      "Step: 1560, Subject: sub5; Training Loss: 0.587393045425415, Val Loss: 0.838333010673523\n",
      "Step: 1560, Subject: sub7; Training Loss: 0.5784941911697388, Val Loss: 0.7497035264968872\n",
      "Step: 1570, Subject: sub1; Training Loss: 0.7766045928001404, Val Loss: 0.9437369704246521\n",
      "Step: 1570, Subject: sub2; Training Loss: 0.61838698387146, Val Loss: 0.9030023217201233\n",
      "Step: 1570, Subject: sub3; Training Loss: 0.75002121925354, Val Loss: 0.8279332518577576\n",
      "Step: 1570, Subject: sub4; Training Loss: 0.5301034450531006, Val Loss: 0.8776897192001343\n",
      "Step: 1570, Subject: sub5; Training Loss: 0.5839059352874756, Val Loss: 0.6439560651779175\n",
      "Saving model with validation loss: 0.6439560651779175\n",
      "\n",
      "Step: 1570, Subject: sub7; Training Loss: 0.6238729953765869, Val Loss: 0.8187503218650818\n",
      "Step: 1580, Subject: sub1; Training Loss: 0.7318949699401855, Val Loss: 0.8707027435302734\n",
      "Step: 1580, Subject: sub2; Training Loss: 0.5738093852996826, Val Loss: 0.8109143376350403\n",
      "Step: 1580, Subject: sub3; Training Loss: 0.7857089638710022, Val Loss: 0.8563925623893738\n",
      "Step: 1580, Subject: sub4; Training Loss: 0.585430383682251, Val Loss: 0.7962247729301453\n",
      "Step: 1580, Subject: sub5; Training Loss: 0.5962772369384766, Val Loss: 0.7915668487548828\n",
      "Step: 1580, Subject: sub7; Training Loss: 0.5847918391227722, Val Loss: 0.9125692844390869\n",
      "Step: 1590, Subject: sub1; Training Loss: 0.6648154258728027, Val Loss: 0.9008761048316956\n",
      "Step: 1590, Subject: sub2; Training Loss: 0.6096688508987427, Val Loss: 0.8940114974975586\n",
      "Step: 1590, Subject: sub3; Training Loss: 0.7511942982673645, Val Loss: 0.803282618522644\n",
      "Step: 1590, Subject: sub4; Training Loss: 0.5476244688034058, Val Loss: 0.8318967819213867\n",
      "Step: 1590, Subject: sub5; Training Loss: 0.5993977785110474, Val Loss: 0.9029256701469421\n",
      "Step: 1590, Subject: sub7; Training Loss: 0.5728857517242432, Val Loss: 0.8520634770393372\n",
      "Step: 1600, Subject: sub1; Training Loss: 0.717218279838562, Val Loss: 0.9881027936935425\n",
      "Step: 1600, Subject: sub2; Training Loss: 0.5700719952583313, Val Loss: 0.8893378973007202\n",
      "Step: 1600, Subject: sub3; Training Loss: 0.6942816972732544, Val Loss: 0.8918445110321045\n",
      "Step: 1600, Subject: sub4; Training Loss: 0.5745542049407959, Val Loss: 0.891934871673584\n",
      "Step: 1600, Subject: sub5; Training Loss: 0.5795422792434692, Val Loss: 0.7512543201446533\n",
      "Step: 1600, Subject: sub7; Training Loss: 0.5551083087921143, Val Loss: 0.7865822315216064\n",
      "Step: 1610, Subject: sub1; Training Loss: 0.7294411063194275, Val Loss: 0.9356733560562134\n",
      "Step: 1610, Subject: sub2; Training Loss: 0.5667457580566406, Val Loss: 0.8876211643218994\n",
      "Step: 1610, Subject: sub3; Training Loss: 0.654000461101532, Val Loss: 0.8470536470413208\n",
      "Step: 1610, Subject: sub4; Training Loss: 0.5792906284332275, Val Loss: 0.868594765663147\n",
      "Step: 1610, Subject: sub5; Training Loss: 0.5753400921821594, Val Loss: 0.850156307220459\n",
      "Step: 1610, Subject: sub7; Training Loss: 0.6110076308250427, Val Loss: 0.7792941331863403\n",
      "Step: 1620, Subject: sub1; Training Loss: 0.6842100024223328, Val Loss: 0.8585786819458008\n",
      "Step: 1620, Subject: sub2; Training Loss: 0.5880188941955566, Val Loss: 0.8633993268013\n",
      "Step: 1620, Subject: sub3; Training Loss: 0.7927824258804321, Val Loss: 0.8568698167800903\n",
      "Step: 1620, Subject: sub4; Training Loss: 0.5431506037712097, Val Loss: 0.8143136501312256\n",
      "Step: 1620, Subject: sub5; Training Loss: 0.6343127489089966, Val Loss: 0.8073755502700806\n",
      "Step: 1620, Subject: sub7; Training Loss: 0.5982081294059753, Val Loss: 0.855347752571106\n",
      "Step: 1630, Subject: sub1; Training Loss: 0.7184416055679321, Val Loss: 0.995408833026886\n",
      "Step: 1630, Subject: sub2; Training Loss: 0.6337498426437378, Val Loss: 0.8071534037590027\n",
      "Step: 1630, Subject: sub3; Training Loss: 0.8007731437683105, Val Loss: 0.8491598963737488\n",
      "Step: 1630, Subject: sub4; Training Loss: 0.5007673501968384, Val Loss: 0.8663673996925354\n",
      "Step: 1630, Subject: sub5; Training Loss: 0.5811874866485596, Val Loss: 0.7466223835945129\n",
      "Step: 1630, Subject: sub7; Training Loss: 0.588657796382904, Val Loss: 0.7404597997665405\n",
      "Step: 1640, Subject: sub1; Training Loss: 0.7065432071685791, Val Loss: 0.7674816250801086\n",
      "Step: 1640, Subject: sub2; Training Loss: 0.655806303024292, Val Loss: 0.8291661143302917\n",
      "Step: 1640, Subject: sub3; Training Loss: 0.7476555109024048, Val Loss: 0.8123937845230103\n",
      "Step: 1640, Subject: sub4; Training Loss: 0.5583879351615906, Val Loss: 0.7984411716461182\n",
      "Step: 1640, Subject: sub5; Training Loss: 0.5512990951538086, Val Loss: 0.7747845649719238\n",
      "Step: 1640, Subject: sub7; Training Loss: 0.5440340638160706, Val Loss: 0.8008390665054321\n",
      "Step: 1650, Subject: sub1; Training Loss: 0.807826042175293, Val Loss: 0.9680503606796265\n",
      "Step: 1650, Subject: sub2; Training Loss: 0.534860372543335, Val Loss: 0.810297966003418\n",
      "Step: 1650, Subject: sub3; Training Loss: 0.6940078735351562, Val Loss: 0.8569202423095703\n",
      "Step: 1650, Subject: sub4; Training Loss: 0.6326318979263306, Val Loss: 0.8678879141807556\n",
      "Step: 1650, Subject: sub5; Training Loss: 0.5591562986373901, Val Loss: 0.8142141699790955\n",
      "Step: 1650, Subject: sub7; Training Loss: 0.5862253904342651, Val Loss: 0.9361730813980103\n",
      "Step: 1660, Subject: sub1; Training Loss: 0.7133677005767822, Val Loss: 0.8219893574714661\n",
      "Step: 1660, Subject: sub2; Training Loss: 0.5341838598251343, Val Loss: 0.8463717699050903\n",
      "Step: 1660, Subject: sub3; Training Loss: 0.745694637298584, Val Loss: 0.9940256476402283\n",
      "Step: 1660, Subject: sub4; Training Loss: 0.5925130844116211, Val Loss: 0.8713561296463013\n",
      "Step: 1660, Subject: sub5; Training Loss: 0.5930772423744202, Val Loss: 0.8242608308792114\n",
      "Step: 1660, Subject: sub7; Training Loss: 0.5911644697189331, Val Loss: 0.9256017208099365\n",
      "Step: 1670, Subject: sub1; Training Loss: 0.7657963037490845, Val Loss: 1.0613689422607422\n",
      "Step: 1670, Subject: sub2; Training Loss: 0.5643008947372437, Val Loss: 0.8798270225524902\n",
      "Step: 1670, Subject: sub3; Training Loss: 0.7150309085845947, Val Loss: 0.9269534349441528\n",
      "Step: 1670, Subject: sub4; Training Loss: 0.5096350312232971, Val Loss: 0.8615540266036987\n",
      "Step: 1670, Subject: sub5; Training Loss: 0.5835816860198975, Val Loss: 0.8067151308059692\n",
      "Step: 1670, Subject: sub7; Training Loss: 0.5966769456863403, Val Loss: 0.7981848120689392\n",
      "Step: 1680, Subject: sub1; Training Loss: 0.7539232969284058, Val Loss: 1.0826752185821533\n",
      "Step: 1680, Subject: sub2; Training Loss: 0.6447722911834717, Val Loss: 0.8327880501747131\n",
      "Step: 1680, Subject: sub3; Training Loss: 0.7623946666717529, Val Loss: 1.0368692874908447\n",
      "Step: 1680, Subject: sub4; Training Loss: 0.61713707447052, Val Loss: 0.8581258654594421\n",
      "Step: 1680, Subject: sub5; Training Loss: 0.4989362359046936, Val Loss: 0.9054433107376099\n",
      "Step: 1680, Subject: sub7; Training Loss: 0.5693531036376953, Val Loss: 0.947017252445221\n",
      "Step: 1690, Subject: sub1; Training Loss: 0.7297661304473877, Val Loss: 0.7310414910316467\n",
      "Step: 1690, Subject: sub2; Training Loss: 0.5727633237838745, Val Loss: 0.8793947696685791\n",
      "Step: 1690, Subject: sub3; Training Loss: 0.6996504068374634, Val Loss: 1.0504984855651855\n",
      "Step: 1690, Subject: sub4; Training Loss: 0.5641835927963257, Val Loss: 0.9001946449279785\n",
      "Step: 1690, Subject: sub5; Training Loss: 0.5606456995010376, Val Loss: 0.8492550253868103\n",
      "Step: 1690, Subject: sub7; Training Loss: 0.521898627281189, Val Loss: 1.2016279697418213\n",
      "Step: 1700, Subject: sub1; Training Loss: 0.6648944020271301, Val Loss: 1.1363904476165771\n",
      "Step: 1700, Subject: sub2; Training Loss: 0.5349158644676208, Val Loss: 0.8767237663269043\n",
      "Step: 1700, Subject: sub3; Training Loss: 0.7732064127922058, Val Loss: 0.9815248250961304\n",
      "Step: 1700, Subject: sub4; Training Loss: 0.5011774897575378, Val Loss: 0.8319330215454102\n",
      "Step: 1700, Subject: sub5; Training Loss: 0.5630744099617004, Val Loss: 0.8253597021102905\n",
      "Step: 1700, Subject: sub7; Training Loss: 0.5768526792526245, Val Loss: 1.097765564918518\n",
      "Step: 1710, Subject: sub1; Training Loss: 0.726216197013855, Val Loss: 0.789553701877594\n",
      "Step: 1710, Subject: sub2; Training Loss: 0.5177338719367981, Val Loss: 0.8887161016464233\n",
      "Step: 1710, Subject: sub3; Training Loss: 0.7203169465065002, Val Loss: 0.819553017616272\n",
      "Step: 1710, Subject: sub4; Training Loss: 0.5127624273300171, Val Loss: 0.8536306619644165\n",
      "Step: 1710, Subject: sub5; Training Loss: 0.598081648349762, Val Loss: 0.7635505199432373\n",
      "Step: 1710, Subject: sub7; Training Loss: 0.5392031073570251, Val Loss: 0.931095540523529\n",
      "Step: 1720, Subject: sub1; Training Loss: 0.7182637453079224, Val Loss: 1.065723180770874\n",
      "Step: 1720, Subject: sub2; Training Loss: 0.5027512907981873, Val Loss: 0.849945068359375\n",
      "Step: 1720, Subject: sub3; Training Loss: 0.7509826421737671, Val Loss: 0.986079216003418\n",
      "Step: 1720, Subject: sub4; Training Loss: 0.5939942598342896, Val Loss: 0.8271013498306274\n",
      "Step: 1720, Subject: sub5; Training Loss: 0.5713266730308533, Val Loss: 0.8430970311164856\n",
      "Step: 1720, Subject: sub7; Training Loss: 0.6303951740264893, Val Loss: 0.8981071710586548\n",
      "Step: 1730, Subject: sub1; Training Loss: 0.7967302799224854, Val Loss: 0.9661483764648438\n",
      "Step: 1730, Subject: sub2; Training Loss: 0.640064537525177, Val Loss: 0.9041200876235962\n",
      "Step: 1730, Subject: sub3; Training Loss: 0.6537680625915527, Val Loss: 0.8977181315422058\n",
      "Step: 1730, Subject: sub4; Training Loss: 0.5040002465248108, Val Loss: 0.7787560820579529\n",
      "Step: 1730, Subject: sub5; Training Loss: 0.600793719291687, Val Loss: 0.9544017314910889\n",
      "Step: 1730, Subject: sub7; Training Loss: 0.5610629320144653, Val Loss: 0.8639198541641235\n",
      "Step: 1740, Subject: sub1; Training Loss: 0.7485266923904419, Val Loss: 1.1609302759170532\n",
      "Step: 1740, Subject: sub2; Training Loss: 0.5742360353469849, Val Loss: 0.8574045896530151\n",
      "Step: 1740, Subject: sub3; Training Loss: 0.7002174854278564, Val Loss: 0.780503511428833\n",
      "Step: 1740, Subject: sub4; Training Loss: 0.638489842414856, Val Loss: 0.8564322590827942\n",
      "Step: 1740, Subject: sub5; Training Loss: 0.5574991703033447, Val Loss: 0.7127789258956909\n",
      "Step: 1740, Subject: sub7; Training Loss: 0.5191091299057007, Val Loss: 0.8594056963920593\n",
      "Step: 1750, Subject: sub1; Training Loss: 0.6817094683647156, Val Loss: 1.2077507972717285\n",
      "Step: 1750, Subject: sub2; Training Loss: 0.5842756628990173, Val Loss: 0.9251302480697632\n",
      "Step: 1750, Subject: sub3; Training Loss: 0.7183146476745605, Val Loss: 0.982724130153656\n",
      "Step: 1750, Subject: sub4; Training Loss: 0.5009978413581848, Val Loss: 0.8534846305847168\n",
      "Step: 1750, Subject: sub5; Training Loss: 0.6054152250289917, Val Loss: 0.8013393878936768\n",
      "Step: 1750, Subject: sub7; Training Loss: 0.5925467014312744, Val Loss: 0.7874685525894165\n",
      "Step: 1760, Subject: sub1; Training Loss: 0.7392105460166931, Val Loss: 1.2780182361602783\n",
      "Step: 1760, Subject: sub2; Training Loss: 0.5441451072692871, Val Loss: 0.8162562251091003\n",
      "Step: 1760, Subject: sub3; Training Loss: 0.7215579748153687, Val Loss: 0.9819234609603882\n",
      "Step: 1760, Subject: sub4; Training Loss: 0.4820324778556824, Val Loss: 0.8821414709091187\n",
      "Step: 1760, Subject: sub5; Training Loss: 0.6269474029541016, Val Loss: 1.00529146194458\n",
      "Step: 1760, Subject: sub7; Training Loss: 0.5614028573036194, Val Loss: 1.0256954431533813\n",
      "Step: 1770, Subject: sub1; Training Loss: 0.6860643625259399, Val Loss: 0.9892724752426147\n",
      "Step: 1770, Subject: sub2; Training Loss: 0.6075267195701599, Val Loss: 0.8453665375709534\n",
      "Step: 1770, Subject: sub3; Training Loss: 0.7332244515419006, Val Loss: 0.8132554292678833\n",
      "Step: 1770, Subject: sub4; Training Loss: 0.39651232957839966, Val Loss: 0.8249748349189758\n",
      "Step: 1770, Subject: sub5; Training Loss: 0.5095387697219849, Val Loss: 0.8200164437294006\n",
      "Step: 1770, Subject: sub7; Training Loss: 0.5845181345939636, Val Loss: 0.976448118686676\n",
      "Step: 1780, Subject: sub1; Training Loss: 0.6920528411865234, Val Loss: 0.8647734522819519\n",
      "Step: 1780, Subject: sub2; Training Loss: 0.5141801834106445, Val Loss: 0.8876668810844421\n",
      "Step: 1780, Subject: sub3; Training Loss: 0.6742799282073975, Val Loss: 0.9401758909225464\n",
      "Step: 1780, Subject: sub4; Training Loss: 0.5339429378509521, Val Loss: 0.8653781414031982\n",
      "Step: 1780, Subject: sub5; Training Loss: 0.5779412388801575, Val Loss: 0.7964862585067749\n",
      "Step: 1780, Subject: sub7; Training Loss: 0.5687520503997803, Val Loss: 0.7608242034912109\n",
      "Step: 1790, Subject: sub1; Training Loss: 0.7210658192634583, Val Loss: 0.9362437129020691\n",
      "Step: 1790, Subject: sub2; Training Loss: 0.5903640985488892, Val Loss: 0.9025461077690125\n",
      "Step: 1790, Subject: sub3; Training Loss: 0.7340558767318726, Val Loss: 0.8795843124389648\n",
      "Step: 1790, Subject: sub4; Training Loss: 0.5647317171096802, Val Loss: 0.8968796133995056\n",
      "Step: 1790, Subject: sub5; Training Loss: 0.5084484815597534, Val Loss: 1.0012412071228027\n",
      "Step: 1790, Subject: sub7; Training Loss: 0.5767827033996582, Val Loss: 1.054545521736145\n",
      "Step: 1800, Subject: sub1; Training Loss: 0.588151216506958, Val Loss: 0.8126873970031738\n",
      "Step: 1800, Subject: sub2; Training Loss: 0.528315544128418, Val Loss: 0.9098656177520752\n",
      "Step: 1800, Subject: sub3; Training Loss: 0.6972731947898865, Val Loss: 0.8980733156204224\n",
      "Step: 1800, Subject: sub4; Training Loss: 0.5564770698547363, Val Loss: 0.954186201095581\n",
      "Step: 1800, Subject: sub5; Training Loss: 0.5696434378623962, Val Loss: 1.0230011940002441\n",
      "Step: 1800, Subject: sub7; Training Loss: 0.5280842781066895, Val Loss: 1.3077325820922852\n",
      "Step: 1810, Subject: sub1; Training Loss: 0.709251880645752, Val Loss: 0.8400371074676514\n",
      "Step: 1810, Subject: sub2; Training Loss: 0.5276530385017395, Val Loss: 0.8236631751060486\n",
      "Step: 1810, Subject: sub3; Training Loss: 0.7244160175323486, Val Loss: 0.9734402894973755\n",
      "Step: 1810, Subject: sub4; Training Loss: 0.4869644045829773, Val Loss: 0.8959560394287109\n",
      "Step: 1810, Subject: sub5; Training Loss: 0.5144302248954773, Val Loss: 0.7797173857688904\n",
      "Step: 1810, Subject: sub7; Training Loss: 0.5439357757568359, Val Loss: 1.0361387729644775\n",
      "Step: 1820, Subject: sub1; Training Loss: 0.5954194664955139, Val Loss: 0.8904680013656616\n",
      "Step: 1820, Subject: sub2; Training Loss: 0.5256479382514954, Val Loss: 0.8648009300231934\n",
      "Step: 1820, Subject: sub3; Training Loss: 0.759221613407135, Val Loss: 0.9381150007247925\n",
      "Step: 1820, Subject: sub4; Training Loss: 0.4922214448451996, Val Loss: 0.9869663715362549\n",
      "Step: 1820, Subject: sub5; Training Loss: 0.5847363471984863, Val Loss: 0.9985801577568054\n",
      "Step: 1820, Subject: sub7; Training Loss: 0.549053430557251, Val Loss: 0.9421871304512024\n",
      "Step: 1830, Subject: sub1; Training Loss: 0.7141194343566895, Val Loss: 0.9491332173347473\n",
      "Step: 1830, Subject: sub2; Training Loss: 0.4529832899570465, Val Loss: 0.7968580722808838\n",
      "Step: 1830, Subject: sub3; Training Loss: 0.776400089263916, Val Loss: 0.9442338943481445\n",
      "Step: 1830, Subject: sub4; Training Loss: 0.424867182970047, Val Loss: 0.9289531707763672\n",
      "Step: 1830, Subject: sub5; Training Loss: 0.5869174003601074, Val Loss: 0.7819167375564575\n",
      "Step: 1830, Subject: sub7; Training Loss: 0.5626789927482605, Val Loss: 0.8921701908111572\n",
      "Step: 1840, Subject: sub1; Training Loss: 0.7303223609924316, Val Loss: 1.0730628967285156\n",
      "Step: 1840, Subject: sub2; Training Loss: 0.5315110683441162, Val Loss: 0.8985929489135742\n",
      "Step: 1840, Subject: sub3; Training Loss: 0.7425474524497986, Val Loss: 0.9132238030433655\n",
      "Step: 1840, Subject: sub4; Training Loss: 0.43745505809783936, Val Loss: 1.0324708223342896\n",
      "Step: 1840, Subject: sub5; Training Loss: 0.5069637298583984, Val Loss: 0.7996608018875122\n",
      "Step: 1840, Subject: sub7; Training Loss: 0.5496805310249329, Val Loss: 0.8971538543701172\n",
      "Step: 1850, Subject: sub1; Training Loss: 0.6568037271499634, Val Loss: 1.1836049556732178\n",
      "Step: 1850, Subject: sub2; Training Loss: 0.4837297201156616, Val Loss: 0.8819483518600464\n",
      "Step: 1850, Subject: sub3; Training Loss: 0.743901789188385, Val Loss: 0.9186855554580688\n",
      "Step: 1850, Subject: sub4; Training Loss: 0.4736443758010864, Val Loss: 0.8099606037139893\n",
      "Step: 1850, Subject: sub5; Training Loss: 0.5910216569900513, Val Loss: 0.7626394033432007\n",
      "Step: 1850, Subject: sub7; Training Loss: 0.5662870407104492, Val Loss: 0.8614903688430786\n",
      "Step: 1860, Subject: sub1; Training Loss: 0.6406720876693726, Val Loss: 0.9592687487602234\n",
      "Step: 1860, Subject: sub2; Training Loss: 0.4977561831474304, Val Loss: 0.8296674489974976\n",
      "Step: 1860, Subject: sub3; Training Loss: 0.7543031573295593, Val Loss: 0.839870035648346\n",
      "Step: 1860, Subject: sub4; Training Loss: 0.5070548057556152, Val Loss: 0.8444089293479919\n",
      "Step: 1860, Subject: sub5; Training Loss: 0.47893351316452026, Val Loss: 0.8248377442359924\n",
      "Step: 1860, Subject: sub7; Training Loss: 0.5847673416137695, Val Loss: 1.0450079441070557\n",
      "Step: 1870, Subject: sub1; Training Loss: 0.6473362445831299, Val Loss: 0.8264350295066833\n",
      "Step: 1870, Subject: sub2; Training Loss: 0.4852035641670227, Val Loss: 0.8750189542770386\n",
      "Step: 1870, Subject: sub3; Training Loss: 0.7342888712882996, Val Loss: 0.9078330993652344\n",
      "Step: 1870, Subject: sub4; Training Loss: 0.4995208978652954, Val Loss: 0.7954250574111938\n",
      "Step: 1870, Subject: sub5; Training Loss: 0.5311813354492188, Val Loss: 0.901858925819397\n",
      "Step: 1870, Subject: sub7; Training Loss: 0.5550761222839355, Val Loss: 1.136564016342163\n",
      "Step: 1880, Subject: sub1; Training Loss: 0.6041226983070374, Val Loss: 0.8013449907302856\n",
      "Step: 1880, Subject: sub2; Training Loss: 0.4732794165611267, Val Loss: 0.8413897156715393\n",
      "Step: 1880, Subject: sub3; Training Loss: 0.6652880311012268, Val Loss: 0.8277649879455566\n",
      "Step: 1880, Subject: sub4; Training Loss: 0.4704080820083618, Val Loss: 0.9023545384407043\n",
      "Step: 1880, Subject: sub5; Training Loss: 0.5881197452545166, Val Loss: 0.7469577193260193\n",
      "Step: 1880, Subject: sub7; Training Loss: 0.5310102701187134, Val Loss: 1.0234649181365967\n",
      "Step: 1890, Subject: sub1; Training Loss: 0.6475869417190552, Val Loss: 0.8704365491867065\n",
      "Step: 1890, Subject: sub2; Training Loss: 0.4646487832069397, Val Loss: 0.8121770620346069\n",
      "Step: 1890, Subject: sub3; Training Loss: 0.7079079151153564, Val Loss: 0.8190114498138428\n",
      "Step: 1890, Subject: sub4; Training Loss: 0.430880069732666, Val Loss: 0.9555080533027649\n",
      "Step: 1890, Subject: sub5; Training Loss: 0.5833961963653564, Val Loss: 0.9787721037864685\n",
      "Step: 1890, Subject: sub7; Training Loss: 0.5371084809303284, Val Loss: 0.9982444643974304\n",
      "Step: 1900, Subject: sub1; Training Loss: 0.6762675642967224, Val Loss: 1.0982794761657715\n",
      "Step: 1900, Subject: sub2; Training Loss: 0.49996158480644226, Val Loss: 0.8039907813072205\n",
      "Step: 1900, Subject: sub3; Training Loss: 0.7012211084365845, Val Loss: 0.8658953309059143\n",
      "Step: 1900, Subject: sub4; Training Loss: 0.4864804446697235, Val Loss: 0.8449767231941223\n",
      "Step: 1900, Subject: sub5; Training Loss: 0.5720007419586182, Val Loss: 0.8653405904769897\n",
      "Step: 1900, Subject: sub7; Training Loss: 0.557489812374115, Val Loss: 1.1223024129867554\n",
      "Step: 1910, Subject: sub1; Training Loss: 0.6716458797454834, Val Loss: 0.984150767326355\n",
      "Step: 1910, Subject: sub2; Training Loss: 0.4665832817554474, Val Loss: 0.8152942657470703\n",
      "Step: 1910, Subject: sub3; Training Loss: 0.6368667483329773, Val Loss: 0.8988206386566162\n",
      "Step: 1910, Subject: sub4; Training Loss: 0.4453819990158081, Val Loss: 0.929650068283081\n",
      "Step: 1910, Subject: sub5; Training Loss: 0.5081777572631836, Val Loss: 0.7888856530189514\n",
      "Step: 1910, Subject: sub7; Training Loss: 0.5877941846847534, Val Loss: 1.144698143005371\n",
      "Step: 1920, Subject: sub1; Training Loss: 0.598630428314209, Val Loss: 0.8171069622039795\n",
      "Step: 1920, Subject: sub2; Training Loss: 0.4719911813735962, Val Loss: 0.8114227056503296\n",
      "Step: 1920, Subject: sub3; Training Loss: 0.6909000873565674, Val Loss: 0.9321519136428833\n",
      "Step: 1920, Subject: sub4; Training Loss: 0.37152472138404846, Val Loss: 0.9114351272583008\n",
      "Step: 1920, Subject: sub5; Training Loss: 0.510116457939148, Val Loss: 0.7703862190246582\n",
      "Step: 1920, Subject: sub7; Training Loss: 0.5927382707595825, Val Loss: 0.784438967704773\n",
      "Step: 1930, Subject: sub1; Training Loss: 0.6281261444091797, Val Loss: 1.086734414100647\n",
      "Step: 1930, Subject: sub2; Training Loss: 0.47532564401626587, Val Loss: 0.7991087436676025\n",
      "Step: 1930, Subject: sub3; Training Loss: 0.7661176919937134, Val Loss: 1.056343674659729\n",
      "Step: 1930, Subject: sub4; Training Loss: 0.6945885419845581, Val Loss: 0.908966600894928\n",
      "Step: 1930, Subject: sub5; Training Loss: 0.5199364423751831, Val Loss: 0.9316394925117493\n",
      "Step: 1930, Subject: sub7; Training Loss: 0.5956451892852783, Val Loss: 0.9058391451835632\n",
      "Step: 1940, Subject: sub1; Training Loss: 0.685775637626648, Val Loss: 0.9408242702484131\n",
      "Step: 1940, Subject: sub2; Training Loss: 0.4711413085460663, Val Loss: 0.9370095729827881\n",
      "Step: 1940, Subject: sub3; Training Loss: 0.7513913512229919, Val Loss: 1.0884956121444702\n",
      "Step: 1940, Subject: sub4; Training Loss: 0.358201801776886, Val Loss: 1.091489553451538\n",
      "Step: 1940, Subject: sub5; Training Loss: 0.5230243802070618, Val Loss: 0.9145388007164001\n",
      "Step: 1940, Subject: sub7; Training Loss: 0.536476731300354, Val Loss: 0.9022467732429504\n",
      "Step: 1950, Subject: sub1; Training Loss: 0.6574485301971436, Val Loss: 0.9128752946853638\n",
      "Step: 1950, Subject: sub2; Training Loss: 0.5082967877388, Val Loss: 0.8293417096138\n",
      "Step: 1950, Subject: sub3; Training Loss: 0.6401567459106445, Val Loss: 0.9322452545166016\n",
      "Step: 1950, Subject: sub4; Training Loss: 0.4331466257572174, Val Loss: 0.8798787593841553\n",
      "Step: 1950, Subject: sub5; Training Loss: 0.4949135184288025, Val Loss: 0.836880087852478\n",
      "Step: 1950, Subject: sub7; Training Loss: 0.6096805334091187, Val Loss: 1.0870848894119263\n",
      "Step: 1960, Subject: sub1; Training Loss: 0.6402188539505005, Val Loss: 0.953217089176178\n",
      "Step: 1960, Subject: sub2; Training Loss: 0.47551804780960083, Val Loss: 0.8845586180686951\n",
      "Step: 1960, Subject: sub3; Training Loss: 0.7189691066741943, Val Loss: 1.1227437257766724\n",
      "Step: 1960, Subject: sub4; Training Loss: 0.38466811180114746, Val Loss: 1.0387752056121826\n",
      "Step: 1960, Subject: sub5; Training Loss: 0.4845287799835205, Val Loss: 0.7918808460235596\n",
      "Step: 1960, Subject: sub7; Training Loss: 0.5169864296913147, Val Loss: 1.0240733623504639\n",
      "Step: 1970, Subject: sub1; Training Loss: 0.6200405359268188, Val Loss: 1.33418607711792\n",
      "Step: 1970, Subject: sub2; Training Loss: 0.5143142938613892, Val Loss: 0.8916526436805725\n",
      "Step: 1970, Subject: sub3; Training Loss: 0.7008955478668213, Val Loss: 1.0502240657806396\n",
      "Step: 1970, Subject: sub4; Training Loss: 0.5037481784820557, Val Loss: 0.7310649156570435\n",
      "Step: 1970, Subject: sub5; Training Loss: 0.4970565438270569, Val Loss: 0.8791053295135498\n",
      "Step: 1970, Subject: sub7; Training Loss: 0.5490982532501221, Val Loss: 0.8342328071594238\n",
      "Step: 1980, Subject: sub1; Training Loss: 0.5930951833724976, Val Loss: 1.112621784210205\n",
      "Step: 1980, Subject: sub2; Training Loss: 0.5387024879455566, Val Loss: 0.9025241136550903\n",
      "Step: 1980, Subject: sub3; Training Loss: 0.7061529159545898, Val Loss: 1.1445221900939941\n",
      "Step: 1980, Subject: sub4; Training Loss: 0.39778202772140503, Val Loss: 0.9665108919143677\n",
      "Step: 1980, Subject: sub5; Training Loss: 0.5275793075561523, Val Loss: 0.9730543494224548\n",
      "Step: 1980, Subject: sub7; Training Loss: 0.5112930536270142, Val Loss: 1.0197505950927734\n",
      "Step: 1990, Subject: sub1; Training Loss: 0.6557159423828125, Val Loss: 0.7129430174827576\n",
      "Step: 1990, Subject: sub2; Training Loss: 0.45842796564102173, Val Loss: 0.8780184984207153\n",
      "Step: 1990, Subject: sub3; Training Loss: 0.7035235166549683, Val Loss: 1.0468549728393555\n",
      "Step: 1990, Subject: sub4; Training Loss: 0.33300864696502686, Val Loss: 0.898128092288971\n",
      "Step: 1990, Subject: sub5; Training Loss: 0.51118004322052, Val Loss: 1.0575631856918335\n",
      "Step: 1990, Subject: sub7; Training Loss: 0.5614968538284302, Val Loss: 1.1924387216567993\n",
      "Step: 2000, Subject: sub1; Training Loss: 0.6306341886520386, Val Loss: 1.375761866569519\n",
      "Step: 2000, Subject: sub2; Training Loss: 0.4258691072463989, Val Loss: 0.8189249634742737\n",
      "Step: 2000, Subject: sub3; Training Loss: 0.6582379341125488, Val Loss: 0.9143936038017273\n",
      "Step: 2000, Subject: sub4; Training Loss: 0.420065701007843, Val Loss: 0.8736411333084106\n",
      "Step: 2000, Subject: sub5; Training Loss: 0.49877917766571045, Val Loss: 0.8600926399230957\n",
      "Step: 2000, Subject: sub7; Training Loss: 0.5750179290771484, Val Loss: 0.9411705136299133\n",
      "Step: 2010, Subject: sub1; Training Loss: 0.6849763989448547, Val Loss: 1.0131876468658447\n",
      "Step: 2010, Subject: sub2; Training Loss: 0.49343258142471313, Val Loss: 0.8863101005554199\n",
      "Step: 2010, Subject: sub3; Training Loss: 0.7991329431533813, Val Loss: 0.8861357569694519\n",
      "Step: 2010, Subject: sub4; Training Loss: 0.5543946027755737, Val Loss: 0.9947265982627869\n",
      "Step: 2010, Subject: sub5; Training Loss: 0.573270320892334, Val Loss: 0.9361807703971863\n",
      "Step: 2010, Subject: sub7; Training Loss: 0.547659158706665, Val Loss: 1.0246856212615967\n",
      "Step: 2020, Subject: sub1; Training Loss: 0.5148499608039856, Val Loss: 1.059061050415039\n",
      "Step: 2020, Subject: sub2; Training Loss: 0.39519602060317993, Val Loss: 0.9181530475616455\n",
      "Step: 2020, Subject: sub3; Training Loss: 0.6898810267448425, Val Loss: 0.8080140352249146\n",
      "Step: 2020, Subject: sub4; Training Loss: 0.3836129903793335, Val Loss: 0.8484613299369812\n",
      "Step: 2020, Subject: sub5; Training Loss: 0.5315814018249512, Val Loss: 0.7851636409759521\n",
      "Step: 2020, Subject: sub7; Training Loss: 0.49466824531555176, Val Loss: 1.143702507019043\n",
      "Step: 2030, Subject: sub1; Training Loss: 0.5870385766029358, Val Loss: 0.9371066689491272\n",
      "Step: 2030, Subject: sub2; Training Loss: 0.40922456979751587, Val Loss: 0.8889288902282715\n",
      "Step: 2030, Subject: sub3; Training Loss: 0.7153587937355042, Val Loss: 1.0588388442993164\n",
      "Step: 2030, Subject: sub4; Training Loss: 0.4299584627151489, Val Loss: 0.9389868378639221\n",
      "Step: 2030, Subject: sub5; Training Loss: 0.5279168486595154, Val Loss: 0.893742561340332\n",
      "Step: 2030, Subject: sub7; Training Loss: 0.5596078038215637, Val Loss: 1.1768722534179688\n",
      "Step: 2040, Subject: sub1; Training Loss: 0.5848881602287292, Val Loss: 1.0881226062774658\n",
      "Step: 2040, Subject: sub2; Training Loss: 0.41187959909439087, Val Loss: 0.8723722696304321\n",
      "Step: 2040, Subject: sub3; Training Loss: 0.6635506749153137, Val Loss: 0.903353214263916\n",
      "Step: 2040, Subject: sub4; Training Loss: 0.46940213441848755, Val Loss: 0.853091299533844\n",
      "Step: 2040, Subject: sub5; Training Loss: 0.5305125117301941, Val Loss: 1.1117842197418213\n",
      "Step: 2040, Subject: sub7; Training Loss: 0.5234842300415039, Val Loss: 1.0878803730010986\n",
      "Step: 2050, Subject: sub1; Training Loss: 0.6445936560630798, Val Loss: 1.2761203050613403\n",
      "Step: 2050, Subject: sub2; Training Loss: 0.449679970741272, Val Loss: 0.864913821220398\n",
      "Step: 2050, Subject: sub3; Training Loss: 0.6905649900436401, Val Loss: 1.1487045288085938\n",
      "Step: 2050, Subject: sub4; Training Loss: 0.4078575372695923, Val Loss: 0.9321308135986328\n",
      "Step: 2050, Subject: sub5; Training Loss: 0.5376421809196472, Val Loss: 1.1852493286132812\n",
      "Step: 2050, Subject: sub7; Training Loss: 0.5415263175964355, Val Loss: 1.3283582925796509\n",
      "Step: 2060, Subject: sub1; Training Loss: 0.5737622976303101, Val Loss: 1.0431010723114014\n",
      "Step: 2060, Subject: sub2; Training Loss: 0.4884234666824341, Val Loss: 0.7694188952445984\n",
      "Step: 2060, Subject: sub3; Training Loss: 0.6080765128135681, Val Loss: 0.8266173601150513\n",
      "Step: 2060, Subject: sub4; Training Loss: 0.3790152370929718, Val Loss: 0.9015412330627441\n",
      "Step: 2060, Subject: sub5; Training Loss: 0.5155864953994751, Val Loss: 0.7767309546470642\n",
      "Step: 2060, Subject: sub7; Training Loss: 0.5420492887496948, Val Loss: 1.0827062129974365\n",
      "Step: 2070, Subject: sub1; Training Loss: 0.5264265537261963, Val Loss: 1.3798848390579224\n",
      "Step: 2070, Subject: sub2; Training Loss: 0.5001420974731445, Val Loss: 0.8649047017097473\n",
      "Step: 2070, Subject: sub3; Training Loss: 0.7457305192947388, Val Loss: 0.9233075976371765\n",
      "Step: 2070, Subject: sub4; Training Loss: 0.4520232379436493, Val Loss: 0.8124949336051941\n",
      "Step: 2070, Subject: sub5; Training Loss: 0.477888286113739, Val Loss: 0.9447068572044373\n",
      "Step: 2070, Subject: sub7; Training Loss: 0.6022269129753113, Val Loss: 1.1691296100616455\n",
      "Step: 2080, Subject: sub1; Training Loss: 0.5162226557731628, Val Loss: 0.9998941421508789\n",
      "Step: 2080, Subject: sub2; Training Loss: 0.5236676931381226, Val Loss: 0.8746919631958008\n",
      "Step: 2080, Subject: sub3; Training Loss: 0.68609219789505, Val Loss: 0.973468005657196\n",
      "Step: 2080, Subject: sub4; Training Loss: 0.46073776483535767, Val Loss: 0.818263053894043\n",
      "Step: 2080, Subject: sub5; Training Loss: 0.44591575860977173, Val Loss: 0.8927525281906128\n",
      "Step: 2080, Subject: sub7; Training Loss: 0.4898499846458435, Val Loss: 1.0012850761413574\n",
      "Step: 2090, Subject: sub1; Training Loss: 0.5678039193153381, Val Loss: 0.7077968716621399\n",
      "Step: 2090, Subject: sub2; Training Loss: 0.4629899561405182, Val Loss: 0.8294793367385864\n",
      "Step: 2090, Subject: sub3; Training Loss: 0.5830898880958557, Val Loss: 0.7996785640716553\n",
      "Step: 2090, Subject: sub4; Training Loss: 0.38191258907318115, Val Loss: 0.8602648973464966\n",
      "Step: 2090, Subject: sub5; Training Loss: 0.43732449412345886, Val Loss: 0.8736370205879211\n",
      "Step: 2090, Subject: sub7; Training Loss: 0.5241445302963257, Val Loss: 0.7232487797737122\n",
      "Step: 2100, Subject: sub1; Training Loss: 0.5556204319000244, Val Loss: 0.92548668384552\n",
      "Step: 2100, Subject: sub2; Training Loss: 0.354913592338562, Val Loss: 0.9087203741073608\n",
      "Step: 2100, Subject: sub3; Training Loss: 0.678837776184082, Val Loss: 0.9646583199501038\n",
      "Step: 2100, Subject: sub4; Training Loss: 0.4281871020793915, Val Loss: 1.025747537612915\n",
      "Step: 2100, Subject: sub5; Training Loss: 0.47289538383483887, Val Loss: 0.835718035697937\n",
      "Step: 2100, Subject: sub7; Training Loss: 0.5319218635559082, Val Loss: 0.7931607961654663\n",
      "Step: 2110, Subject: sub1; Training Loss: 0.5851991176605225, Val Loss: 0.9658058881759644\n",
      "Step: 2110, Subject: sub2; Training Loss: 0.45073196291923523, Val Loss: 0.8896752595901489\n",
      "Step: 2110, Subject: sub3; Training Loss: 0.6884518265724182, Val Loss: 1.0876188278198242\n",
      "Step: 2110, Subject: sub4; Training Loss: 0.35641252994537354, Val Loss: 0.8385993242263794\n",
      "Step: 2110, Subject: sub5; Training Loss: 0.48580169677734375, Val Loss: 0.9356707334518433\n",
      "Step: 2110, Subject: sub7; Training Loss: 0.49929141998291016, Val Loss: 0.9475498199462891\n",
      "Step: 2120, Subject: sub1; Training Loss: 0.6743892431259155, Val Loss: 1.1662423610687256\n",
      "Step: 2120, Subject: sub2; Training Loss: 0.3971624970436096, Val Loss: 0.9476839900016785\n",
      "Step: 2120, Subject: sub3; Training Loss: 0.7138800024986267, Val Loss: 0.8880214095115662\n",
      "Step: 2120, Subject: sub4; Training Loss: 0.3565122187137604, Val Loss: 0.7539263963699341\n",
      "Step: 2120, Subject: sub5; Training Loss: 0.5152394771575928, Val Loss: 1.0294315814971924\n",
      "Step: 2120, Subject: sub7; Training Loss: 0.47083601355552673, Val Loss: 1.0521488189697266\n",
      "Step: 2130, Subject: sub1; Training Loss: 0.4933076500892639, Val Loss: 1.1218252182006836\n",
      "Step: 2130, Subject: sub2; Training Loss: 0.44646304845809937, Val Loss: 0.8114496469497681\n",
      "Step: 2130, Subject: sub3; Training Loss: 0.6243644952774048, Val Loss: 0.7998747825622559\n",
      "Step: 2130, Subject: sub4; Training Loss: 0.39176762104034424, Val Loss: 0.8792104721069336\n",
      "Step: 2130, Subject: sub5; Training Loss: 0.5522066950798035, Val Loss: 0.8705823421478271\n",
      "Step: 2130, Subject: sub7; Training Loss: 0.5553925037384033, Val Loss: 0.7112901210784912\n",
      "Step: 2140, Subject: sub1; Training Loss: 0.6068720817565918, Val Loss: 0.7931088209152222\n",
      "Step: 2140, Subject: sub2; Training Loss: 0.4817003011703491, Val Loss: 0.8131527900695801\n",
      "Step: 2140, Subject: sub3; Training Loss: 0.6169425845146179, Val Loss: 0.8278641104698181\n",
      "Step: 2140, Subject: sub4; Training Loss: 0.4693743884563446, Val Loss: 0.997005820274353\n",
      "Step: 2140, Subject: sub5; Training Loss: 0.5016984939575195, Val Loss: 0.8115817308425903\n",
      "Step: 2140, Subject: sub7; Training Loss: 0.5736840963363647, Val Loss: 1.0656914710998535\n",
      "Step: 2150, Subject: sub1; Training Loss: 0.48453468084335327, Val Loss: 1.122023582458496\n",
      "Step: 2150, Subject: sub2; Training Loss: 0.3560229241847992, Val Loss: 0.9315820932388306\n",
      "Step: 2150, Subject: sub3; Training Loss: 0.7658445239067078, Val Loss: 1.109513521194458\n",
      "Step: 2150, Subject: sub4; Training Loss: 0.4126225709915161, Val Loss: 0.8075088262557983\n",
      "Step: 2150, Subject: sub5; Training Loss: 0.5062838196754456, Val Loss: 0.9306752681732178\n",
      "Step: 2150, Subject: sub7; Training Loss: 0.5146059989929199, Val Loss: 1.2380383014678955\n",
      "Step: 2160, Subject: sub1; Training Loss: 0.5061048269271851, Val Loss: 1.067533254623413\n",
      "Step: 2160, Subject: sub2; Training Loss: 0.3742172122001648, Val Loss: 0.898442804813385\n",
      "Step: 2160, Subject: sub3; Training Loss: 0.7435373067855835, Val Loss: 0.9864082336425781\n",
      "Step: 2160, Subject: sub4; Training Loss: 0.481548935174942, Val Loss: 0.7356277704238892\n",
      "Step: 2160, Subject: sub5; Training Loss: 0.559325098991394, Val Loss: 0.6806143522262573\n",
      "Step: 2160, Subject: sub7; Training Loss: 0.4967050850391388, Val Loss: 1.1548123359680176\n",
      "Step: 2170, Subject: sub1; Training Loss: 0.5777665972709656, Val Loss: 0.978196382522583\n",
      "Step: 2170, Subject: sub2; Training Loss: 0.4466167092323303, Val Loss: 0.8679771423339844\n",
      "Step: 2170, Subject: sub3; Training Loss: 0.656579852104187, Val Loss: 1.0350098609924316\n",
      "Step: 2170, Subject: sub4; Training Loss: 0.3872445523738861, Val Loss: 0.8485692143440247\n",
      "Step: 2170, Subject: sub5; Training Loss: 0.551427960395813, Val Loss: 0.8843453526496887\n",
      "Step: 2170, Subject: sub7; Training Loss: 0.522653341293335, Val Loss: 0.8112044930458069\n",
      "Step: 2180, Subject: sub1; Training Loss: 0.48542240262031555, Val Loss: 1.1916160583496094\n",
      "Step: 2180, Subject: sub2; Training Loss: 0.4226731061935425, Val Loss: 0.8698868751525879\n",
      "Step: 2180, Subject: sub3; Training Loss: 0.6484492421150208, Val Loss: 0.8465388417243958\n",
      "Step: 2180, Subject: sub4; Training Loss: 0.48920345306396484, Val Loss: 0.8789384365081787\n",
      "Step: 2180, Subject: sub5; Training Loss: 0.45298874378204346, Val Loss: 0.8567819595336914\n",
      "Step: 2180, Subject: sub7; Training Loss: 0.4666561782360077, Val Loss: 1.074937343597412\n",
      "Step: 2190, Subject: sub1; Training Loss: 0.5715197920799255, Val Loss: 1.162008285522461\n",
      "Step: 2190, Subject: sub2; Training Loss: 0.4671032726764679, Val Loss: 0.842375636100769\n",
      "Step: 2190, Subject: sub3; Training Loss: 0.6985353231430054, Val Loss: 0.9052081108093262\n",
      "Step: 2190, Subject: sub4; Training Loss: 0.4053381681442261, Val Loss: 0.9958866834640503\n",
      "Step: 2190, Subject: sub5; Training Loss: 0.4425129294395447, Val Loss: 0.997949481010437\n",
      "Step: 2190, Subject: sub7; Training Loss: 0.49847638607025146, Val Loss: 1.17256498336792\n",
      "Step: 2200, Subject: sub1; Training Loss: 0.5759983658790588, Val Loss: 1.1384062767028809\n",
      "Step: 2200, Subject: sub2; Training Loss: 0.3686980903148651, Val Loss: 0.8472017049789429\n",
      "Step: 2200, Subject: sub3; Training Loss: 0.6800481081008911, Val Loss: 0.9117152690887451\n",
      "Step: 2200, Subject: sub4; Training Loss: 0.4356132745742798, Val Loss: 0.8599159121513367\n",
      "Step: 2200, Subject: sub5; Training Loss: 0.48275431990623474, Val Loss: 0.8310714960098267\n",
      "Step: 2200, Subject: sub7; Training Loss: 0.4602295756340027, Val Loss: 0.9923535585403442\n",
      "Step: 2210, Subject: sub1; Training Loss: 0.5297002196311951, Val Loss: 1.1416501998901367\n",
      "Step: 2210, Subject: sub2; Training Loss: 0.4221654534339905, Val Loss: 0.8351906538009644\n",
      "Step: 2210, Subject: sub3; Training Loss: 0.6833541393280029, Val Loss: 0.9181923866271973\n",
      "Step: 2210, Subject: sub4; Training Loss: 0.44348499178886414, Val Loss: 0.9138769507408142\n",
      "Step: 2210, Subject: sub5; Training Loss: 0.4877243638038635, Val Loss: 0.8751581311225891\n",
      "Step: 2210, Subject: sub7; Training Loss: 0.5389930009841919, Val Loss: 0.8297044038772583\n",
      "Step: 2220, Subject: sub1; Training Loss: 0.5555551648139954, Val Loss: 1.1833102703094482\n",
      "Step: 2220, Subject: sub2; Training Loss: 0.4275093972682953, Val Loss: 0.8166316747665405\n",
      "Step: 2220, Subject: sub3; Training Loss: 0.7679612636566162, Val Loss: 0.9552899599075317\n",
      "Step: 2220, Subject: sub4; Training Loss: 0.37254059314727783, Val Loss: 0.818413257598877\n",
      "Step: 2220, Subject: sub5; Training Loss: 0.43872106075286865, Val Loss: 0.8708517551422119\n",
      "Step: 2220, Subject: sub7; Training Loss: 0.5068457722663879, Val Loss: 0.8604958057403564\n",
      "Step: 2230, Subject: sub1; Training Loss: 0.4769134521484375, Val Loss: 1.1411010026931763\n",
      "Step: 2230, Subject: sub2; Training Loss: 0.35694801807403564, Val Loss: 0.9132171869277954\n",
      "Step: 2230, Subject: sub3; Training Loss: 0.6665788888931274, Val Loss: 0.8745916485786438\n",
      "Step: 2230, Subject: sub4; Training Loss: 0.3715530037879944, Val Loss: 0.9756064414978027\n",
      "Step: 2230, Subject: sub5; Training Loss: 0.5146195888519287, Val Loss: 1.082578420639038\n",
      "Step: 2230, Subject: sub7; Training Loss: 0.5527435541152954, Val Loss: 1.271675705909729\n",
      "Step: 2240, Subject: sub1; Training Loss: 0.5637171268463135, Val Loss: 1.2221295833587646\n",
      "Step: 2240, Subject: sub2; Training Loss: 0.4108644723892212, Val Loss: 0.8054661750793457\n",
      "Step: 2240, Subject: sub3; Training Loss: 0.6857147216796875, Val Loss: 0.8293414115905762\n",
      "Step: 2240, Subject: sub4; Training Loss: 0.44146451354026794, Val Loss: 0.8579088449478149\n",
      "Step: 2240, Subject: sub5; Training Loss: 0.3700258135795593, Val Loss: 0.8960614204406738\n",
      "Step: 2240, Subject: sub7; Training Loss: 0.480528324842453, Val Loss: 0.8710160255432129\n",
      "Step: 2250, Subject: sub1; Training Loss: 0.5598692893981934, Val Loss: 1.2802269458770752\n",
      "Step: 2250, Subject: sub2; Training Loss: 0.46076953411102295, Val Loss: 0.9397591352462769\n",
      "Step: 2250, Subject: sub3; Training Loss: 0.657260537147522, Val Loss: 0.9204866886138916\n",
      "Step: 2250, Subject: sub4; Training Loss: 0.5479139089584351, Val Loss: 0.9307323694229126\n",
      "Step: 2250, Subject: sub5; Training Loss: 0.5102757215499878, Val Loss: 0.9710630178451538\n",
      "Step: 2250, Subject: sub7; Training Loss: 0.4966185986995697, Val Loss: 1.0180532932281494\n",
      "Step: 2260, Subject: sub1; Training Loss: 0.4995213449001312, Val Loss: 1.244042158126831\n",
      "Step: 2260, Subject: sub2; Training Loss: 0.38807201385498047, Val Loss: 0.862430214881897\n",
      "Step: 2260, Subject: sub3; Training Loss: 0.6447539329528809, Val Loss: 0.8738224506378174\n",
      "Step: 2260, Subject: sub4; Training Loss: 0.3396831750869751, Val Loss: 0.8714287281036377\n",
      "Step: 2260, Subject: sub5; Training Loss: 0.49591386318206787, Val Loss: 0.812301754951477\n",
      "Step: 2260, Subject: sub7; Training Loss: 0.5106526613235474, Val Loss: 1.1451551914215088\n",
      "Step: 2270, Subject: sub1; Training Loss: 0.4424697160720825, Val Loss: 0.8609241247177124\n",
      "Step: 2270, Subject: sub2; Training Loss: 0.3875308632850647, Val Loss: 0.7909765243530273\n",
      "Step: 2270, Subject: sub3; Training Loss: 0.6433222889900208, Val Loss: 0.8901161551475525\n",
      "Step: 2270, Subject: sub4; Training Loss: 0.34840652346611023, Val Loss: 0.852453887462616\n",
      "Step: 2270, Subject: sub5; Training Loss: 0.5204318761825562, Val Loss: 1.0050387382507324\n",
      "Step: 2270, Subject: sub7; Training Loss: 0.5309724807739258, Val Loss: 0.9157232046127319\n",
      "Step: 2280, Subject: sub1; Training Loss: 0.5601945519447327, Val Loss: 1.1653707027435303\n",
      "Step: 2280, Subject: sub2; Training Loss: 0.39608293771743774, Val Loss: 0.7732397317886353\n",
      "Step: 2280, Subject: sub3; Training Loss: 0.7174688577651978, Val Loss: 0.7871257662773132\n",
      "Step: 2280, Subject: sub4; Training Loss: 0.45588579773902893, Val Loss: 0.8416844606399536\n",
      "Step: 2280, Subject: sub5; Training Loss: 0.5577709674835205, Val Loss: 0.8568525910377502\n",
      "Step: 2280, Subject: sub7; Training Loss: 0.5578296184539795, Val Loss: 1.0369466543197632\n",
      "Step: 2290, Subject: sub1; Training Loss: 0.5458400249481201, Val Loss: 1.3555915355682373\n",
      "Step: 2290, Subject: sub2; Training Loss: 0.4388611912727356, Val Loss: 0.8226662874221802\n",
      "Step: 2290, Subject: sub3; Training Loss: 0.73514723777771, Val Loss: 1.249516248703003\n",
      "Step: 2290, Subject: sub4; Training Loss: 0.46397438645362854, Val Loss: 0.8875856995582581\n",
      "Step: 2290, Subject: sub5; Training Loss: 0.5077077746391296, Val Loss: 1.0645617246627808\n",
      "Step: 2290, Subject: sub7; Training Loss: 0.4881878197193146, Val Loss: 1.2613673210144043\n",
      "Step: 2300, Subject: sub1; Training Loss: 0.5407215356826782, Val Loss: 1.226096510887146\n",
      "Step: 2300, Subject: sub2; Training Loss: 0.36290085315704346, Val Loss: 0.8843851089477539\n",
      "Step: 2300, Subject: sub3; Training Loss: 0.6167742609977722, Val Loss: 1.0188310146331787\n",
      "Step: 2300, Subject: sub4; Training Loss: 0.35654371976852417, Val Loss: 0.7409231662750244\n",
      "Step: 2300, Subject: sub5; Training Loss: 0.5300449132919312, Val Loss: 0.9441952109336853\n",
      "Step: 2300, Subject: sub7; Training Loss: 0.5530708432197571, Val Loss: 1.0018162727355957\n",
      "Step: 2310, Subject: sub1; Training Loss: 0.43349456787109375, Val Loss: 0.9528123140335083\n",
      "Step: 2310, Subject: sub2; Training Loss: 0.36751478910446167, Val Loss: 0.8248751759529114\n",
      "Step: 2310, Subject: sub3; Training Loss: 0.6958861947059631, Val Loss: 0.9880393743515015\n",
      "Step: 2310, Subject: sub4; Training Loss: 0.2824491560459137, Val Loss: 0.8166041374206543\n",
      "Step: 2310, Subject: sub5; Training Loss: 0.4480835199356079, Val Loss: 0.9175662994384766\n",
      "Step: 2310, Subject: sub7; Training Loss: 0.5128304958343506, Val Loss: 1.0811080932617188\n",
      "Step: 2320, Subject: sub1; Training Loss: 0.4361497461795807, Val Loss: 1.2814233303070068\n",
      "Step: 2320, Subject: sub2; Training Loss: 0.3394795060157776, Val Loss: 0.9008692502975464\n",
      "Step: 2320, Subject: sub3; Training Loss: 0.7201330661773682, Val Loss: 0.8548343777656555\n",
      "Step: 2320, Subject: sub4; Training Loss: 0.5010421276092529, Val Loss: 0.8641525506973267\n",
      "Step: 2320, Subject: sub5; Training Loss: 0.5460423231124878, Val Loss: 0.802261471748352\n",
      "Step: 2320, Subject: sub7; Training Loss: 0.6124763488769531, Val Loss: 0.9859015345573425\n",
      "Step: 2330, Subject: sub1; Training Loss: 0.5706141591072083, Val Loss: 1.5048539638519287\n",
      "Step: 2330, Subject: sub2; Training Loss: 0.3638555407524109, Val Loss: 0.8073422312736511\n",
      "Step: 2330, Subject: sub3; Training Loss: 0.6741934418678284, Val Loss: 0.8834462761878967\n",
      "Step: 2330, Subject: sub4; Training Loss: 0.2252482920885086, Val Loss: 0.8921484351158142\n",
      "Step: 2330, Subject: sub5; Training Loss: 0.4679793119430542, Val Loss: 0.8741248250007629\n",
      "Step: 2330, Subject: sub7; Training Loss: 0.5485209226608276, Val Loss: 1.1319184303283691\n",
      "Step: 2340, Subject: sub1; Training Loss: 0.5462828278541565, Val Loss: 1.1423547267913818\n",
      "Step: 2340, Subject: sub2; Training Loss: 0.4462265372276306, Val Loss: 0.8270705938339233\n",
      "Step: 2340, Subject: sub3; Training Loss: 0.6658719778060913, Val Loss: 1.3084487915039062\n",
      "Step: 2340, Subject: sub4; Training Loss: 0.31970933079719543, Val Loss: 0.9692394733428955\n",
      "Step: 2340, Subject: sub5; Training Loss: 0.4626295566558838, Val Loss: 1.1525801420211792\n",
      "Step: 2340, Subject: sub7; Training Loss: 0.5085358619689941, Val Loss: 1.109305500984192\n",
      "Step: 2350, Subject: sub1; Training Loss: 0.42357346415519714, Val Loss: 1.6859941482543945\n",
      "Step: 2350, Subject: sub2; Training Loss: 0.37806931138038635, Val Loss: 0.8505792617797852\n",
      "Step: 2350, Subject: sub3; Training Loss: 0.6244034171104431, Val Loss: 0.8577960729598999\n",
      "Step: 2350, Subject: sub4; Training Loss: 0.3694993257522583, Val Loss: 0.7993754744529724\n",
      "Step: 2350, Subject: sub5; Training Loss: 0.48808419704437256, Val Loss: 1.0387686491012573\n",
      "Step: 2350, Subject: sub7; Training Loss: 0.4938998818397522, Val Loss: 1.0935701131820679\n",
      "Step: 2360, Subject: sub1; Training Loss: 0.44868266582489014, Val Loss: 0.8957117199897766\n",
      "Step: 2360, Subject: sub2; Training Loss: 0.285460889339447, Val Loss: 0.8596084117889404\n",
      "Step: 2360, Subject: sub3; Training Loss: 0.7041490077972412, Val Loss: 0.8943638801574707\n",
      "Step: 2360, Subject: sub4; Training Loss: 0.4803297817707062, Val Loss: 1.0043580532073975\n",
      "Step: 2360, Subject: sub5; Training Loss: 0.36357206106185913, Val Loss: 1.0310626029968262\n",
      "Step: 2360, Subject: sub7; Training Loss: 0.5309715270996094, Val Loss: 1.2643681764602661\n",
      "Step: 2370, Subject: sub1; Training Loss: 0.4681764543056488, Val Loss: 0.7459405064582825\n",
      "Step: 2370, Subject: sub2; Training Loss: 0.42168617248535156, Val Loss: 0.9605486989021301\n",
      "Step: 2370, Subject: sub3; Training Loss: 0.6515353918075562, Val Loss: 0.9254528284072876\n",
      "Step: 2370, Subject: sub4; Training Loss: 0.3473224639892578, Val Loss: 0.9197723865509033\n",
      "Step: 2370, Subject: sub5; Training Loss: 0.49613192677497864, Val Loss: 0.8001921772956848\n",
      "Step: 2370, Subject: sub7; Training Loss: 0.5105111598968506, Val Loss: 0.8366297483444214\n",
      "Step: 2380, Subject: sub1; Training Loss: 0.44419458508491516, Val Loss: 1.086305856704712\n",
      "Step: 2380, Subject: sub2; Training Loss: 0.3973657786846161, Val Loss: 0.8127975463867188\n",
      "Step: 2380, Subject: sub3; Training Loss: 0.6768065094947815, Val Loss: 0.8762523531913757\n",
      "Step: 2380, Subject: sub4; Training Loss: 0.31551963090896606, Val Loss: 0.8869407176971436\n",
      "Step: 2380, Subject: sub5; Training Loss: 0.4507041871547699, Val Loss: 1.0592820644378662\n",
      "Step: 2380, Subject: sub7; Training Loss: 0.5059942603111267, Val Loss: 1.2190383672714233\n",
      "Step: 2390, Subject: sub1; Training Loss: 0.5364478826522827, Val Loss: 1.7265026569366455\n",
      "Step: 2390, Subject: sub2; Training Loss: 0.329629123210907, Val Loss: 0.7894180417060852\n",
      "Step: 2390, Subject: sub3; Training Loss: 0.6709697842597961, Val Loss: 1.3580647706985474\n",
      "Step: 2390, Subject: sub4; Training Loss: 0.48081541061401367, Val Loss: 1.12089204788208\n",
      "Step: 2390, Subject: sub5; Training Loss: 0.5540181398391724, Val Loss: 1.1228320598602295\n",
      "Step: 2390, Subject: sub7; Training Loss: 0.4101507067680359, Val Loss: 1.1964170932769775\n",
      "Step: 2400, Subject: sub1; Training Loss: 0.4860408902168274, Val Loss: 1.2446973323822021\n",
      "Step: 2400, Subject: sub2; Training Loss: 0.33908993005752563, Val Loss: 0.880402684211731\n",
      "Step: 2400, Subject: sub3; Training Loss: 0.6078252792358398, Val Loss: 0.8400325775146484\n",
      "Step: 2400, Subject: sub4; Training Loss: 0.34049075841903687, Val Loss: 0.8751324415206909\n",
      "Step: 2400, Subject: sub5; Training Loss: 0.48353785276412964, Val Loss: 0.8185816407203674\n",
      "Step: 2400, Subject: sub7; Training Loss: 0.3941437602043152, Val Loss: 1.1697676181793213\n",
      "Step: 2410, Subject: sub1; Training Loss: 0.3944009244441986, Val Loss: 1.1941684484481812\n",
      "Step: 2410, Subject: sub2; Training Loss: 0.3911321759223938, Val Loss: 0.8418625593185425\n",
      "Step: 2410, Subject: sub3; Training Loss: 0.6476017236709595, Val Loss: 0.8210562467575073\n",
      "Step: 2410, Subject: sub4; Training Loss: 0.3510361611843109, Val Loss: 0.9172278642654419\n",
      "Step: 2410, Subject: sub5; Training Loss: 0.4541768431663513, Val Loss: 1.244938850402832\n",
      "Step: 2410, Subject: sub7; Training Loss: 0.5566644668579102, Val Loss: 1.581251859664917\n",
      "Step: 2420, Subject: sub1; Training Loss: 0.4425573945045471, Val Loss: 1.2244127988815308\n",
      "Step: 2420, Subject: sub2; Training Loss: 0.29025524854660034, Val Loss: 0.8635380268096924\n",
      "Step: 2420, Subject: sub3; Training Loss: 0.6255196332931519, Val Loss: 1.2499237060546875\n",
      "Step: 2420, Subject: sub4; Training Loss: 0.3983049690723419, Val Loss: 1.0066924095153809\n",
      "Step: 2420, Subject: sub5; Training Loss: 0.45449236035346985, Val Loss: 1.0762383937835693\n",
      "Step: 2420, Subject: sub7; Training Loss: 0.5592687129974365, Val Loss: 1.1974818706512451\n",
      "Step: 2430, Subject: sub1; Training Loss: 0.4232392907142639, Val Loss: 1.162615180015564\n",
      "Step: 2430, Subject: sub2; Training Loss: 0.45232170820236206, Val Loss: 0.8925930857658386\n",
      "Step: 2430, Subject: sub3; Training Loss: 0.6567986011505127, Val Loss: 0.9030296802520752\n",
      "Step: 2430, Subject: sub4; Training Loss: 0.4023624062538147, Val Loss: 0.8860810995101929\n",
      "Step: 2430, Subject: sub5; Training Loss: 0.525160014629364, Val Loss: 1.1719377040863037\n",
      "Step: 2430, Subject: sub7; Training Loss: 0.4957672655582428, Val Loss: 1.60616135597229\n",
      "Step: 2440, Subject: sub1; Training Loss: 0.4402841329574585, Val Loss: 0.9904578328132629\n",
      "Step: 2440, Subject: sub2; Training Loss: 0.31539714336395264, Val Loss: 0.8051689267158508\n",
      "Step: 2440, Subject: sub3; Training Loss: 0.6243616342544556, Val Loss: 1.1187081336975098\n",
      "Step: 2440, Subject: sub4; Training Loss: 0.41459694504737854, Val Loss: 0.9461967945098877\n",
      "Step: 2440, Subject: sub5; Training Loss: 0.465175986289978, Val Loss: 1.088245153427124\n",
      "Step: 2440, Subject: sub7; Training Loss: 0.4367068111896515, Val Loss: 1.1997594833374023\n",
      "Step: 2450, Subject: sub1; Training Loss: 0.43329954147338867, Val Loss: 1.286087155342102\n",
      "Step: 2450, Subject: sub2; Training Loss: 0.312493234872818, Val Loss: 0.8042265176773071\n",
      "Step: 2450, Subject: sub3; Training Loss: 0.595676839351654, Val Loss: 0.717552661895752\n",
      "Step: 2450, Subject: sub4; Training Loss: 0.2861492335796356, Val Loss: 0.918175995349884\n",
      "Step: 2450, Subject: sub5; Training Loss: 0.3794384002685547, Val Loss: 0.8554232120513916\n",
      "Step: 2450, Subject: sub7; Training Loss: 0.42721429467201233, Val Loss: 0.8496373891830444\n",
      "Step: 2460, Subject: sub1; Training Loss: 0.48717376589775085, Val Loss: 1.0232375860214233\n",
      "Step: 2460, Subject: sub2; Training Loss: 0.33036530017852783, Val Loss: 0.8196108341217041\n",
      "Step: 2460, Subject: sub3; Training Loss: 0.610957145690918, Val Loss: 1.0435032844543457\n",
      "Step: 2460, Subject: sub4; Training Loss: 0.36971962451934814, Val Loss: 0.9180638790130615\n",
      "Step: 2460, Subject: sub5; Training Loss: 0.41345855593681335, Val Loss: 0.9529297947883606\n",
      "Step: 2460, Subject: sub7; Training Loss: 0.5276898145675659, Val Loss: 1.0740835666656494\n",
      "Step: 2470, Subject: sub1; Training Loss: 0.4703417122364044, Val Loss: 0.9629620313644409\n",
      "Step: 2470, Subject: sub2; Training Loss: 0.4698505103588104, Val Loss: 0.7609989643096924\n",
      "Step: 2470, Subject: sub3; Training Loss: 0.6099879741668701, Val Loss: 0.9376426935195923\n",
      "Step: 2470, Subject: sub4; Training Loss: 0.3722839951515198, Val Loss: 0.8372067213058472\n",
      "Step: 2470, Subject: sub5; Training Loss: 0.48426511883735657, Val Loss: 0.7961866855621338\n",
      "Step: 2470, Subject: sub7; Training Loss: 0.44063156843185425, Val Loss: 1.0043234825134277\n",
      "Step: 2480, Subject: sub1; Training Loss: 0.34320035576820374, Val Loss: 0.9532220959663391\n",
      "Step: 2480, Subject: sub2; Training Loss: 0.3273296356201172, Val Loss: 0.77653568983078\n",
      "Step: 2480, Subject: sub3; Training Loss: 0.61058509349823, Val Loss: 0.7703350782394409\n",
      "Step: 2480, Subject: sub4; Training Loss: 0.3774109482765198, Val Loss: 0.7782407999038696\n",
      "Step: 2480, Subject: sub5; Training Loss: 0.45298686623573303, Val Loss: 0.7684663534164429\n",
      "Step: 2480, Subject: sub7; Training Loss: 0.5344460010528564, Val Loss: 0.9270811080932617\n",
      "Step: 2490, Subject: sub1; Training Loss: 0.45406901836395264, Val Loss: 1.5980796813964844\n",
      "Step: 2490, Subject: sub2; Training Loss: 0.2788527011871338, Val Loss: 0.8951989412307739\n",
      "Step: 2490, Subject: sub3; Training Loss: 0.6460778713226318, Val Loss: 1.2647314071655273\n",
      "Step: 2490, Subject: sub4; Training Loss: 0.4634922742843628, Val Loss: 0.8301637172698975\n",
      "Step: 2490, Subject: sub5; Training Loss: 0.43563181161880493, Val Loss: 1.017808437347412\n",
      "Step: 2490, Subject: sub7; Training Loss: 0.6389709711074829, Val Loss: 1.0574798583984375\n",
      "Step: 2500, Subject: sub1; Training Loss: 0.5774117112159729, Val Loss: 1.2997158765792847\n",
      "Step: 2500, Subject: sub2; Training Loss: 0.3833286762237549, Val Loss: 0.8297152519226074\n",
      "Step: 2500, Subject: sub3; Training Loss: 0.6158369779586792, Val Loss: 0.955294132232666\n",
      "Step: 2500, Subject: sub4; Training Loss: 0.3730195164680481, Val Loss: 0.8466944694519043\n",
      "Step: 2500, Subject: sub5; Training Loss: 0.4396328032016754, Val Loss: 0.8594090342521667\n",
      "Step: 2500, Subject: sub7; Training Loss: 0.43170225620269775, Val Loss: 0.8841265439987183\n",
      "Step: 2510, Subject: sub1; Training Loss: 0.4602546691894531, Val Loss: 1.352198600769043\n",
      "Step: 2510, Subject: sub2; Training Loss: 0.2874346375465393, Val Loss: 0.8907641172409058\n",
      "Step: 2510, Subject: sub3; Training Loss: 0.6361318230628967, Val Loss: 0.9684123992919922\n",
      "Step: 2510, Subject: sub4; Training Loss: 0.35851675271987915, Val Loss: 1.016305923461914\n",
      "Step: 2510, Subject: sub5; Training Loss: 0.4588044583797455, Val Loss: 1.0380771160125732\n",
      "Step: 2510, Subject: sub7; Training Loss: 0.4428361654281616, Val Loss: 1.3180625438690186\n",
      "Step: 2520, Subject: sub1; Training Loss: 0.38204699754714966, Val Loss: 1.2623502016067505\n",
      "Step: 2520, Subject: sub2; Training Loss: 0.3399302363395691, Val Loss: 0.884437084197998\n",
      "Step: 2520, Subject: sub3; Training Loss: 0.6070627570152283, Val Loss: 0.9150189757347107\n",
      "Step: 2520, Subject: sub4; Training Loss: 0.42757564783096313, Val Loss: 1.0321784019470215\n",
      "Step: 2520, Subject: sub5; Training Loss: 0.4841189384460449, Val Loss: 0.9245717525482178\n",
      "Step: 2520, Subject: sub7; Training Loss: 0.47093653678894043, Val Loss: 0.9398617148399353\n",
      "Step: 2530, Subject: sub1; Training Loss: 0.38178080320358276, Val Loss: 1.3984993696212769\n",
      "Step: 2530, Subject: sub2; Training Loss: 0.3801015615463257, Val Loss: 0.865169107913971\n",
      "Step: 2530, Subject: sub3; Training Loss: 0.6677785515785217, Val Loss: 1.2336121797561646\n",
      "Step: 2530, Subject: sub4; Training Loss: 0.32381170988082886, Val Loss: 0.8656126260757446\n",
      "Step: 2530, Subject: sub5; Training Loss: 0.4629274606704712, Val Loss: 1.2610224485397339\n",
      "Step: 2530, Subject: sub7; Training Loss: 0.4860984981060028, Val Loss: 1.4564146995544434\n",
      "Step: 2540, Subject: sub1; Training Loss: 0.4194262623786926, Val Loss: 0.6357803344726562\n",
      "Saving model with validation loss: 0.6357803344726562\n",
      "\n",
      "Step: 2540, Subject: sub2; Training Loss: 0.3611137568950653, Val Loss: 0.836111307144165\n",
      "Step: 2540, Subject: sub3; Training Loss: 0.676031768321991, Val Loss: 1.0563998222351074\n",
      "Step: 2540, Subject: sub4; Training Loss: 0.3241342306137085, Val Loss: 0.8870059251785278\n",
      "Step: 2540, Subject: sub5; Training Loss: 0.47129225730895996, Val Loss: 0.9604251384735107\n",
      "Step: 2540, Subject: sub7; Training Loss: 0.4820106327533722, Val Loss: 0.8453147411346436\n",
      "Step: 2550, Subject: sub1; Training Loss: 0.44622358679771423, Val Loss: 1.0527331829071045\n",
      "Step: 2550, Subject: sub2; Training Loss: 0.28424984216690063, Val Loss: 0.836462140083313\n",
      "Step: 2550, Subject: sub3; Training Loss: 0.5681126117706299, Val Loss: 0.9180899858474731\n",
      "Step: 2550, Subject: sub4; Training Loss: 0.30851563811302185, Val Loss: 0.6652332544326782\n",
      "Step: 2550, Subject: sub5; Training Loss: 0.36795735359191895, Val Loss: 1.0540063381195068\n",
      "Step: 2550, Subject: sub7; Training Loss: 0.4490395188331604, Val Loss: 1.0171936750411987\n",
      "Step: 2560, Subject: sub1; Training Loss: 0.5396449565887451, Val Loss: 1.022491216659546\n",
      "Step: 2560, Subject: sub2; Training Loss: 0.22711199522018433, Val Loss: 0.7523783445358276\n",
      "Step: 2560, Subject: sub3; Training Loss: 0.5840439796447754, Val Loss: 0.8025267124176025\n",
      "Step: 2560, Subject: sub4; Training Loss: 0.37220650911331177, Val Loss: 0.8771373629570007\n",
      "Step: 2560, Subject: sub5; Training Loss: 0.4953821897506714, Val Loss: 0.7020158767700195\n",
      "Step: 2560, Subject: sub7; Training Loss: 0.5635725855827332, Val Loss: 1.03639817237854\n",
      "Step: 2570, Subject: sub1; Training Loss: 0.5325730443000793, Val Loss: 1.3177069425582886\n",
      "Step: 2570, Subject: sub2; Training Loss: 0.2929195761680603, Val Loss: 0.8683702349662781\n",
      "Step: 2570, Subject: sub3; Training Loss: 0.7193491458892822, Val Loss: 0.9462002515792847\n",
      "Step: 2570, Subject: sub4; Training Loss: 0.39740216732025146, Val Loss: 0.9571070671081543\n",
      "Step: 2570, Subject: sub5; Training Loss: 0.43477416038513184, Val Loss: 0.950774073600769\n",
      "Step: 2570, Subject: sub7; Training Loss: 0.4816259443759918, Val Loss: 1.256493091583252\n",
      "Step: 2580, Subject: sub1; Training Loss: 0.3617819547653198, Val Loss: 0.8169578313827515\n",
      "Step: 2580, Subject: sub2; Training Loss: 0.33007127046585083, Val Loss: 0.8507254123687744\n",
      "Step: 2580, Subject: sub3; Training Loss: 0.5592236518859863, Val Loss: 0.8400635123252869\n",
      "Step: 2580, Subject: sub4; Training Loss: 0.4109424948692322, Val Loss: 1.0244187116622925\n",
      "Step: 2580, Subject: sub5; Training Loss: 0.4287858009338379, Val Loss: 0.8393522500991821\n",
      "Step: 2580, Subject: sub7; Training Loss: 0.4861968457698822, Val Loss: 0.9959070682525635\n",
      "Step: 2590, Subject: sub1; Training Loss: 0.38842880725860596, Val Loss: 0.782120406627655\n",
      "Step: 2590, Subject: sub2; Training Loss: 0.28469225764274597, Val Loss: 0.7715491056442261\n",
      "Step: 2590, Subject: sub3; Training Loss: 0.7015691995620728, Val Loss: 0.7927172780036926\n",
      "Step: 2590, Subject: sub4; Training Loss: 0.3869193196296692, Val Loss: 0.9016717672348022\n",
      "Step: 2590, Subject: sub5; Training Loss: 0.47100430727005005, Val Loss: 0.7109951972961426\n",
      "Step: 2590, Subject: sub7; Training Loss: 0.5246599912643433, Val Loss: 0.9936648607254028\n",
      "Step: 2600, Subject: sub1; Training Loss: 0.5064988136291504, Val Loss: 1.1204030513763428\n",
      "Step: 2600, Subject: sub2; Training Loss: 0.25302624702453613, Val Loss: 0.8795660734176636\n",
      "Step: 2600, Subject: sub3; Training Loss: 0.5829536318778992, Val Loss: 1.32917320728302\n",
      "Step: 2600, Subject: sub4; Training Loss: 0.2586444616317749, Val Loss: 1.09834623336792\n",
      "Step: 2600, Subject: sub5; Training Loss: 0.488264262676239, Val Loss: 1.1059068441390991\n",
      "Step: 2600, Subject: sub7; Training Loss: 0.4812980890274048, Val Loss: 1.1498990058898926\n",
      "Step: 2610, Subject: sub1; Training Loss: 0.4902336001396179, Val Loss: 0.750328004360199\n",
      "Step: 2610, Subject: sub2; Training Loss: 0.26694703102111816, Val Loss: 0.8598521947860718\n",
      "Step: 2610, Subject: sub3; Training Loss: 0.6117273569107056, Val Loss: 0.9796729683876038\n",
      "Step: 2610, Subject: sub4; Training Loss: 0.30811581015586853, Val Loss: 0.8746447563171387\n",
      "Step: 2610, Subject: sub5; Training Loss: 0.5367700457572937, Val Loss: 1.0833661556243896\n",
      "Step: 2610, Subject: sub7; Training Loss: 0.48330801725387573, Val Loss: 1.037062644958496\n",
      "Step: 2620, Subject: sub1; Training Loss: 0.5405182242393494, Val Loss: 0.767585039138794\n",
      "Step: 2620, Subject: sub2; Training Loss: 0.2500898241996765, Val Loss: 0.8276439905166626\n",
      "Step: 2620, Subject: sub3; Training Loss: 0.6012691259384155, Val Loss: 1.0596580505371094\n",
      "Step: 2620, Subject: sub4; Training Loss: 0.32294806838035583, Val Loss: 0.7626639008522034\n",
      "Step: 2620, Subject: sub5; Training Loss: 0.4182971119880676, Val Loss: 0.8711622953414917\n",
      "Step: 2620, Subject: sub7; Training Loss: 0.43903177976608276, Val Loss: 0.9922292828559875\n",
      "Step: 2630, Subject: sub1; Training Loss: 0.3644557595252991, Val Loss: 1.1136366128921509\n",
      "Step: 2630, Subject: sub2; Training Loss: 0.2836090326309204, Val Loss: 0.7705719470977783\n",
      "Step: 2630, Subject: sub3; Training Loss: 0.6570844054222107, Val Loss: 0.8542345762252808\n",
      "Step: 2630, Subject: sub4; Training Loss: 0.2926942706108093, Val Loss: 0.9650858640670776\n",
      "Step: 2630, Subject: sub5; Training Loss: 0.39921021461486816, Val Loss: 0.8778623342514038\n",
      "Step: 2630, Subject: sub7; Training Loss: 0.42102277278900146, Val Loss: 1.288559913635254\n",
      "Step: 2640, Subject: sub1; Training Loss: 0.40822136402130127, Val Loss: 0.9397311210632324\n",
      "Step: 2640, Subject: sub2; Training Loss: 0.2597741484642029, Val Loss: 0.8362863063812256\n",
      "Step: 2640, Subject: sub3; Training Loss: 0.5972113609313965, Val Loss: 0.9702328443527222\n",
      "Step: 2640, Subject: sub4; Training Loss: 0.3154129087924957, Val Loss: 0.8526168465614319\n",
      "Step: 2640, Subject: sub5; Training Loss: 0.4865702688694, Val Loss: 0.6426755785942078\n",
      "Step: 2640, Subject: sub7; Training Loss: 0.5058087706565857, Val Loss: 1.0442672967910767\n",
      "Step: 2650, Subject: sub1; Training Loss: 0.41139310598373413, Val Loss: 1.109485149383545\n",
      "Step: 2650, Subject: sub2; Training Loss: 0.27240267395973206, Val Loss: 0.8006067276000977\n",
      "Step: 2650, Subject: sub3; Training Loss: 0.6135348081588745, Val Loss: 0.9012841582298279\n",
      "Step: 2650, Subject: sub4; Training Loss: 0.4064667820930481, Val Loss: 0.7852919101715088\n",
      "Step: 2650, Subject: sub5; Training Loss: 0.4539652466773987, Val Loss: 0.7840518355369568\n",
      "Step: 2650, Subject: sub7; Training Loss: 0.4887375235557556, Val Loss: 1.1436022520065308\n",
      "Step: 2660, Subject: sub1; Training Loss: 0.3432047963142395, Val Loss: 1.2263245582580566\n",
      "Step: 2660, Subject: sub2; Training Loss: 0.24331025779247284, Val Loss: 0.8838856816291809\n",
      "Step: 2660, Subject: sub3; Training Loss: 0.6673217415809631, Val Loss: 0.9981587529182434\n",
      "Step: 2660, Subject: sub4; Training Loss: 0.3526611924171448, Val Loss: 0.9269735217094421\n",
      "Step: 2660, Subject: sub5; Training Loss: 0.3467820882797241, Val Loss: 1.0380113124847412\n",
      "Step: 2660, Subject: sub7; Training Loss: 0.48780304193496704, Val Loss: 1.2730860710144043\n",
      "Step: 2670, Subject: sub1; Training Loss: 0.29947784543037415, Val Loss: 0.8141636848449707\n",
      "Step: 2670, Subject: sub2; Training Loss: 0.367917001247406, Val Loss: 0.792075514793396\n",
      "Step: 2670, Subject: sub3; Training Loss: 0.6115233302116394, Val Loss: 0.8621174693107605\n",
      "Step: 2670, Subject: sub4; Training Loss: 0.33471792936325073, Val Loss: 1.025863528251648\n",
      "Step: 2670, Subject: sub5; Training Loss: 0.44593456387519836, Val Loss: 0.8320585489273071\n",
      "Step: 2670, Subject: sub7; Training Loss: 0.4583838880062103, Val Loss: 0.8906364440917969\n",
      "Step: 2680, Subject: sub1; Training Loss: 0.43743371963500977, Val Loss: 1.0406395196914673\n",
      "Step: 2680, Subject: sub2; Training Loss: 0.22928407788276672, Val Loss: 0.8976575136184692\n",
      "Step: 2680, Subject: sub3; Training Loss: 0.5910552144050598, Val Loss: 0.8330065608024597\n",
      "Step: 2680, Subject: sub4; Training Loss: 0.31390613317489624, Val Loss: 0.9397348165512085\n",
      "Step: 2680, Subject: sub5; Training Loss: 0.38982319831848145, Val Loss: 1.0483894348144531\n",
      "Step: 2680, Subject: sub7; Training Loss: 0.41998663544654846, Val Loss: 1.3040399551391602\n",
      "Step: 2690, Subject: sub1; Training Loss: 0.35084980726242065, Val Loss: 1.5816569328308105\n",
      "Step: 2690, Subject: sub2; Training Loss: 0.25143569707870483, Val Loss: 0.9840830564498901\n",
      "Step: 2690, Subject: sub3; Training Loss: 0.513041615486145, Val Loss: 0.886438250541687\n",
      "Step: 2690, Subject: sub4; Training Loss: 0.32865291833877563, Val Loss: 0.900748610496521\n",
      "Step: 2690, Subject: sub5; Training Loss: 0.4415218234062195, Val Loss: 1.1549206972122192\n",
      "Step: 2690, Subject: sub7; Training Loss: 0.5283420085906982, Val Loss: 1.375285267829895\n",
      "Step: 2700, Subject: sub1; Training Loss: 0.3119453489780426, Val Loss: 1.4925949573516846\n",
      "Step: 2700, Subject: sub2; Training Loss: 0.24433323740959167, Val Loss: 0.8543077707290649\n",
      "Step: 2700, Subject: sub3; Training Loss: 0.5744448304176331, Val Loss: 0.8211380839347839\n",
      "Step: 2700, Subject: sub4; Training Loss: 0.2801353931427002, Val Loss: 0.9561309814453125\n",
      "Step: 2700, Subject: sub5; Training Loss: 0.43261489272117615, Val Loss: 0.7141900062561035\n",
      "Step: 2700, Subject: sub7; Training Loss: 0.4579058289527893, Val Loss: 0.8807723522186279\n",
      "Step: 2710, Subject: sub1; Training Loss: 0.31628766655921936, Val Loss: 0.7699404954910278\n",
      "Step: 2710, Subject: sub2; Training Loss: 0.25082504749298096, Val Loss: 0.9752410650253296\n",
      "Step: 2710, Subject: sub3; Training Loss: 0.6750889420509338, Val Loss: 0.9768573641777039\n",
      "Step: 2710, Subject: sub4; Training Loss: 0.3934347331523895, Val Loss: 0.836178183555603\n",
      "Step: 2710, Subject: sub5; Training Loss: 0.5055394172668457, Val Loss: 0.9334256052970886\n",
      "Step: 2710, Subject: sub7; Training Loss: 0.47048988938331604, Val Loss: 1.0193332433700562\n",
      "Step: 2720, Subject: sub1; Training Loss: 0.3083595037460327, Val Loss: 1.5892833471298218\n",
      "Step: 2720, Subject: sub2; Training Loss: 0.25815266370773315, Val Loss: 0.8341917395591736\n",
      "Step: 2720, Subject: sub3; Training Loss: 0.5865600109100342, Val Loss: 1.1503957509994507\n",
      "Step: 2720, Subject: sub4; Training Loss: 0.4018215835094452, Val Loss: 0.8315873742103577\n",
      "Step: 2720, Subject: sub5; Training Loss: 0.46180570125579834, Val Loss: 0.924889862537384\n",
      "Step: 2720, Subject: sub7; Training Loss: 0.5203783512115479, Val Loss: 1.48392915725708\n",
      "Step: 2730, Subject: sub1; Training Loss: 0.29497307538986206, Val Loss: 0.8675409555435181\n",
      "Step: 2730, Subject: sub2; Training Loss: 0.1975213885307312, Val Loss: 0.8514325618743896\n",
      "Step: 2730, Subject: sub3; Training Loss: 0.6292776465415955, Val Loss: 0.9595187306404114\n",
      "Step: 2730, Subject: sub4; Training Loss: 0.4013545513153076, Val Loss: 0.9671545624732971\n",
      "Step: 2730, Subject: sub5; Training Loss: 0.4282742440700531, Val Loss: 0.8792136311531067\n",
      "Step: 2730, Subject: sub7; Training Loss: 0.46176353096961975, Val Loss: 0.9322447180747986\n",
      "Step: 2740, Subject: sub1; Training Loss: 0.45534753799438477, Val Loss: 0.7848979234695435\n",
      "Step: 2740, Subject: sub2; Training Loss: 0.2976810336112976, Val Loss: 0.8444966673851013\n",
      "Step: 2740, Subject: sub3; Training Loss: 0.6417617797851562, Val Loss: 0.9736917018890381\n",
      "Step: 2740, Subject: sub4; Training Loss: 0.4105421006679535, Val Loss: 0.7952725291252136\n",
      "Step: 2740, Subject: sub5; Training Loss: 0.38956350088119507, Val Loss: 0.9052888751029968\n",
      "Step: 2740, Subject: sub7; Training Loss: 0.46730610728263855, Val Loss: 0.9290634989738464\n",
      "Step: 2750, Subject: sub1; Training Loss: 0.3193487524986267, Val Loss: 1.1052708625793457\n",
      "Step: 2750, Subject: sub2; Training Loss: 0.23360106348991394, Val Loss: 0.8131494522094727\n",
      "Step: 2750, Subject: sub3; Training Loss: 0.6271157264709473, Val Loss: 1.2584924697875977\n",
      "Step: 2750, Subject: sub4; Training Loss: 0.2358318418264389, Val Loss: 0.982539713382721\n",
      "Step: 2750, Subject: sub5; Training Loss: 0.4029538631439209, Val Loss: 1.0660291910171509\n",
      "Step: 2750, Subject: sub7; Training Loss: 0.42688649892807007, Val Loss: 1.1142542362213135\n",
      "Step: 2760, Subject: sub1; Training Loss: 0.3343639671802521, Val Loss: 1.1935217380523682\n",
      "Step: 2760, Subject: sub2; Training Loss: 0.25928378105163574, Val Loss: 0.926785945892334\n",
      "Step: 2760, Subject: sub3; Training Loss: 0.5927228927612305, Val Loss: 0.8996629118919373\n",
      "Step: 2760, Subject: sub4; Training Loss: 0.3154590427875519, Val Loss: 1.0050415992736816\n",
      "Step: 2760, Subject: sub5; Training Loss: 0.4319375157356262, Val Loss: 0.9847398996353149\n",
      "Step: 2760, Subject: sub7; Training Loss: 0.4257827401161194, Val Loss: 0.9794665575027466\n",
      "Step: 2770, Subject: sub1; Training Loss: 0.32276153564453125, Val Loss: 1.328819751739502\n",
      "Step: 2770, Subject: sub2; Training Loss: 0.2823992669582367, Val Loss: 0.9057360887527466\n",
      "Step: 2770, Subject: sub3; Training Loss: 0.6183896660804749, Val Loss: 0.9070301055908203\n",
      "Step: 2770, Subject: sub4; Training Loss: 0.2822533845901489, Val Loss: 0.8507456183433533\n",
      "Step: 2770, Subject: sub5; Training Loss: 0.4670943319797516, Val Loss: 1.0275815725326538\n",
      "Step: 2770, Subject: sub7; Training Loss: 0.4297521710395813, Val Loss: 1.2230069637298584\n",
      "Step: 2780, Subject: sub1; Training Loss: 0.39057376980781555, Val Loss: 0.8302737474441528\n",
      "Step: 2780, Subject: sub2; Training Loss: 0.2883725166320801, Val Loss: 0.7917239665985107\n",
      "Step: 2780, Subject: sub3; Training Loss: 0.6608362197875977, Val Loss: 0.9014744758605957\n",
      "Step: 2780, Subject: sub4; Training Loss: 0.2577725648880005, Val Loss: 0.9100497961044312\n",
      "Step: 2780, Subject: sub5; Training Loss: 0.4128865599632263, Val Loss: 0.8543874621391296\n",
      "Step: 2780, Subject: sub7; Training Loss: 0.5347991585731506, Val Loss: 1.110729455947876\n",
      "Step: 2790, Subject: sub1; Training Loss: 0.41409730911254883, Val Loss: 0.9079601168632507\n",
      "Step: 2790, Subject: sub2; Training Loss: 0.24041175842285156, Val Loss: 0.7709032297134399\n",
      "Step: 2790, Subject: sub3; Training Loss: 0.6001519560813904, Val Loss: 1.020289421081543\n",
      "Step: 2790, Subject: sub4; Training Loss: 0.3316454589366913, Val Loss: 0.8771399855613708\n",
      "Step: 2790, Subject: sub5; Training Loss: 0.4081895351409912, Val Loss: 1.0713330507278442\n",
      "Step: 2790, Subject: sub7; Training Loss: 0.4771649241447449, Val Loss: 1.33645498752594\n",
      "Step: 2800, Subject: sub1; Training Loss: 0.3664718568325043, Val Loss: 0.9403266906738281\n",
      "Step: 2800, Subject: sub2; Training Loss: 0.24981282651424408, Val Loss: 0.8165040016174316\n",
      "Step: 2800, Subject: sub3; Training Loss: 0.6899024248123169, Val Loss: 0.815460205078125\n",
      "Step: 2800, Subject: sub4; Training Loss: 0.2967795729637146, Val Loss: 0.8380931615829468\n",
      "Step: 2800, Subject: sub5; Training Loss: 0.41032618284225464, Val Loss: 0.741003692150116\n",
      "Step: 2800, Subject: sub7; Training Loss: 0.4411456882953644, Val Loss: 1.2234270572662354\n",
      "Step: 2810, Subject: sub1; Training Loss: 0.3653414249420166, Val Loss: 1.2477387189865112\n",
      "Step: 2810, Subject: sub2; Training Loss: 0.27050691843032837, Val Loss: 0.8622166514396667\n",
      "Step: 2810, Subject: sub3; Training Loss: 0.5709028244018555, Val Loss: 0.833659827709198\n",
      "Step: 2810, Subject: sub4; Training Loss: 0.28094953298568726, Val Loss: 0.8779013156890869\n",
      "Step: 2810, Subject: sub5; Training Loss: 0.4708324670791626, Val Loss: 1.044512391090393\n",
      "Step: 2810, Subject: sub7; Training Loss: 0.3583807945251465, Val Loss: 1.0961270332336426\n",
      "Step: 2820, Subject: sub1; Training Loss: 0.27875036001205444, Val Loss: 0.8617503643035889\n",
      "Step: 2820, Subject: sub2; Training Loss: 0.23642878234386444, Val Loss: 0.8223958015441895\n",
      "Step: 2820, Subject: sub3; Training Loss: 0.5736532211303711, Val Loss: 1.0402040481567383\n",
      "Step: 2820, Subject: sub4; Training Loss: 0.32462266087532043, Val Loss: 0.8207093477249146\n",
      "Step: 2820, Subject: sub5; Training Loss: 0.43463945388793945, Val Loss: 0.855878472328186\n",
      "Step: 2820, Subject: sub7; Training Loss: 0.469970166683197, Val Loss: 0.8774239420890808\n",
      "Step: 2830, Subject: sub1; Training Loss: 0.31134670972824097, Val Loss: 0.8314138650894165\n",
      "Step: 2830, Subject: sub2; Training Loss: 0.3201369643211365, Val Loss: 0.8922916650772095\n",
      "Step: 2830, Subject: sub3; Training Loss: 0.5460870265960693, Val Loss: 0.8767896890640259\n",
      "Step: 2830, Subject: sub4; Training Loss: 0.35034269094467163, Val Loss: 0.8793836832046509\n",
      "Step: 2830, Subject: sub5; Training Loss: 0.4195742607116699, Val Loss: 0.7798939943313599\n",
      "Step: 2830, Subject: sub7; Training Loss: 0.3669290542602539, Val Loss: 0.813440203666687\n",
      "Step: 2840, Subject: sub1; Training Loss: 0.4515143632888794, Val Loss: 1.1095335483551025\n",
      "Step: 2840, Subject: sub2; Training Loss: 0.30896440148353577, Val Loss: 0.8845470547676086\n",
      "Step: 2840, Subject: sub3; Training Loss: 0.5890921354293823, Val Loss: 0.8170065879821777\n",
      "Step: 2840, Subject: sub4; Training Loss: 0.3100733757019043, Val Loss: 0.9169643521308899\n",
      "Step: 2840, Subject: sub5; Training Loss: 0.34089356660842896, Val Loss: 1.0667293071746826\n",
      "Step: 2840, Subject: sub7; Training Loss: 0.4540238380432129, Val Loss: 1.1470346450805664\n",
      "Step: 2850, Subject: sub1; Training Loss: 0.42072975635528564, Val Loss: 1.2627322673797607\n",
      "Step: 2850, Subject: sub2; Training Loss: 0.23788604140281677, Val Loss: 0.9427249431610107\n",
      "Step: 2850, Subject: sub3; Training Loss: 0.6205283403396606, Val Loss: 0.9121021032333374\n",
      "Step: 2850, Subject: sub4; Training Loss: 0.4429478049278259, Val Loss: 0.9334138631820679\n",
      "Step: 2850, Subject: sub5; Training Loss: 0.48002976179122925, Val Loss: 0.8241561651229858\n",
      "Step: 2850, Subject: sub7; Training Loss: 0.35276567935943604, Val Loss: 0.8912215828895569\n",
      "Step: 2860, Subject: sub1; Training Loss: 0.3060818314552307, Val Loss: 1.2417268753051758\n",
      "Step: 2860, Subject: sub2; Training Loss: 0.24481762945652008, Val Loss: 0.8539276719093323\n",
      "Step: 2860, Subject: sub3; Training Loss: 0.6494249701499939, Val Loss: 0.9283954501152039\n",
      "Step: 2860, Subject: sub4; Training Loss: 0.369509220123291, Val Loss: 0.9192829728126526\n",
      "Step: 2860, Subject: sub5; Training Loss: 0.43946415185928345, Val Loss: 1.136368989944458\n",
      "Step: 2860, Subject: sub7; Training Loss: 0.4143795371055603, Val Loss: 1.522813320159912\n",
      "Step: 2870, Subject: sub1; Training Loss: 0.21483096480369568, Val Loss: 1.3413993120193481\n",
      "Step: 2870, Subject: sub2; Training Loss: 0.2291635274887085, Val Loss: 0.792449951171875\n",
      "Step: 2870, Subject: sub3; Training Loss: 0.5622742176055908, Val Loss: 0.8686736822128296\n",
      "Step: 2870, Subject: sub4; Training Loss: 0.41403400897979736, Val Loss: 0.9525372982025146\n",
      "Step: 2870, Subject: sub5; Training Loss: 0.5319737195968628, Val Loss: 1.0877597332000732\n",
      "Step: 2870, Subject: sub7; Training Loss: 0.4558344781398773, Val Loss: 1.271963119506836\n",
      "Step: 2880, Subject: sub1; Training Loss: 0.3567119836807251, Val Loss: 1.5329289436340332\n",
      "Step: 2880, Subject: sub2; Training Loss: 0.3200058341026306, Val Loss: 0.8555487394332886\n",
      "Step: 2880, Subject: sub3; Training Loss: 0.4995492696762085, Val Loss: 0.8295661211013794\n",
      "Step: 2880, Subject: sub4; Training Loss: 0.30698680877685547, Val Loss: 0.9000427722930908\n",
      "Step: 2880, Subject: sub5; Training Loss: 0.42029333114624023, Val Loss: 1.1273040771484375\n",
      "Step: 2880, Subject: sub7; Training Loss: 0.3869733214378357, Val Loss: 1.5023794174194336\n",
      "Step: 2890, Subject: sub1; Training Loss: 0.3392517566680908, Val Loss: 1.3859541416168213\n",
      "Step: 2890, Subject: sub2; Training Loss: 0.2524973452091217, Val Loss: 0.8817467093467712\n",
      "Step: 2890, Subject: sub3; Training Loss: 0.5848002433776855, Val Loss: 1.09828519821167\n",
      "Step: 2890, Subject: sub4; Training Loss: 0.2907070517539978, Val Loss: 0.836603045463562\n",
      "Step: 2890, Subject: sub5; Training Loss: 0.4090254604816437, Val Loss: 0.8324673175811768\n",
      "Step: 2890, Subject: sub7; Training Loss: 0.5612941980361938, Val Loss: 1.056681513786316\n",
      "Step: 2900, Subject: sub1; Training Loss: 0.3557198643684387, Val Loss: 1.2752009630203247\n",
      "Step: 2900, Subject: sub2; Training Loss: 0.22271490097045898, Val Loss: 0.8435078859329224\n",
      "Step: 2900, Subject: sub3; Training Loss: 0.5478157997131348, Val Loss: 0.9313311576843262\n",
      "Step: 2900, Subject: sub4; Training Loss: 0.2784802317619324, Val Loss: 1.1316672563552856\n",
      "Step: 2900, Subject: sub5; Training Loss: 0.35550564527511597, Val Loss: 0.7319158315658569\n",
      "Step: 2900, Subject: sub7; Training Loss: 0.44108134508132935, Val Loss: 0.8907457590103149\n",
      "Step: 2910, Subject: sub1; Training Loss: 0.39398208260536194, Val Loss: 1.0166759490966797\n",
      "Step: 2910, Subject: sub2; Training Loss: 0.2727677822113037, Val Loss: 0.9393012523651123\n",
      "Step: 2910, Subject: sub3; Training Loss: 0.6060779690742493, Val Loss: 1.057286262512207\n",
      "Step: 2910, Subject: sub4; Training Loss: 0.3530919551849365, Val Loss: 0.8441008925437927\n",
      "Step: 2910, Subject: sub5; Training Loss: 0.4210852384567261, Val Loss: 1.0930957794189453\n",
      "Step: 2910, Subject: sub7; Training Loss: 0.32028472423553467, Val Loss: 1.4629430770874023\n",
      "Step: 2920, Subject: sub1; Training Loss: 0.30089715123176575, Val Loss: 0.9103168845176697\n",
      "Step: 2920, Subject: sub2; Training Loss: 0.2451619803905487, Val Loss: 0.8307158946990967\n",
      "Step: 2920, Subject: sub3; Training Loss: 0.5866304636001587, Val Loss: 0.9088077545166016\n",
      "Step: 2920, Subject: sub4; Training Loss: 0.25571930408477783, Val Loss: 0.9328448176383972\n",
      "Step: 2920, Subject: sub5; Training Loss: 0.34707212448120117, Val Loss: 0.6931349635124207\n",
      "Step: 2920, Subject: sub7; Training Loss: 0.4378628432750702, Val Loss: 1.4286468029022217\n",
      "Step: 2930, Subject: sub1; Training Loss: 0.27357247471809387, Val Loss: 1.4943598508834839\n",
      "Step: 2930, Subject: sub2; Training Loss: 0.24668380618095398, Val Loss: 0.9604290127754211\n",
      "Step: 2930, Subject: sub3; Training Loss: 0.49447131156921387, Val Loss: 0.9509624242782593\n",
      "Step: 2930, Subject: sub4; Training Loss: 0.25859367847442627, Val Loss: 1.0004982948303223\n",
      "Step: 2930, Subject: sub5; Training Loss: 0.37859416007995605, Val Loss: 0.8652957081794739\n",
      "Step: 2930, Subject: sub7; Training Loss: 0.4154107868671417, Val Loss: 1.4206745624542236\n",
      "Step: 2940, Subject: sub1; Training Loss: 0.28651148080825806, Val Loss: 1.0927995443344116\n",
      "Step: 2940, Subject: sub2; Training Loss: 0.2157088667154312, Val Loss: 1.0131909847259521\n",
      "Step: 2940, Subject: sub3; Training Loss: 0.511013388633728, Val Loss: 1.201878547668457\n",
      "Step: 2940, Subject: sub4; Training Loss: 0.2466629445552826, Val Loss: 0.8750762939453125\n",
      "Step: 2940, Subject: sub5; Training Loss: 0.3958219289779663, Val Loss: 1.0714281797409058\n",
      "Step: 2940, Subject: sub7; Training Loss: 0.4216347336769104, Val Loss: 0.9664360880851746\n",
      "Step: 2950, Subject: sub1; Training Loss: 0.3085072636604309, Val Loss: 1.0188794136047363\n",
      "Step: 2950, Subject: sub2; Training Loss: 0.2411005198955536, Val Loss: 0.9286091327667236\n",
      "Step: 2950, Subject: sub3; Training Loss: 0.5650526285171509, Val Loss: 1.0543248653411865\n",
      "Step: 2950, Subject: sub4; Training Loss: 0.3211086094379425, Val Loss: 0.9171217679977417\n",
      "Step: 2950, Subject: sub5; Training Loss: 0.38670778274536133, Val Loss: 1.1603550910949707\n",
      "Step: 2950, Subject: sub7; Training Loss: 0.41843515634536743, Val Loss: 1.046998143196106\n",
      "Step: 2960, Subject: sub1; Training Loss: 0.22353410720825195, Val Loss: 0.7657753229141235\n",
      "Step: 2960, Subject: sub2; Training Loss: 0.1804998219013214, Val Loss: 0.8302369117736816\n",
      "Step: 2960, Subject: sub3; Training Loss: 0.5580447912216187, Val Loss: 0.8436152338981628\n",
      "Step: 2960, Subject: sub4; Training Loss: 0.2999463379383087, Val Loss: 0.9724246859550476\n",
      "Step: 2960, Subject: sub5; Training Loss: 0.4085901379585266, Val Loss: 0.7866119146347046\n",
      "Step: 2960, Subject: sub7; Training Loss: 0.4410947859287262, Val Loss: 0.7969014644622803\n",
      "Step: 2970, Subject: sub1; Training Loss: 0.3303402066230774, Val Loss: 1.6183438301086426\n",
      "Step: 2970, Subject: sub2; Training Loss: 0.16797125339508057, Val Loss: 0.84433513879776\n",
      "Step: 2970, Subject: sub3; Training Loss: 0.5438706278800964, Val Loss: 0.9663922786712646\n",
      "Step: 2970, Subject: sub4; Training Loss: 0.3001634180545807, Val Loss: 0.9170480966567993\n",
      "Step: 2970, Subject: sub5; Training Loss: 0.41567856073379517, Val Loss: 1.0700480937957764\n",
      "Step: 2970, Subject: sub7; Training Loss: 0.3800910711288452, Val Loss: 1.102095365524292\n",
      "Step: 2980, Subject: sub1; Training Loss: 0.2567504942417145, Val Loss: 1.0484920740127563\n",
      "Step: 2980, Subject: sub2; Training Loss: 0.23032240569591522, Val Loss: 0.8828042149543762\n",
      "Step: 2980, Subject: sub3; Training Loss: 0.5521516799926758, Val Loss: 0.9472267627716064\n",
      "Step: 2980, Subject: sub4; Training Loss: 0.23741018772125244, Val Loss: 0.7847452163696289\n",
      "Step: 2980, Subject: sub5; Training Loss: 0.4513899087905884, Val Loss: 0.9756242632865906\n",
      "Step: 2980, Subject: sub7; Training Loss: 0.4267929196357727, Val Loss: 0.9658005237579346\n",
      "Step: 2990, Subject: sub1; Training Loss: 0.25487905740737915, Val Loss: 0.826911211013794\n",
      "Step: 2990, Subject: sub2; Training Loss: 0.23401758074760437, Val Loss: 0.7863776683807373\n",
      "Step: 2990, Subject: sub3; Training Loss: 0.5834962129592896, Val Loss: 0.9245322346687317\n",
      "Step: 2990, Subject: sub4; Training Loss: 0.3032360374927521, Val Loss: 0.9143964052200317\n",
      "Step: 2990, Subject: sub5; Training Loss: 0.3278766870498657, Val Loss: 0.8349795341491699\n",
      "Step: 2990, Subject: sub7; Training Loss: 0.4343254268169403, Val Loss: 0.831688404083252\n",
      "Step: 3000, Subject: sub1; Training Loss: 0.2567024230957031, Val Loss: 1.0739469528198242\n",
      "Step: 3000, Subject: sub2; Training Loss: 0.2602168321609497, Val Loss: 0.9897100925445557\n",
      "Step: 3000, Subject: sub3; Training Loss: 0.4641472101211548, Val Loss: 1.0091959238052368\n",
      "Step: 3000, Subject: sub4; Training Loss: 0.34717851877212524, Val Loss: 1.1150695085525513\n",
      "Step: 3000, Subject: sub5; Training Loss: 0.4093482494354248, Val Loss: 1.0397489070892334\n",
      "Step: 3000, Subject: sub7; Training Loss: 0.42057564854621887, Val Loss: 1.1504454612731934\n",
      "Step: 3010, Subject: sub1; Training Loss: 0.4006669223308563, Val Loss: 1.1562154293060303\n",
      "Step: 3010, Subject: sub2; Training Loss: 0.27781805396080017, Val Loss: 0.8625510334968567\n",
      "Step: 3010, Subject: sub3; Training Loss: 0.5415232181549072, Val Loss: 1.037522554397583\n",
      "Step: 3010, Subject: sub4; Training Loss: 0.2640911936759949, Val Loss: 0.9559952020645142\n",
      "Step: 3010, Subject: sub5; Training Loss: 0.38699132204055786, Val Loss: 0.9286468029022217\n",
      "Step: 3010, Subject: sub7; Training Loss: 0.36199623346328735, Val Loss: 1.269412636756897\n",
      "Step: 3020, Subject: sub1; Training Loss: 0.28157126903533936, Val Loss: 0.9347805976867676\n",
      "Step: 3020, Subject: sub2; Training Loss: 0.24767836928367615, Val Loss: 0.9556511044502258\n",
      "Step: 3020, Subject: sub3; Training Loss: 0.5424200296401978, Val Loss: 0.876008152961731\n",
      "Step: 3020, Subject: sub4; Training Loss: 0.3186398446559906, Val Loss: 0.8622885942459106\n",
      "Step: 3020, Subject: sub5; Training Loss: 0.43954789638519287, Val Loss: 0.9111561179161072\n",
      "Step: 3020, Subject: sub7; Training Loss: 0.4374948740005493, Val Loss: 0.7353476881980896\n",
      "Step: 3030, Subject: sub1; Training Loss: 0.25658971071243286, Val Loss: 1.7203192710876465\n",
      "Step: 3030, Subject: sub2; Training Loss: 0.2996640205383301, Val Loss: 0.8703291416168213\n",
      "Step: 3030, Subject: sub3; Training Loss: 0.5150893926620483, Val Loss: 0.8493745923042297\n",
      "Step: 3030, Subject: sub4; Training Loss: 0.3462272584438324, Val Loss: 0.8558429479598999\n",
      "Step: 3030, Subject: sub5; Training Loss: 0.36501002311706543, Val Loss: 1.1805955171585083\n",
      "Step: 3030, Subject: sub7; Training Loss: 0.40164312720298767, Val Loss: 1.0707838535308838\n",
      "Step: 3040, Subject: sub1; Training Loss: 0.3064899444580078, Val Loss: 0.7553800344467163\n",
      "Step: 3040, Subject: sub2; Training Loss: 0.18887591361999512, Val Loss: 0.7810258865356445\n",
      "Step: 3040, Subject: sub3; Training Loss: 0.5901647806167603, Val Loss: 0.8325434923171997\n",
      "Step: 3040, Subject: sub4; Training Loss: 0.2776561975479126, Val Loss: 0.8069925308227539\n",
      "Step: 3040, Subject: sub5; Training Loss: 0.374045193195343, Val Loss: 0.6788912415504456\n",
      "Step: 3040, Subject: sub7; Training Loss: 0.41480517387390137, Val Loss: 0.7908707857131958\n",
      "Step: 3050, Subject: sub1; Training Loss: 0.24002481997013092, Val Loss: 0.9983807802200317\n",
      "Step: 3050, Subject: sub2; Training Loss: 0.1813199520111084, Val Loss: 0.8908412456512451\n",
      "Step: 3050, Subject: sub3; Training Loss: 0.5513575077056885, Val Loss: 0.9472945332527161\n",
      "Step: 3050, Subject: sub4; Training Loss: 0.38483184576034546, Val Loss: 0.7129939794540405\n",
      "Step: 3050, Subject: sub5; Training Loss: 0.3713293671607971, Val Loss: 0.9380676746368408\n",
      "Step: 3050, Subject: sub7; Training Loss: 0.3792967200279236, Val Loss: 1.240193486213684\n",
      "Step: 3060, Subject: sub1; Training Loss: 0.30079078674316406, Val Loss: 0.853134274482727\n",
      "Step: 3060, Subject: sub2; Training Loss: 0.24628251791000366, Val Loss: 0.9898005723953247\n",
      "Step: 3060, Subject: sub3; Training Loss: 0.5601524114608765, Val Loss: 0.9177782535552979\n",
      "Step: 3060, Subject: sub4; Training Loss: 0.27046293020248413, Val Loss: 1.0748744010925293\n",
      "Step: 3060, Subject: sub5; Training Loss: 0.42534130811691284, Val Loss: 0.9290868043899536\n",
      "Step: 3060, Subject: sub7; Training Loss: 0.43315112590789795, Val Loss: 0.8607996106147766\n",
      "Step: 3070, Subject: sub1; Training Loss: 0.27059704065322876, Val Loss: 1.3292316198349\n",
      "Step: 3070, Subject: sub2; Training Loss: 0.20183464884757996, Val Loss: 0.9296324253082275\n",
      "Step: 3070, Subject: sub3; Training Loss: 0.4851417541503906, Val Loss: 0.9268238544464111\n",
      "Step: 3070, Subject: sub4; Training Loss: 0.20033679902553558, Val Loss: 0.8460764288902283\n",
      "Step: 3070, Subject: sub5; Training Loss: 0.4671754240989685, Val Loss: 1.0532230138778687\n",
      "Step: 3070, Subject: sub7; Training Loss: 0.4238732159137726, Val Loss: 1.0912805795669556\n",
      "Step: 3080, Subject: sub1; Training Loss: 0.246665358543396, Val Loss: 0.8690236210823059\n",
      "Step: 3080, Subject: sub2; Training Loss: 0.2916806936264038, Val Loss: 0.7751387357711792\n",
      "Step: 3080, Subject: sub3; Training Loss: 0.5488086342811584, Val Loss: 0.9413970708847046\n",
      "Step: 3080, Subject: sub4; Training Loss: 0.3285859525203705, Val Loss: 0.8595723509788513\n",
      "Step: 3080, Subject: sub5; Training Loss: 0.3656488358974457, Val Loss: 0.7345610857009888\n",
      "Step: 3080, Subject: sub7; Training Loss: 0.35766851902008057, Val Loss: 0.8462661504745483\n",
      "Step: 3090, Subject: sub1; Training Loss: 0.24052239954471588, Val Loss: 0.9808655977249146\n",
      "Step: 3090, Subject: sub2; Training Loss: 0.19509956240653992, Val Loss: 1.1400501728057861\n",
      "Step: 3090, Subject: sub3; Training Loss: 0.4414229393005371, Val Loss: 0.803522527217865\n",
      "Step: 3090, Subject: sub4; Training Loss: 0.3360498249530792, Val Loss: 1.0024161338806152\n",
      "Step: 3090, Subject: sub5; Training Loss: 0.4396267235279083, Val Loss: 0.7302971482276917\n",
      "Step: 3090, Subject: sub7; Training Loss: 0.3495396375656128, Val Loss: 0.8131206035614014\n",
      "Step: 3100, Subject: sub1; Training Loss: 0.25522130727767944, Val Loss: 0.77886563539505\n",
      "Step: 3100, Subject: sub2; Training Loss: 0.23892559111118317, Val Loss: 0.826606035232544\n",
      "Step: 3100, Subject: sub3; Training Loss: 0.5127939581871033, Val Loss: 0.802793025970459\n",
      "Step: 3100, Subject: sub4; Training Loss: 0.2939821481704712, Val Loss: 0.896833598613739\n",
      "Step: 3100, Subject: sub5; Training Loss: 0.37952786684036255, Val Loss: 0.7369793057441711\n",
      "Step: 3100, Subject: sub7; Training Loss: 0.38165730237960815, Val Loss: 1.0044000148773193\n",
      "Step: 3110, Subject: sub1; Training Loss: 0.3485766053199768, Val Loss: 1.3416975736618042\n",
      "Step: 3110, Subject: sub2; Training Loss: 0.1935662478208542, Val Loss: 0.9639565944671631\n",
      "Step: 3110, Subject: sub3; Training Loss: 0.5393942594528198, Val Loss: 1.2146576642990112\n",
      "Step: 3110, Subject: sub4; Training Loss: 0.2050757110118866, Val Loss: 0.8952804803848267\n",
      "Step: 3110, Subject: sub5; Training Loss: 0.3923138380050659, Val Loss: 1.3003098964691162\n",
      "Step: 3110, Subject: sub7; Training Loss: 0.3385617733001709, Val Loss: 1.3562721014022827\n",
      "Step: 3120, Subject: sub1; Training Loss: 0.28450942039489746, Val Loss: 0.9946545958518982\n",
      "Step: 3120, Subject: sub2; Training Loss: 0.18682646751403809, Val Loss: 0.8789272904396057\n",
      "Step: 3120, Subject: sub3; Training Loss: 0.516496479511261, Val Loss: 1.0196533203125\n",
      "Step: 3120, Subject: sub4; Training Loss: 0.24877040088176727, Val Loss: 0.9031636714935303\n",
      "Step: 3120, Subject: sub5; Training Loss: 0.44147902727127075, Val Loss: 1.2146339416503906\n",
      "Step: 3120, Subject: sub7; Training Loss: 0.44915851950645447, Val Loss: 1.5996687412261963\n",
      "Step: 3130, Subject: sub1; Training Loss: 0.216434508562088, Val Loss: 0.9862121343612671\n",
      "Step: 3130, Subject: sub2; Training Loss: 0.25165772438049316, Val Loss: 0.869179368019104\n",
      "Step: 3130, Subject: sub3; Training Loss: 0.5072586536407471, Val Loss: 0.906078577041626\n",
      "Step: 3130, Subject: sub4; Training Loss: 0.30290675163269043, Val Loss: 1.027685523033142\n",
      "Step: 3130, Subject: sub5; Training Loss: 0.3643236756324768, Val Loss: 0.8634077310562134\n",
      "Step: 3130, Subject: sub7; Training Loss: 0.3658631443977356, Val Loss: 0.9775339365005493\n",
      "Step: 3140, Subject: sub1; Training Loss: 0.20768827199935913, Val Loss: 1.382106900215149\n",
      "Step: 3140, Subject: sub2; Training Loss: 0.2637268304824829, Val Loss: 0.865217924118042\n",
      "Step: 3140, Subject: sub3; Training Loss: 0.5174323320388794, Val Loss: 1.028771996498108\n",
      "Step: 3140, Subject: sub4; Training Loss: 0.3127443194389343, Val Loss: 0.8752413988113403\n",
      "Step: 3140, Subject: sub5; Training Loss: 0.4007822573184967, Val Loss: 0.9821990728378296\n",
      "Step: 3140, Subject: sub7; Training Loss: 0.37915071845054626, Val Loss: 1.304517388343811\n",
      "Step: 3150, Subject: sub1; Training Loss: 0.23466725647449493, Val Loss: 1.046600103378296\n",
      "Step: 3150, Subject: sub2; Training Loss: 0.25261572003364563, Val Loss: 0.8888938426971436\n",
      "Step: 3150, Subject: sub3; Training Loss: 0.530265748500824, Val Loss: 0.8423093557357788\n",
      "Step: 3150, Subject: sub4; Training Loss: 0.3415125012397766, Val Loss: 0.9402053356170654\n",
      "Step: 3150, Subject: sub5; Training Loss: 0.4808260202407837, Val Loss: 0.6967853903770447\n",
      "Step: 3150, Subject: sub7; Training Loss: 0.3666667640209198, Val Loss: 1.0047268867492676\n",
      "Step: 3160, Subject: sub1; Training Loss: 0.25648927688598633, Val Loss: 1.6902804374694824\n",
      "Step: 3160, Subject: sub2; Training Loss: 0.23958733677864075, Val Loss: 0.8085737228393555\n",
      "Step: 3160, Subject: sub3; Training Loss: 0.530436635017395, Val Loss: 1.2182950973510742\n",
      "Step: 3160, Subject: sub4; Training Loss: 0.22662603855133057, Val Loss: 0.9990766048431396\n",
      "Step: 3160, Subject: sub5; Training Loss: 0.37633317708969116, Val Loss: 1.0957591533660889\n",
      "Step: 3160, Subject: sub7; Training Loss: 0.4347887635231018, Val Loss: 1.0329052209854126\n",
      "Step: 3170, Subject: sub1; Training Loss: 0.25895583629608154, Val Loss: 1.2381303310394287\n",
      "Step: 3170, Subject: sub2; Training Loss: 0.1822684407234192, Val Loss: 0.8561602830886841\n",
      "Step: 3170, Subject: sub3; Training Loss: 0.47471755743026733, Val Loss: 0.8789026737213135\n",
      "Step: 3170, Subject: sub4; Training Loss: 0.34107789397239685, Val Loss: 0.843877911567688\n",
      "Step: 3170, Subject: sub5; Training Loss: 0.44403308629989624, Val Loss: 0.6790732145309448\n",
      "Step: 3170, Subject: sub7; Training Loss: 0.43151718378067017, Val Loss: 1.031339168548584\n",
      "Step: 3180, Subject: sub1; Training Loss: 0.3171229362487793, Val Loss: 1.1434931755065918\n",
      "Step: 3180, Subject: sub2; Training Loss: 0.220589280128479, Val Loss: 0.9164842367172241\n",
      "Step: 3180, Subject: sub3; Training Loss: 0.48642921447753906, Val Loss: 0.8157805800437927\n",
      "Step: 3180, Subject: sub4; Training Loss: 0.23467057943344116, Val Loss: 0.905070960521698\n",
      "Step: 3180, Subject: sub5; Training Loss: 0.4461686611175537, Val Loss: 0.8399794101715088\n",
      "Step: 3180, Subject: sub7; Training Loss: 0.39107847213745117, Val Loss: 1.1224862337112427\n",
      "Step: 3190, Subject: sub1; Training Loss: 0.31289443373680115, Val Loss: 1.5139966011047363\n",
      "Step: 3190, Subject: sub2; Training Loss: 0.21324092149734497, Val Loss: 0.8100849390029907\n",
      "Step: 3190, Subject: sub3; Training Loss: 0.444216251373291, Val Loss: 0.8078001737594604\n",
      "Step: 3190, Subject: sub4; Training Loss: 0.26044797897338867, Val Loss: 1.1129276752471924\n",
      "Step: 3190, Subject: sub5; Training Loss: 0.4107944965362549, Val Loss: 1.0058046579360962\n",
      "Step: 3190, Subject: sub7; Training Loss: 0.39384520053863525, Val Loss: 1.2136049270629883\n",
      "Step: 3200, Subject: sub1; Training Loss: 0.33292171359062195, Val Loss: 1.810238003730774\n",
      "Step: 3200, Subject: sub2; Training Loss: 0.2089301198720932, Val Loss: 0.920956552028656\n",
      "Step: 3200, Subject: sub3; Training Loss: 0.5128558874130249, Val Loss: 1.1377191543579102\n",
      "Step: 3200, Subject: sub4; Training Loss: 0.2854989767074585, Val Loss: 0.9359930753707886\n",
      "Step: 3200, Subject: sub5; Training Loss: 0.4405860900878906, Val Loss: 0.9649502635002136\n",
      "Step: 3200, Subject: sub7; Training Loss: 0.35836032032966614, Val Loss: 1.1159372329711914\n",
      "Step: 3210, Subject: sub1; Training Loss: 0.24269907176494598, Val Loss: 1.0298815965652466\n",
      "Step: 3210, Subject: sub2; Training Loss: 0.18938890099525452, Val Loss: 0.87497878074646\n",
      "Step: 3210, Subject: sub3; Training Loss: 0.5234569907188416, Val Loss: 1.136643648147583\n",
      "Step: 3210, Subject: sub4; Training Loss: 0.25687795877456665, Val Loss: 1.3185840845108032\n",
      "Step: 3210, Subject: sub5; Training Loss: 0.43440693616867065, Val Loss: 1.1400768756866455\n",
      "Step: 3210, Subject: sub7; Training Loss: 0.31965547800064087, Val Loss: 1.6288275718688965\n",
      "Step: 3220, Subject: sub1; Training Loss: 0.24134092032909393, Val Loss: 0.8372786045074463\n",
      "Step: 3220, Subject: sub2; Training Loss: 0.17133674025535583, Val Loss: 0.8793346881866455\n",
      "Step: 3220, Subject: sub3; Training Loss: 0.5360058546066284, Val Loss: 1.0050965547561646\n",
      "Step: 3220, Subject: sub4; Training Loss: 0.29657694697380066, Val Loss: 1.1006133556365967\n",
      "Step: 3220, Subject: sub5; Training Loss: 0.32435670495033264, Val Loss: 0.9944963455200195\n",
      "Step: 3220, Subject: sub7; Training Loss: 0.3752697706222534, Val Loss: 1.2378946542739868\n",
      "Step: 3230, Subject: sub1; Training Loss: 0.23456460237503052, Val Loss: 1.91448974609375\n",
      "Step: 3230, Subject: sub2; Training Loss: 0.1767202615737915, Val Loss: 1.1099684238433838\n",
      "Step: 3230, Subject: sub3; Training Loss: 0.5712220668792725, Val Loss: 0.8343174457550049\n",
      "Step: 3230, Subject: sub4; Training Loss: 0.18776847422122955, Val Loss: 0.8196947574615479\n",
      "Step: 3230, Subject: sub5; Training Loss: 0.45493075251579285, Val Loss: 0.8985360264778137\n",
      "Step: 3230, Subject: sub7; Training Loss: 0.38536298274993896, Val Loss: 1.1274943351745605\n",
      "Step: 3240, Subject: sub1; Training Loss: 0.2023785263299942, Val Loss: 0.8909832239151001\n",
      "Step: 3240, Subject: sub2; Training Loss: 0.18819281458854675, Val Loss: 0.8208718299865723\n",
      "Step: 3240, Subject: sub3; Training Loss: 0.5342621803283691, Val Loss: 0.8396728038787842\n",
      "Step: 3240, Subject: sub4; Training Loss: 0.33874204754829407, Val Loss: 0.964146077632904\n",
      "Step: 3240, Subject: sub5; Training Loss: 0.33643102645874023, Val Loss: 0.9797916412353516\n",
      "Step: 3240, Subject: sub7; Training Loss: 0.350538969039917, Val Loss: 1.0799214839935303\n",
      "Step: 3250, Subject: sub1; Training Loss: 0.22149619460105896, Val Loss: 0.9154191017150879\n",
      "Step: 3250, Subject: sub2; Training Loss: 0.1822081208229065, Val Loss: 0.7749278545379639\n",
      "Step: 3250, Subject: sub3; Training Loss: 0.5029313564300537, Val Loss: 0.7246667146682739\n",
      "Step: 3250, Subject: sub4; Training Loss: 0.32864171266555786, Val Loss: 0.9257744550704956\n",
      "Step: 3250, Subject: sub5; Training Loss: 0.3425779342651367, Val Loss: 0.6624576449394226\n",
      "Step: 3250, Subject: sub7; Training Loss: 0.4248868227005005, Val Loss: 1.0115070343017578\n",
      "Step: 3260, Subject: sub1; Training Loss: 0.2506639361381531, Val Loss: 1.1859065294265747\n",
      "Step: 3260, Subject: sub2; Training Loss: 0.17744080722332, Val Loss: 0.8096798658370972\n",
      "Step: 3260, Subject: sub3; Training Loss: 0.5115636587142944, Val Loss: 0.8514137864112854\n",
      "Step: 3260, Subject: sub4; Training Loss: 0.3327282667160034, Val Loss: 0.5894036293029785\n",
      "Saving model with validation loss: 0.5894036293029785\n",
      "\n",
      "Step: 3260, Subject: sub5; Training Loss: 0.3972451090812683, Val Loss: 0.7249701023101807\n",
      "Step: 3260, Subject: sub7; Training Loss: 0.3565825819969177, Val Loss: 1.2327609062194824\n",
      "Step: 3270, Subject: sub1; Training Loss: 0.28830382227897644, Val Loss: 1.1870684623718262\n",
      "Step: 3270, Subject: sub2; Training Loss: 0.1642812192440033, Val Loss: 0.8318618535995483\n",
      "Step: 3270, Subject: sub3; Training Loss: 0.5042839050292969, Val Loss: 0.8896769881248474\n",
      "Step: 3270, Subject: sub4; Training Loss: 0.4423489570617676, Val Loss: 0.8795011639595032\n",
      "Step: 3270, Subject: sub5; Training Loss: 0.3176387548446655, Val Loss: 1.074037790298462\n",
      "Step: 3270, Subject: sub7; Training Loss: 0.3852640688419342, Val Loss: 1.430307149887085\n",
      "Step: 3280, Subject: sub1; Training Loss: 0.2996591627597809, Val Loss: 0.7386696934700012\n",
      "Step: 3280, Subject: sub2; Training Loss: 0.2362823784351349, Val Loss: 1.0145518779754639\n",
      "Step: 3280, Subject: sub3; Training Loss: 0.5441277623176575, Val Loss: 1.134604573249817\n",
      "Step: 3280, Subject: sub4; Training Loss: 0.26158279180526733, Val Loss: 1.0875217914581299\n",
      "Step: 3280, Subject: sub5; Training Loss: 0.47852370142936707, Val Loss: 0.9304984211921692\n",
      "Step: 3280, Subject: sub7; Training Loss: 0.3625381886959076, Val Loss: 1.2798926830291748\n",
      "Step: 3290, Subject: sub1; Training Loss: 0.24841046333312988, Val Loss: 0.83342045545578\n",
      "Step: 3290, Subject: sub2; Training Loss: 0.1772887110710144, Val Loss: 0.7970782518386841\n",
      "Step: 3290, Subject: sub3; Training Loss: 0.55379319190979, Val Loss: 0.755243182182312\n",
      "Step: 3290, Subject: sub4; Training Loss: 0.2456861138343811, Val Loss: 0.9094929695129395\n",
      "Step: 3290, Subject: sub5; Training Loss: 0.37029266357421875, Val Loss: 0.7057214975357056\n",
      "Step: 3290, Subject: sub7; Training Loss: 0.29654431343078613, Val Loss: 0.8929260969161987\n",
      "Step: 3300, Subject: sub1; Training Loss: 0.25220465660095215, Val Loss: 1.5942946672439575\n",
      "Step: 3300, Subject: sub2; Training Loss: 0.18764378130435944, Val Loss: 0.9639923572540283\n",
      "Step: 3300, Subject: sub3; Training Loss: 0.5449293851852417, Val Loss: 1.009849190711975\n",
      "Step: 3300, Subject: sub4; Training Loss: 0.2388966828584671, Val Loss: 1.0935629606246948\n",
      "Step: 3300, Subject: sub5; Training Loss: 0.37171465158462524, Val Loss: 1.190058708190918\n",
      "Step: 3300, Subject: sub7; Training Loss: 0.4185451865196228, Val Loss: 1.385114312171936\n",
      "Step: 3310, Subject: sub1; Training Loss: 0.20029349625110626, Val Loss: 1.2597943544387817\n",
      "Step: 3310, Subject: sub2; Training Loss: 0.1666272133588791, Val Loss: 0.9182893633842468\n",
      "Step: 3310, Subject: sub3; Training Loss: 0.4832518696784973, Val Loss: 0.820864737033844\n",
      "Step: 3310, Subject: sub4; Training Loss: 0.2305561900138855, Val Loss: 0.891398549079895\n",
      "Step: 3310, Subject: sub5; Training Loss: 0.4333963990211487, Val Loss: 0.8765634894371033\n",
      "Step: 3310, Subject: sub7; Training Loss: 0.38050904870033264, Val Loss: 1.1059702634811401\n",
      "Step: 3320, Subject: sub1; Training Loss: 0.2660529613494873, Val Loss: 1.0852938890457153\n",
      "Step: 3320, Subject: sub2; Training Loss: 0.1637909710407257, Val Loss: 1.0059022903442383\n",
      "Step: 3320, Subject: sub3; Training Loss: 0.4991564154624939, Val Loss: 0.9043042659759521\n",
      "Step: 3320, Subject: sub4; Training Loss: 0.254437118768692, Val Loss: 1.1035680770874023\n",
      "Step: 3320, Subject: sub5; Training Loss: 0.36045411229133606, Val Loss: 0.7760008573532104\n",
      "Step: 3320, Subject: sub7; Training Loss: 0.32178547978401184, Val Loss: 0.890055775642395\n",
      "Step: 3330, Subject: sub1; Training Loss: 0.24955177307128906, Val Loss: 0.9414300918579102\n",
      "Step: 3330, Subject: sub2; Training Loss: 0.16557708382606506, Val Loss: 1.0413790941238403\n",
      "Step: 3330, Subject: sub3; Training Loss: 0.4386892020702362, Val Loss: 1.1219934225082397\n",
      "Step: 3330, Subject: sub4; Training Loss: 0.2998606860637665, Val Loss: 1.1945980787277222\n",
      "Step: 3330, Subject: sub5; Training Loss: 0.42419129610061646, Val Loss: 0.7125563621520996\n",
      "Step: 3330, Subject: sub7; Training Loss: 0.3595297932624817, Val Loss: 0.9405249953269958\n",
      "Step: 3340, Subject: sub1; Training Loss: 0.3329671621322632, Val Loss: 1.3919100761413574\n",
      "Step: 3340, Subject: sub2; Training Loss: 0.256848007440567, Val Loss: 0.9212403297424316\n",
      "Step: 3340, Subject: sub3; Training Loss: 0.5814980268478394, Val Loss: 0.86643385887146\n",
      "Step: 3340, Subject: sub4; Training Loss: 0.39842674136161804, Val Loss: 0.9938997626304626\n",
      "Step: 3340, Subject: sub5; Training Loss: 0.3672361969947815, Val Loss: 0.7265185713768005\n",
      "Step: 3340, Subject: sub7; Training Loss: 0.28912249207496643, Val Loss: 0.8951987028121948\n",
      "Step: 3350, Subject: sub1; Training Loss: 0.2838324308395386, Val Loss: 1.198483943939209\n",
      "Step: 3350, Subject: sub2; Training Loss: 0.19567137956619263, Val Loss: 0.9265925288200378\n",
      "Step: 3350, Subject: sub3; Training Loss: 0.4273120164871216, Val Loss: 0.8619920015335083\n",
      "Step: 3350, Subject: sub4; Training Loss: 0.2554393410682678, Val Loss: 0.8385229110717773\n",
      "Step: 3350, Subject: sub5; Training Loss: 0.3722098171710968, Val Loss: 0.7236859798431396\n",
      "Step: 3350, Subject: sub7; Training Loss: 0.38070255517959595, Val Loss: 1.243396282196045\n",
      "Step: 3360, Subject: sub1; Training Loss: 0.20442284643650055, Val Loss: 0.6990530490875244\n",
      "Step: 3360, Subject: sub2; Training Loss: 0.19420647621154785, Val Loss: 0.8628293871879578\n",
      "Step: 3360, Subject: sub3; Training Loss: 0.6035110950469971, Val Loss: 0.924436628818512\n",
      "Step: 3360, Subject: sub4; Training Loss: 0.2721264958381653, Val Loss: 0.8962665796279907\n",
      "Step: 3360, Subject: sub5; Training Loss: 0.34839382767677307, Val Loss: 0.7373647689819336\n",
      "Step: 3360, Subject: sub7; Training Loss: 0.3695964813232422, Val Loss: 0.8664034605026245\n",
      "Step: 3370, Subject: sub1; Training Loss: 0.32672420144081116, Val Loss: 1.2376911640167236\n",
      "Step: 3370, Subject: sub2; Training Loss: 0.17592109739780426, Val Loss: 1.0220048427581787\n",
      "Step: 3370, Subject: sub3; Training Loss: 0.5471103191375732, Val Loss: 0.9595407247543335\n",
      "Step: 3370, Subject: sub4; Training Loss: 0.30131474137306213, Val Loss: 1.1414072513580322\n",
      "Step: 3370, Subject: sub5; Training Loss: 0.46494945883750916, Val Loss: 1.1831542253494263\n",
      "Step: 3370, Subject: sub7; Training Loss: 0.3710160553455353, Val Loss: 1.314194917678833\n",
      "Step: 3380, Subject: sub1; Training Loss: 0.236016184091568, Val Loss: 0.9769675731658936\n",
      "Step: 3380, Subject: sub2; Training Loss: 0.1713772565126419, Val Loss: 0.8685995936393738\n",
      "Step: 3380, Subject: sub3; Training Loss: 0.4876444339752197, Val Loss: 1.2797093391418457\n",
      "Step: 3380, Subject: sub4; Training Loss: 0.28716763854026794, Val Loss: 0.7655938863754272\n",
      "Step: 3380, Subject: sub5; Training Loss: 0.37152794003486633, Val Loss: 1.2420462369918823\n",
      "Step: 3380, Subject: sub7; Training Loss: 0.3454970717430115, Val Loss: 1.0008413791656494\n",
      "Step: 3390, Subject: sub1; Training Loss: 0.2362130582332611, Val Loss: 1.3970911502838135\n",
      "Step: 3390, Subject: sub2; Training Loss: 0.20274491608142853, Val Loss: 0.927932858467102\n",
      "Step: 3390, Subject: sub3; Training Loss: 0.44101256132125854, Val Loss: 0.9134919047355652\n",
      "Step: 3390, Subject: sub4; Training Loss: 0.25878989696502686, Val Loss: 0.8642195463180542\n",
      "Step: 3390, Subject: sub5; Training Loss: 0.3052521347999573, Val Loss: 1.05820631980896\n",
      "Step: 3390, Subject: sub7; Training Loss: 0.39869457483291626, Val Loss: 0.9108233451843262\n",
      "Step: 3400, Subject: sub1; Training Loss: 0.23956051468849182, Val Loss: 1.1294103860855103\n",
      "Step: 3400, Subject: sub2; Training Loss: 0.16341698169708252, Val Loss: 0.970710813999176\n",
      "Step: 3400, Subject: sub3; Training Loss: 0.5231720209121704, Val Loss: 0.9445202350616455\n",
      "Step: 3400, Subject: sub4; Training Loss: 0.25880223512649536, Val Loss: 0.8043767213821411\n",
      "Step: 3400, Subject: sub5; Training Loss: 0.3937166929244995, Val Loss: 1.0132503509521484\n",
      "Step: 3400, Subject: sub7; Training Loss: 0.2906123399734497, Val Loss: 0.8640673756599426\n",
      "Step: 3410, Subject: sub1; Training Loss: 0.29336071014404297, Val Loss: 1.628361463546753\n",
      "Step: 3410, Subject: sub2; Training Loss: 0.15965089201927185, Val Loss: 1.0150084495544434\n",
      "Step: 3410, Subject: sub3; Training Loss: 0.49744653701782227, Val Loss: 0.8493548631668091\n",
      "Step: 3410, Subject: sub4; Training Loss: 0.3273881673812866, Val Loss: 0.9106581807136536\n",
      "Step: 3410, Subject: sub5; Training Loss: 0.36234936118125916, Val Loss: 0.6971316337585449\n",
      "Step: 3410, Subject: sub7; Training Loss: 0.3550819754600525, Val Loss: 1.0572224855422974\n",
      "Step: 3420, Subject: sub1; Training Loss: 0.23916767537593842, Val Loss: 1.4212126731872559\n",
      "Step: 3420, Subject: sub2; Training Loss: 0.2100313901901245, Val Loss: 0.8328433036804199\n",
      "Step: 3420, Subject: sub3; Training Loss: 0.44447776675224304, Val Loss: 0.818276047706604\n",
      "Step: 3420, Subject: sub4; Training Loss: 0.29164767265319824, Val Loss: 1.0470712184906006\n",
      "Step: 3420, Subject: sub5; Training Loss: 0.3435874283313751, Val Loss: 0.6706823110580444\n",
      "Step: 3420, Subject: sub7; Training Loss: 0.3544791340827942, Val Loss: 0.9639783501625061\n",
      "Step: 3430, Subject: sub1; Training Loss: 0.17587408423423767, Val Loss: 0.9632117748260498\n",
      "Step: 3430, Subject: sub2; Training Loss: 0.18211252987384796, Val Loss: 0.8394248485565186\n",
      "Step: 3430, Subject: sub3; Training Loss: 0.4646214544773102, Val Loss: 0.8592380881309509\n",
      "Step: 3430, Subject: sub4; Training Loss: 0.27979493141174316, Val Loss: 0.6125146150588989\n",
      "Step: 3430, Subject: sub5; Training Loss: 0.42862817645072937, Val Loss: 0.9983277916908264\n",
      "Step: 3430, Subject: sub7; Training Loss: 0.34385234117507935, Val Loss: 1.189246654510498\n",
      "Step: 3440, Subject: sub1; Training Loss: 0.1833750605583191, Val Loss: 1.0049667358398438\n",
      "Step: 3440, Subject: sub2; Training Loss: 0.15994954109191895, Val Loss: 0.9810725450515747\n",
      "Step: 3440, Subject: sub3; Training Loss: 0.5090467929840088, Val Loss: 1.0905885696411133\n",
      "Step: 3440, Subject: sub4; Training Loss: 0.2131439745426178, Val Loss: 0.8689470291137695\n",
      "Step: 3440, Subject: sub5; Training Loss: 0.4206673502922058, Val Loss: 0.9724664092063904\n",
      "Step: 3440, Subject: sub7; Training Loss: 0.3944363296031952, Val Loss: 1.0028443336486816\n",
      "Step: 3450, Subject: sub1; Training Loss: 0.20900166034698486, Val Loss: 0.996950626373291\n",
      "Step: 3450, Subject: sub2; Training Loss: 0.21325716376304626, Val Loss: 0.7907335758209229\n",
      "Step: 3450, Subject: sub3; Training Loss: 0.5887696743011475, Val Loss: 0.7672719359397888\n",
      "Step: 3450, Subject: sub4; Training Loss: 0.24815133213996887, Val Loss: 0.8334723114967346\n",
      "Step: 3450, Subject: sub5; Training Loss: 0.33970749378204346, Val Loss: 0.622211217880249\n",
      "Step: 3450, Subject: sub7; Training Loss: 0.37616950273513794, Val Loss: 1.210597276687622\n",
      "Step: 3460, Subject: sub1; Training Loss: 0.1853344887495041, Val Loss: 0.5980342626571655\n",
      "Step: 3460, Subject: sub2; Training Loss: 0.18745958805084229, Val Loss: 0.8432525396347046\n",
      "Step: 3460, Subject: sub3; Training Loss: 0.5544278025627136, Val Loss: 0.9233603477478027\n",
      "Step: 3460, Subject: sub4; Training Loss: 0.18104425072669983, Val Loss: 0.8606204986572266\n",
      "Step: 3460, Subject: sub5; Training Loss: 0.3511193096637726, Val Loss: 1.0153188705444336\n",
      "Step: 3460, Subject: sub7; Training Loss: 0.31945520639419556, Val Loss: 1.1663352251052856\n",
      "Step: 3470, Subject: sub1; Training Loss: 0.28775113821029663, Val Loss: 1.1198821067810059\n",
      "Step: 3470, Subject: sub2; Training Loss: 0.19303177297115326, Val Loss: 0.8740878105163574\n",
      "Step: 3470, Subject: sub3; Training Loss: 0.4212358593940735, Val Loss: 0.7405027151107788\n",
      "Step: 3470, Subject: sub4; Training Loss: 0.28122520446777344, Val Loss: 1.0250835418701172\n",
      "Step: 3470, Subject: sub5; Training Loss: 0.4046764075756073, Val Loss: 0.845016598701477\n",
      "Step: 3470, Subject: sub7; Training Loss: 0.31113654375076294, Val Loss: 0.8534265160560608\n",
      "Step: 3480, Subject: sub1; Training Loss: 0.25226861238479614, Val Loss: 1.5701828002929688\n",
      "Step: 3480, Subject: sub2; Training Loss: 0.1777486652135849, Val Loss: 0.9213172197341919\n",
      "Step: 3480, Subject: sub3; Training Loss: 0.47898465394973755, Val Loss: 0.8164900541305542\n",
      "Step: 3480, Subject: sub4; Training Loss: 0.36090534925460815, Val Loss: 0.883971095085144\n",
      "Step: 3480, Subject: sub5; Training Loss: 0.28515154123306274, Val Loss: 1.0131042003631592\n",
      "Step: 3480, Subject: sub7; Training Loss: 0.36819398403167725, Val Loss: 1.161505103111267\n",
      "Step: 3490, Subject: sub1; Training Loss: 0.18903209269046783, Val Loss: 0.9790465831756592\n",
      "Step: 3490, Subject: sub2; Training Loss: 0.21458739042282104, Val Loss: 0.954227864742279\n",
      "Step: 3490, Subject: sub3; Training Loss: 0.4813472628593445, Val Loss: 1.021161675453186\n",
      "Step: 3490, Subject: sub4; Training Loss: 0.23580335080623627, Val Loss: 0.7929982542991638\n",
      "Step: 3490, Subject: sub5; Training Loss: 0.35287195444107056, Val Loss: 0.7541043162345886\n",
      "Step: 3490, Subject: sub7; Training Loss: 0.33754315972328186, Val Loss: 0.973549485206604\n",
      "Step: 3500, Subject: sub1; Training Loss: 0.21562573313713074, Val Loss: 1.0257378816604614\n",
      "Step: 3500, Subject: sub2; Training Loss: 0.18610212206840515, Val Loss: 0.870937705039978\n",
      "Step: 3500, Subject: sub3; Training Loss: 0.551699697971344, Val Loss: 0.8232648968696594\n",
      "Step: 3500, Subject: sub4; Training Loss: 0.19295558333396912, Val Loss: 0.905407190322876\n",
      "Step: 3500, Subject: sub5; Training Loss: 0.4112403094768524, Val Loss: 0.7383546829223633\n",
      "Step: 3500, Subject: sub7; Training Loss: 0.2844659984111786, Val Loss: 1.0913825035095215\n",
      "Step: 3510, Subject: sub1; Training Loss: 0.1552668809890747, Val Loss: 0.6323135495185852\n",
      "Step: 3510, Subject: sub2; Training Loss: 0.22743403911590576, Val Loss: 0.9329298734664917\n",
      "Step: 3510, Subject: sub3; Training Loss: 0.5641298294067383, Val Loss: 0.9580432772636414\n",
      "Step: 3510, Subject: sub4; Training Loss: 0.24131065607070923, Val Loss: 0.6976461410522461\n",
      "Step: 3510, Subject: sub5; Training Loss: 0.40162497758865356, Val Loss: 0.8655969500541687\n",
      "Step: 3510, Subject: sub7; Training Loss: 0.3410889506340027, Val Loss: 0.9811492562294006\n",
      "Step: 3520, Subject: sub1; Training Loss: 0.2230217158794403, Val Loss: 1.2497543096542358\n",
      "Step: 3520, Subject: sub2; Training Loss: 0.13129302859306335, Val Loss: 0.9397655129432678\n",
      "Step: 3520, Subject: sub3; Training Loss: 0.49917346239089966, Val Loss: 1.0334445238113403\n",
      "Step: 3520, Subject: sub4; Training Loss: 0.22942069172859192, Val Loss: 0.9373856782913208\n",
      "Step: 3520, Subject: sub5; Training Loss: 0.3326321840286255, Val Loss: 0.8744868040084839\n",
      "Step: 3520, Subject: sub7; Training Loss: 0.3854649066925049, Val Loss: 1.0880745649337769\n",
      "Step: 3530, Subject: sub1; Training Loss: 0.20598232746124268, Val Loss: 0.6611138582229614\n",
      "Step: 3530, Subject: sub2; Training Loss: 0.1855962574481964, Val Loss: 1.0231181383132935\n",
      "Step: 3530, Subject: sub3; Training Loss: 0.4294254183769226, Val Loss: 1.0113368034362793\n",
      "Step: 3530, Subject: sub4; Training Loss: 0.19066882133483887, Val Loss: 1.1310327053070068\n",
      "Step: 3530, Subject: sub5; Training Loss: 0.324260413646698, Val Loss: 0.8100882768630981\n",
      "Step: 3530, Subject: sub7; Training Loss: 0.30783432722091675, Val Loss: 1.1456489562988281\n",
      "Step: 3540, Subject: sub1; Training Loss: 0.17118164896965027, Val Loss: 1.0996291637420654\n",
      "Step: 3540, Subject: sub2; Training Loss: 0.1718280166387558, Val Loss: 0.840830385684967\n",
      "Step: 3540, Subject: sub3; Training Loss: 0.4810382127761841, Val Loss: 0.8883649110794067\n",
      "Step: 3540, Subject: sub4; Training Loss: 0.23197239637374878, Val Loss: 0.9169868230819702\n",
      "Step: 3540, Subject: sub5; Training Loss: 0.4748351573944092, Val Loss: 0.730096161365509\n",
      "Step: 3540, Subject: sub7; Training Loss: 0.31538113951683044, Val Loss: 0.866010308265686\n",
      "Step: 3550, Subject: sub1; Training Loss: 0.23081672191619873, Val Loss: 0.7496761679649353\n",
      "Step: 3550, Subject: sub2; Training Loss: 0.1864575743675232, Val Loss: 0.8834325075149536\n",
      "Step: 3550, Subject: sub3; Training Loss: 0.5340499877929688, Val Loss: 0.8689777255058289\n",
      "Step: 3550, Subject: sub4; Training Loss: 0.23942524194717407, Val Loss: 1.008489966392517\n",
      "Step: 3550, Subject: sub5; Training Loss: 0.27862757444381714, Val Loss: 0.7584315538406372\n",
      "Step: 3550, Subject: sub7; Training Loss: 0.301728755235672, Val Loss: 0.8233022689819336\n",
      "Step: 3560, Subject: sub1; Training Loss: 0.20974934101104736, Val Loss: 1.3135449886322021\n",
      "Step: 3560, Subject: sub2; Training Loss: 0.24335435032844543, Val Loss: 0.8780229687690735\n",
      "Step: 3560, Subject: sub3; Training Loss: 0.4700218141078949, Val Loss: 0.9799811840057373\n",
      "Step: 3560, Subject: sub4; Training Loss: 0.3318738043308258, Val Loss: 0.8216791749000549\n",
      "Step: 3560, Subject: sub5; Training Loss: 0.3077360987663269, Val Loss: 1.03302800655365\n",
      "Step: 3560, Subject: sub7; Training Loss: 0.32355284690856934, Val Loss: 1.2765352725982666\n",
      "Step: 3570, Subject: sub1; Training Loss: 0.215654194355011, Val Loss: 1.5062505006790161\n",
      "Step: 3570, Subject: sub2; Training Loss: 0.264592707157135, Val Loss: 0.8892788887023926\n",
      "Step: 3570, Subject: sub3; Training Loss: 0.48401981592178345, Val Loss: 0.9689743518829346\n",
      "Step: 3570, Subject: sub4; Training Loss: 0.293599396944046, Val Loss: 0.7244629859924316\n",
      "Step: 3570, Subject: sub5; Training Loss: 0.421194851398468, Val Loss: 0.664638876914978\n",
      "Step: 3570, Subject: sub7; Training Loss: 0.3175630271434784, Val Loss: 1.1648011207580566\n",
      "Step: 3580, Subject: sub1; Training Loss: 0.2676541209220886, Val Loss: 0.7866073846817017\n",
      "Step: 3580, Subject: sub2; Training Loss: 0.17593182623386383, Val Loss: 0.8904255628585815\n",
      "Step: 3580, Subject: sub3; Training Loss: 0.44848212599754333, Val Loss: 0.772962212562561\n",
      "Step: 3580, Subject: sub4; Training Loss: 0.2651461362838745, Val Loss: 1.1571162939071655\n",
      "Step: 3580, Subject: sub5; Training Loss: 0.40874576568603516, Val Loss: 0.6734703779220581\n",
      "Step: 3580, Subject: sub7; Training Loss: 0.32895639538764954, Val Loss: 1.0183155536651611\n",
      "Step: 3590, Subject: sub1; Training Loss: 0.2104164958000183, Val Loss: 0.7342112064361572\n",
      "Step: 3590, Subject: sub2; Training Loss: 0.295340895652771, Val Loss: 0.8822356462478638\n",
      "Step: 3590, Subject: sub3; Training Loss: 0.4417991042137146, Val Loss: 0.7804915308952332\n",
      "Step: 3590, Subject: sub4; Training Loss: 0.238864928483963, Val Loss: 0.8499292135238647\n",
      "Step: 3590, Subject: sub5; Training Loss: 0.4310593605041504, Val Loss: 0.9029606580734253\n",
      "Step: 3590, Subject: sub7; Training Loss: 0.3277449309825897, Val Loss: 1.241838812828064\n",
      "Step: 3600, Subject: sub1; Training Loss: 0.21239347755908966, Val Loss: 1.146091341972351\n",
      "Step: 3600, Subject: sub2; Training Loss: 0.20019397139549255, Val Loss: 0.8491103053092957\n",
      "Step: 3600, Subject: sub3; Training Loss: 0.4464474320411682, Val Loss: 0.8447321057319641\n",
      "Step: 3600, Subject: sub4; Training Loss: 0.2617694139480591, Val Loss: 0.9389151334762573\n",
      "Step: 3600, Subject: sub5; Training Loss: 0.28626710176467896, Val Loss: 0.8583050966262817\n",
      "Step: 3600, Subject: sub7; Training Loss: 0.34402137994766235, Val Loss: 0.8539689779281616\n",
      "Step: 3610, Subject: sub1; Training Loss: 0.2157277911901474, Val Loss: 1.355360984802246\n",
      "Step: 3610, Subject: sub2; Training Loss: 0.17656341195106506, Val Loss: 1.0774471759796143\n",
      "Step: 3610, Subject: sub3; Training Loss: 0.41996634006500244, Val Loss: 1.0551197528839111\n",
      "Step: 3610, Subject: sub4; Training Loss: 0.27691423892974854, Val Loss: 1.3425822257995605\n",
      "Step: 3610, Subject: sub5; Training Loss: 0.31015896797180176, Val Loss: 0.7653200030326843\n",
      "Step: 3610, Subject: sub7; Training Loss: 0.28975093364715576, Val Loss: 0.84116530418396\n",
      "Step: 3620, Subject: sub1; Training Loss: 0.25569528341293335, Val Loss: 1.061759114265442\n",
      "Step: 3620, Subject: sub2; Training Loss: 0.1421494483947754, Val Loss: 1.0634342432022095\n",
      "Step: 3620, Subject: sub3; Training Loss: 0.5456017255783081, Val Loss: 1.1814074516296387\n",
      "Step: 3620, Subject: sub4; Training Loss: 0.24594035744667053, Val Loss: 0.8349794149398804\n",
      "Step: 3620, Subject: sub5; Training Loss: 0.38192784786224365, Val Loss: 0.977976381778717\n",
      "Step: 3620, Subject: sub7; Training Loss: 0.34994572401046753, Val Loss: 1.094024896621704\n",
      "Step: 3630, Subject: sub1; Training Loss: 0.17398524284362793, Val Loss: 0.8857145309448242\n",
      "Step: 3630, Subject: sub2; Training Loss: 0.1427559107542038, Val Loss: 0.8761762380599976\n",
      "Step: 3630, Subject: sub3; Training Loss: 0.5023662447929382, Val Loss: 0.9597324132919312\n",
      "Step: 3630, Subject: sub4; Training Loss: 0.24527426064014435, Val Loss: 0.8492811918258667\n",
      "Step: 3630, Subject: sub5; Training Loss: 0.330117404460907, Val Loss: 0.769188642501831\n",
      "Step: 3630, Subject: sub7; Training Loss: 0.3414890766143799, Val Loss: 0.7764506936073303\n",
      "Step: 3640, Subject: sub1; Training Loss: 0.23738086223602295, Val Loss: 1.05361008644104\n",
      "Step: 3640, Subject: sub2; Training Loss: 0.16671983897686005, Val Loss: 0.9203222393989563\n",
      "Step: 3640, Subject: sub3; Training Loss: 0.5040075778961182, Val Loss: 0.8157347440719604\n",
      "Step: 3640, Subject: sub4; Training Loss: 0.3088902235031128, Val Loss: 0.8247711062431335\n",
      "Step: 3640, Subject: sub5; Training Loss: 0.3080253601074219, Val Loss: 1.0138710737228394\n",
      "Step: 3640, Subject: sub7; Training Loss: 0.29495927691459656, Val Loss: 0.8406867384910583\n",
      "Step: 3650, Subject: sub1; Training Loss: 0.1898563802242279, Val Loss: 0.9315115213394165\n",
      "Step: 3650, Subject: sub2; Training Loss: 0.1501670777797699, Val Loss: 0.7956799864768982\n",
      "Step: 3650, Subject: sub3; Training Loss: 0.5016903877258301, Val Loss: 0.7958431243896484\n",
      "Step: 3650, Subject: sub4; Training Loss: 0.20348291099071503, Val Loss: 0.9496407508850098\n",
      "Step: 3650, Subject: sub5; Training Loss: 0.3591623604297638, Val Loss: 0.8138235807418823\n",
      "Step: 3650, Subject: sub7; Training Loss: 0.2931356132030487, Val Loss: 1.0878691673278809\n",
      "Step: 3660, Subject: sub1; Training Loss: 0.17786094546318054, Val Loss: 0.7505967617034912\n",
      "Step: 3660, Subject: sub2; Training Loss: 0.12927301228046417, Val Loss: 1.0732965469360352\n",
      "Step: 3660, Subject: sub3; Training Loss: 0.4267459511756897, Val Loss: 0.8683072924613953\n",
      "Step: 3660, Subject: sub4; Training Loss: 0.29678457975387573, Val Loss: 1.0193960666656494\n",
      "Step: 3660, Subject: sub5; Training Loss: 0.3552645742893219, Val Loss: 0.659448504447937\n",
      "Step: 3660, Subject: sub7; Training Loss: 0.3207011818885803, Val Loss: 1.0369112491607666\n",
      "Step: 3670, Subject: sub1; Training Loss: 0.25818848609924316, Val Loss: 0.9677555561065674\n",
      "Step: 3670, Subject: sub2; Training Loss: 0.2056688368320465, Val Loss: 0.8887103199958801\n",
      "Step: 3670, Subject: sub3; Training Loss: 0.4078434109687805, Val Loss: 0.9879419803619385\n",
      "Step: 3670, Subject: sub4; Training Loss: 0.18279752135276794, Val Loss: 0.8663522601127625\n",
      "Step: 3670, Subject: sub5; Training Loss: 0.35238927602767944, Val Loss: 0.9817801713943481\n",
      "Step: 3670, Subject: sub7; Training Loss: 0.2886669635772705, Val Loss: 1.3179383277893066\n",
      "Step: 3680, Subject: sub1; Training Loss: 0.2736590504646301, Val Loss: 0.8819619417190552\n",
      "Step: 3680, Subject: sub2; Training Loss: 0.20886853337287903, Val Loss: 0.9574100375175476\n",
      "Step: 3680, Subject: sub3; Training Loss: 0.4064958095550537, Val Loss: 0.94742351770401\n",
      "Step: 3680, Subject: sub4; Training Loss: 0.3714628219604492, Val Loss: 0.7253667712211609\n",
      "Step: 3680, Subject: sub5; Training Loss: 0.446058988571167, Val Loss: 0.8274798393249512\n",
      "Step: 3680, Subject: sub7; Training Loss: 0.3219038248062134, Val Loss: 0.9097046852111816\n",
      "Step: 3690, Subject: sub1; Training Loss: 0.2928696274757385, Val Loss: 2.0414674282073975\n",
      "Step: 3690, Subject: sub2; Training Loss: 0.18833783268928528, Val Loss: 0.8162021636962891\n",
      "Step: 3690, Subject: sub3; Training Loss: 0.4334832429885864, Val Loss: 1.094658613204956\n",
      "Step: 3690, Subject: sub4; Training Loss: 0.2878429889678955, Val Loss: 1.0889065265655518\n",
      "Step: 3690, Subject: sub5; Training Loss: 0.34364330768585205, Val Loss: 1.1425766944885254\n",
      "Step: 3690, Subject: sub7; Training Loss: 0.3282504975795746, Val Loss: 1.216820478439331\n",
      "Step: 3700, Subject: sub1; Training Loss: 0.24806302785873413, Val Loss: 1.039060354232788\n",
      "Step: 3700, Subject: sub2; Training Loss: 0.15421552956104279, Val Loss: 0.9471054077148438\n",
      "Step: 3700, Subject: sub3; Training Loss: 0.5398697853088379, Val Loss: 1.0105398893356323\n",
      "Step: 3700, Subject: sub4; Training Loss: 0.3035905957221985, Val Loss: 0.9722880125045776\n",
      "Step: 3700, Subject: sub5; Training Loss: 0.3679235577583313, Val Loss: 0.7349888682365417\n",
      "Step: 3700, Subject: sub7; Training Loss: 0.33293235301971436, Val Loss: 0.7999194264411926\n",
      "Step: 3710, Subject: sub1; Training Loss: 0.17037604749202728, Val Loss: 0.7623715400695801\n",
      "Step: 3710, Subject: sub2; Training Loss: 0.1796014904975891, Val Loss: 0.8493396043777466\n",
      "Step: 3710, Subject: sub3; Training Loss: 0.46861734986305237, Val Loss: 1.187375783920288\n",
      "Step: 3710, Subject: sub4; Training Loss: 0.24413305521011353, Val Loss: 0.9249373078346252\n",
      "Step: 3710, Subject: sub5; Training Loss: 0.39837977290153503, Val Loss: 1.1829012632369995\n",
      "Step: 3710, Subject: sub7; Training Loss: 0.32398390769958496, Val Loss: 1.1626991033554077\n",
      "Step: 3720, Subject: sub1; Training Loss: 0.19412261247634888, Val Loss: 1.0832293033599854\n",
      "Step: 3720, Subject: sub2; Training Loss: 0.2364102452993393, Val Loss: 0.9036641120910645\n",
      "Step: 3720, Subject: sub3; Training Loss: 0.4412406086921692, Val Loss: 1.145027756690979\n",
      "Step: 3720, Subject: sub4; Training Loss: 0.22193345427513123, Val Loss: 1.0800957679748535\n",
      "Step: 3720, Subject: sub5; Training Loss: 0.4068352282047272, Val Loss: 0.8285496830940247\n",
      "Step: 3720, Subject: sub7; Training Loss: 0.37441006302833557, Val Loss: 1.072387933731079\n",
      "Step: 3730, Subject: sub1; Training Loss: 0.2276879847049713, Val Loss: 0.7506058812141418\n",
      "Step: 3730, Subject: sub2; Training Loss: 0.23310843110084534, Val Loss: 0.9359464049339294\n",
      "Step: 3730, Subject: sub3; Training Loss: 0.4756750166416168, Val Loss: 0.8354883193969727\n",
      "Step: 3730, Subject: sub4; Training Loss: 0.3070128858089447, Val Loss: 0.8789345026016235\n",
      "Step: 3730, Subject: sub5; Training Loss: 0.4685274064540863, Val Loss: 0.9961562156677246\n",
      "Step: 3730, Subject: sub7; Training Loss: 0.41233378648757935, Val Loss: 1.0704400539398193\n",
      "Step: 3740, Subject: sub1; Training Loss: 0.2448301464319229, Val Loss: 0.7561001181602478\n",
      "Step: 3740, Subject: sub2; Training Loss: 0.1718606948852539, Val Loss: 0.906073808670044\n",
      "Step: 3740, Subject: sub3; Training Loss: 0.4127267301082611, Val Loss: 0.7934664487838745\n",
      "Step: 3740, Subject: sub4; Training Loss: 0.2910694479942322, Val Loss: 0.767103374004364\n",
      "Step: 3740, Subject: sub5; Training Loss: 0.31451672315597534, Val Loss: 0.8476768136024475\n",
      "Step: 3740, Subject: sub7; Training Loss: 0.2606285810470581, Val Loss: 1.029712200164795\n",
      "Step: 3750, Subject: sub1; Training Loss: 0.2770140469074249, Val Loss: 1.4118320941925049\n",
      "Step: 3750, Subject: sub2; Training Loss: 0.21365812420845032, Val Loss: 0.9586933255195618\n",
      "Step: 3750, Subject: sub3; Training Loss: 0.3979710638523102, Val Loss: 1.1126506328582764\n",
      "Step: 3750, Subject: sub4; Training Loss: 0.2622784376144409, Val Loss: 0.9480853080749512\n",
      "Step: 3750, Subject: sub5; Training Loss: 0.29651761054992676, Val Loss: 0.9805381298065186\n",
      "Step: 3750, Subject: sub7; Training Loss: 0.39425355195999146, Val Loss: 1.1759063005447388\n",
      "Step: 3760, Subject: sub1; Training Loss: 0.214789479970932, Val Loss: 1.0415523052215576\n",
      "Step: 3760, Subject: sub2; Training Loss: 0.17562679946422577, Val Loss: 1.0601091384887695\n",
      "Step: 3760, Subject: sub3; Training Loss: 0.4434365928173065, Val Loss: 1.0509028434753418\n",
      "Step: 3760, Subject: sub4; Training Loss: 0.2168498933315277, Val Loss: 1.1770544052124023\n",
      "Step: 3760, Subject: sub5; Training Loss: 0.2857418954372406, Val Loss: 1.0466387271881104\n",
      "Step: 3760, Subject: sub7; Training Loss: 0.3096868395805359, Val Loss: 1.1484392881393433\n",
      "Step: 3770, Subject: sub1; Training Loss: 0.18680931627750397, Val Loss: 1.5397756099700928\n",
      "Step: 3770, Subject: sub2; Training Loss: 0.15142861008644104, Val Loss: 0.9589154124259949\n",
      "Step: 3770, Subject: sub3; Training Loss: 0.4431019425392151, Val Loss: 1.10649836063385\n",
      "Step: 3770, Subject: sub4; Training Loss: 0.2612997889518738, Val Loss: 1.0028672218322754\n",
      "Step: 3770, Subject: sub5; Training Loss: 0.32463714480400085, Val Loss: 0.6821538209915161\n",
      "Step: 3770, Subject: sub7; Training Loss: 0.33329078555107117, Val Loss: 1.1970338821411133\n",
      "Step: 3780, Subject: sub1; Training Loss: 0.20219382643699646, Val Loss: 1.8493156433105469\n",
      "Step: 3780, Subject: sub2; Training Loss: 0.16248078644275665, Val Loss: 0.8084775805473328\n",
      "Step: 3780, Subject: sub3; Training Loss: 0.4642091989517212, Val Loss: 1.2338588237762451\n",
      "Step: 3780, Subject: sub4; Training Loss: 0.2586967945098877, Val Loss: 1.117192029953003\n",
      "Step: 3780, Subject: sub5; Training Loss: 0.32979458570480347, Val Loss: 0.9164620041847229\n",
      "Step: 3780, Subject: sub7; Training Loss: 0.30469441413879395, Val Loss: 1.51310396194458\n",
      "Step: 3790, Subject: sub1; Training Loss: 0.1882532387971878, Val Loss: 1.1391513347625732\n",
      "Step: 3790, Subject: sub2; Training Loss: 0.21519061923027039, Val Loss: 0.8433229923248291\n",
      "Step: 3790, Subject: sub3; Training Loss: 0.4570572078227997, Val Loss: 1.2251173257827759\n",
      "Step: 3790, Subject: sub4; Training Loss: 0.3184223473072052, Val Loss: 0.7757116556167603\n",
      "Step: 3790, Subject: sub5; Training Loss: 0.3388781249523163, Val Loss: 1.1736031770706177\n",
      "Step: 3790, Subject: sub7; Training Loss: 0.2728528082370758, Val Loss: 1.5554105043411255\n",
      "Step: 3800, Subject: sub1; Training Loss: 0.22226232290267944, Val Loss: 0.6404035091400146\n",
      "Step: 3800, Subject: sub2; Training Loss: 0.12276327610015869, Val Loss: 0.8641908764839172\n",
      "Step: 3800, Subject: sub3; Training Loss: 0.4259200096130371, Val Loss: 0.8118220567703247\n",
      "Step: 3800, Subject: sub4; Training Loss: 0.20171087980270386, Val Loss: 0.9161754846572876\n",
      "Step: 3800, Subject: sub5; Training Loss: 0.26790767908096313, Val Loss: 0.7994412183761597\n",
      "Step: 3800, Subject: sub7; Training Loss: 0.38469409942626953, Val Loss: 1.0153629779815674\n",
      "Step: 3810, Subject: sub1; Training Loss: 0.17867521941661835, Val Loss: 0.8504858016967773\n",
      "Step: 3810, Subject: sub2; Training Loss: 0.13835519552230835, Val Loss: 0.8874786496162415\n",
      "Step: 3810, Subject: sub3; Training Loss: 0.40031367540359497, Val Loss: 0.8276833295822144\n",
      "Step: 3810, Subject: sub4; Training Loss: 0.18768972158432007, Val Loss: 0.9332548379898071\n",
      "Step: 3810, Subject: sub5; Training Loss: 0.3934505581855774, Val Loss: 0.8976832628250122\n",
      "Step: 3810, Subject: sub7; Training Loss: 0.2846412658691406, Val Loss: 0.9604403972625732\n",
      "Step: 3820, Subject: sub1; Training Loss: 0.16743656992912292, Val Loss: 0.44159945845603943\n",
      "Saving model with validation loss: 0.44159945845603943\n",
      "\n",
      "Step: 3820, Subject: sub2; Training Loss: 0.20610088109970093, Val Loss: 0.867016077041626\n",
      "Step: 3820, Subject: sub3; Training Loss: 0.4087735712528229, Val Loss: 0.77171790599823\n",
      "Step: 3820, Subject: sub4; Training Loss: 0.20829299092292786, Val Loss: 1.0050089359283447\n",
      "Step: 3820, Subject: sub5; Training Loss: 0.35069161653518677, Val Loss: 0.8607281446456909\n",
      "Step: 3820, Subject: sub7; Training Loss: 0.36028605699539185, Val Loss: 0.8919190764427185\n",
      "Step: 3830, Subject: sub1; Training Loss: 0.23867449164390564, Val Loss: 1.0040068626403809\n",
      "Step: 3830, Subject: sub2; Training Loss: 0.21155761182308197, Val Loss: 0.8074557781219482\n",
      "Step: 3830, Subject: sub3; Training Loss: 0.4433411955833435, Val Loss: 1.0992209911346436\n",
      "Step: 3830, Subject: sub4; Training Loss: 0.16688641905784607, Val Loss: 0.7873122692108154\n",
      "Step: 3830, Subject: sub5; Training Loss: 0.34801310300827026, Val Loss: 0.9780431389808655\n",
      "Step: 3830, Subject: sub7; Training Loss: 0.2724882960319519, Val Loss: 0.9845812320709229\n",
      "Step: 3840, Subject: sub1; Training Loss: 0.1973995566368103, Val Loss: 1.069008708000183\n",
      "Step: 3840, Subject: sub2; Training Loss: 0.21904829144477844, Val Loss: 0.8308873176574707\n",
      "Step: 3840, Subject: sub3; Training Loss: 0.5125697255134583, Val Loss: 1.2041889429092407\n",
      "Step: 3840, Subject: sub4; Training Loss: 0.2420923411846161, Val Loss: 0.9889459609985352\n",
      "Step: 3840, Subject: sub5; Training Loss: 0.3299543857574463, Val Loss: 1.15694260597229\n",
      "Step: 3840, Subject: sub7; Training Loss: 0.3011670708656311, Val Loss: 1.3973751068115234\n",
      "Step: 3850, Subject: sub1; Training Loss: 0.1938493400812149, Val Loss: 1.6880605220794678\n",
      "Step: 3850, Subject: sub2; Training Loss: 0.14740093052387238, Val Loss: 0.8863341808319092\n",
      "Step: 3850, Subject: sub3; Training Loss: 0.45576000213623047, Val Loss: 0.9621599316596985\n",
      "Step: 3850, Subject: sub4; Training Loss: 0.22579099237918854, Val Loss: 0.9887399673461914\n",
      "Step: 3850, Subject: sub5; Training Loss: 0.2917628288269043, Val Loss: 0.8922942876815796\n",
      "Step: 3850, Subject: sub7; Training Loss: 0.3141578137874603, Val Loss: 1.1335980892181396\n",
      "Step: 3860, Subject: sub1; Training Loss: 0.17470486462116241, Val Loss: 1.051806926727295\n",
      "Step: 3860, Subject: sub2; Training Loss: 0.1429096758365631, Val Loss: 0.8061331510543823\n",
      "Step: 3860, Subject: sub3; Training Loss: 0.4880814254283905, Val Loss: 1.0008823871612549\n",
      "Step: 3860, Subject: sub4; Training Loss: 0.2002985179424286, Val Loss: 1.066954255104065\n",
      "Step: 3860, Subject: sub5; Training Loss: 0.40237200260162354, Val Loss: 0.8346811532974243\n",
      "Step: 3860, Subject: sub7; Training Loss: 0.28343135118484497, Val Loss: 1.2359392642974854\n",
      "Step: 3870, Subject: sub1; Training Loss: 0.2019738256931305, Val Loss: 0.9895443916320801\n",
      "Step: 3870, Subject: sub2; Training Loss: 0.17851248383522034, Val Loss: 0.7997050285339355\n",
      "Step: 3870, Subject: sub3; Training Loss: 0.47536641359329224, Val Loss: 1.0543768405914307\n",
      "Step: 3870, Subject: sub4; Training Loss: 0.2866339087486267, Val Loss: 0.7300406694412231\n",
      "Step: 3870, Subject: sub5; Training Loss: 0.3173885941505432, Val Loss: 0.6438721418380737\n",
      "Step: 3870, Subject: sub7; Training Loss: 0.2960532307624817, Val Loss: 1.1908986568450928\n",
      "Step: 3880, Subject: sub1; Training Loss: 0.17262589931488037, Val Loss: 1.2792150974273682\n",
      "Step: 3880, Subject: sub2; Training Loss: 0.17427043616771698, Val Loss: 0.8172103762626648\n",
      "Step: 3880, Subject: sub3; Training Loss: 0.4438549280166626, Val Loss: 1.065146565437317\n",
      "Step: 3880, Subject: sub4; Training Loss: 0.3451864719390869, Val Loss: 0.9589851498603821\n",
      "Step: 3880, Subject: sub5; Training Loss: 0.3788224756717682, Val Loss: 0.9449587464332581\n",
      "Step: 3880, Subject: sub7; Training Loss: 0.3169935643672943, Val Loss: 1.1086623668670654\n",
      "Step: 3890, Subject: sub1; Training Loss: 0.16376638412475586, Val Loss: 0.9679408073425293\n",
      "Step: 3890, Subject: sub2; Training Loss: 0.16679206490516663, Val Loss: 1.0349433422088623\n",
      "Step: 3890, Subject: sub3; Training Loss: 0.40097200870513916, Val Loss: 0.9300430417060852\n",
      "Step: 3890, Subject: sub4; Training Loss: 0.2951582670211792, Val Loss: 0.944111704826355\n",
      "Step: 3890, Subject: sub5; Training Loss: 0.33534225821495056, Val Loss: 1.0671324729919434\n",
      "Step: 3890, Subject: sub7; Training Loss: 0.24842481315135956, Val Loss: 1.271522045135498\n",
      "Step: 3900, Subject: sub1; Training Loss: 0.20991277694702148, Val Loss: 0.7866125702857971\n",
      "Step: 3900, Subject: sub2; Training Loss: 0.1653435379266739, Val Loss: 1.0240309238433838\n",
      "Step: 3900, Subject: sub3; Training Loss: 0.4269116520881653, Val Loss: 0.8666163682937622\n",
      "Step: 3900, Subject: sub4; Training Loss: 0.19144730269908905, Val Loss: 0.9503100514411926\n",
      "Step: 3900, Subject: sub5; Training Loss: 0.3007083833217621, Val Loss: 0.7362698912620544\n",
      "Step: 3900, Subject: sub7; Training Loss: 0.32708096504211426, Val Loss: 1.4247788190841675\n",
      "Step: 3910, Subject: sub1; Training Loss: 0.17010870575904846, Val Loss: 1.6209138631820679\n",
      "Step: 3910, Subject: sub2; Training Loss: 0.2627076208591461, Val Loss: 0.8721814155578613\n",
      "Step: 3910, Subject: sub3; Training Loss: 0.4486037492752075, Val Loss: 1.1251318454742432\n",
      "Step: 3910, Subject: sub4; Training Loss: 0.27568063139915466, Val Loss: 1.0750417709350586\n",
      "Step: 3910, Subject: sub5; Training Loss: 0.32448846101760864, Val Loss: 0.8811138868331909\n",
      "Step: 3910, Subject: sub7; Training Loss: 0.36492711305618286, Val Loss: 1.0967851877212524\n",
      "Step: 3920, Subject: sub1; Training Loss: 0.20407536625862122, Val Loss: 1.3016570806503296\n",
      "Step: 3920, Subject: sub2; Training Loss: 0.15921734273433685, Val Loss: 0.8547850251197815\n",
      "Step: 3920, Subject: sub3; Training Loss: 0.3847668766975403, Val Loss: 1.1150943040847778\n",
      "Step: 3920, Subject: sub4; Training Loss: 0.196809783577919, Val Loss: 1.1847968101501465\n",
      "Step: 3920, Subject: sub5; Training Loss: 0.34246063232421875, Val Loss: 0.9344334006309509\n",
      "Step: 3920, Subject: sub7; Training Loss: 0.26729267835617065, Val Loss: 1.060141682624817\n",
      "Step: 3930, Subject: sub1; Training Loss: 0.192739337682724, Val Loss: 0.7657711505889893\n",
      "Step: 3930, Subject: sub2; Training Loss: 0.13443508744239807, Val Loss: 0.8553261756896973\n",
      "Step: 3930, Subject: sub3; Training Loss: 0.42883536219596863, Val Loss: 0.9772640466690063\n",
      "Step: 3930, Subject: sub4; Training Loss: 0.2677205502986908, Val Loss: 0.9417275786399841\n",
      "Step: 3930, Subject: sub5; Training Loss: 0.35377126932144165, Val Loss: 1.3121623992919922\n",
      "Step: 3930, Subject: sub7; Training Loss: 0.39948371052742004, Val Loss: 1.2370433807373047\n",
      "Step: 3940, Subject: sub1; Training Loss: 0.19407059252262115, Val Loss: 1.172151803970337\n",
      "Step: 3940, Subject: sub2; Training Loss: 0.16333967447280884, Val Loss: 0.8138248920440674\n",
      "Step: 3940, Subject: sub3; Training Loss: 0.4198644161224365, Val Loss: 0.7796438932418823\n",
      "Step: 3940, Subject: sub4; Training Loss: 0.27976295351982117, Val Loss: 0.9271724820137024\n",
      "Step: 3940, Subject: sub5; Training Loss: 0.2937759757041931, Val Loss: 1.0190311670303345\n",
      "Step: 3940, Subject: sub7; Training Loss: 0.4056565761566162, Val Loss: 1.0283708572387695\n",
      "Step: 3950, Subject: sub1; Training Loss: 0.1909940242767334, Val Loss: 0.7744274139404297\n",
      "Step: 3950, Subject: sub2; Training Loss: 0.14699894189834595, Val Loss: 0.7576637268066406\n",
      "Step: 3950, Subject: sub3; Training Loss: 0.44725435972213745, Val Loss: 0.7986652851104736\n",
      "Step: 3950, Subject: sub4; Training Loss: 0.26139068603515625, Val Loss: 0.8789456486701965\n",
      "Step: 3950, Subject: sub5; Training Loss: 0.3377506136894226, Val Loss: 0.8399069309234619\n",
      "Step: 3950, Subject: sub7; Training Loss: 0.3028440475463867, Val Loss: 1.093146562576294\n",
      "Step: 3960, Subject: sub1; Training Loss: 0.14106184244155884, Val Loss: 1.382288932800293\n",
      "Step: 3960, Subject: sub2; Training Loss: 0.14168769121170044, Val Loss: 1.0377403497695923\n",
      "Step: 3960, Subject: sub3; Training Loss: 0.4823263883590698, Val Loss: 0.9875239133834839\n",
      "Step: 3960, Subject: sub4; Training Loss: 0.17424854636192322, Val Loss: 0.8320375680923462\n",
      "Step: 3960, Subject: sub5; Training Loss: 0.3057366609573364, Val Loss: 0.9071941375732422\n",
      "Step: 3960, Subject: sub7; Training Loss: 0.3307381272315979, Val Loss: 1.1426645517349243\n",
      "Step: 3970, Subject: sub1; Training Loss: 0.20588085055351257, Val Loss: 1.1163585186004639\n",
      "Step: 3970, Subject: sub2; Training Loss: 0.16831710934638977, Val Loss: 0.8266037702560425\n",
      "Step: 3970, Subject: sub3; Training Loss: 0.36167722940444946, Val Loss: 0.7982022762298584\n",
      "Step: 3970, Subject: sub4; Training Loss: 0.28984349966049194, Val Loss: 0.9307008981704712\n",
      "Step: 3970, Subject: sub5; Training Loss: 0.26359492540359497, Val Loss: 1.0198032855987549\n",
      "Step: 3970, Subject: sub7; Training Loss: 0.26464027166366577, Val Loss: 1.3482584953308105\n",
      "Step: 3980, Subject: sub1; Training Loss: 0.15118519961833954, Val Loss: 0.7894846200942993\n",
      "Step: 3980, Subject: sub2; Training Loss: 0.20834819972515106, Val Loss: 0.9095692038536072\n",
      "Step: 3980, Subject: sub3; Training Loss: 0.4073825478553772, Val Loss: 0.9462699294090271\n",
      "Step: 3980, Subject: sub4; Training Loss: 0.2813113033771515, Val Loss: 1.0427422523498535\n",
      "Step: 3980, Subject: sub5; Training Loss: 0.3151438534259796, Val Loss: 0.9046010971069336\n",
      "Step: 3980, Subject: sub7; Training Loss: 0.24595963954925537, Val Loss: 1.2055487632751465\n",
      "Step: 3990, Subject: sub1; Training Loss: 0.16716040670871735, Val Loss: 1.2090051174163818\n",
      "Step: 3990, Subject: sub2; Training Loss: 0.13885179162025452, Val Loss: 1.1114633083343506\n",
      "Step: 3990, Subject: sub3; Training Loss: 0.41433608531951904, Val Loss: 1.5087862014770508\n",
      "Step: 3990, Subject: sub4; Training Loss: 0.3534810543060303, Val Loss: 1.0692976713180542\n",
      "Step: 3990, Subject: sub5; Training Loss: 0.3264603614807129, Val Loss: 1.1118561029434204\n",
      "Step: 3990, Subject: sub7; Training Loss: 0.32354503870010376, Val Loss: 1.1183346509933472\n",
      "Step: 4000, Subject: sub1; Training Loss: 0.18814069032669067, Val Loss: 1.3688689470291138\n",
      "Step: 4000, Subject: sub2; Training Loss: 0.17136085033416748, Val Loss: 1.0217100381851196\n",
      "Step: 4000, Subject: sub3; Training Loss: 0.4380096197128296, Val Loss: 1.1092047691345215\n",
      "Step: 4000, Subject: sub4; Training Loss: 0.1755812019109726, Val Loss: 0.9609078168869019\n",
      "Step: 4000, Subject: sub5; Training Loss: 0.30865973234176636, Val Loss: 0.9665193557739258\n",
      "Step: 4000, Subject: sub7; Training Loss: 0.31942254304885864, Val Loss: 1.3738508224487305\n",
      "Step: 4010, Subject: sub1; Training Loss: 0.26671847701072693, Val Loss: 1.0217182636260986\n",
      "Step: 4010, Subject: sub2; Training Loss: 0.2033395618200302, Val Loss: 0.9032341837882996\n",
      "Step: 4010, Subject: sub3; Training Loss: 0.4381692409515381, Val Loss: 0.7937784194946289\n",
      "Step: 4010, Subject: sub4; Training Loss: 0.24829110503196716, Val Loss: 1.1058378219604492\n",
      "Step: 4010, Subject: sub5; Training Loss: 0.421048104763031, Val Loss: 0.9007208943367004\n",
      "Step: 4010, Subject: sub7; Training Loss: 0.32151052355766296, Val Loss: 1.2135276794433594\n",
      "Step: 4020, Subject: sub1; Training Loss: 0.140362948179245, Val Loss: 1.1261670589447021\n",
      "Step: 4020, Subject: sub2; Training Loss: 0.1703163981437683, Val Loss: 0.9448683261871338\n",
      "Step: 4020, Subject: sub3; Training Loss: 0.45166751742362976, Val Loss: 0.7807055115699768\n",
      "Step: 4020, Subject: sub4; Training Loss: 0.2527787685394287, Val Loss: 1.0407955646514893\n",
      "Step: 4020, Subject: sub5; Training Loss: 0.2638118863105774, Val Loss: 0.9109921455383301\n",
      "Step: 4020, Subject: sub7; Training Loss: 0.27724629640579224, Val Loss: 1.0046288967132568\n",
      "Step: 4030, Subject: sub1; Training Loss: 0.17765754461288452, Val Loss: 1.2984185218811035\n",
      "Step: 4030, Subject: sub2; Training Loss: 0.15786972641944885, Val Loss: 0.9067009687423706\n",
      "Step: 4030, Subject: sub3; Training Loss: 0.3760561943054199, Val Loss: 0.8665903806686401\n",
      "Step: 4030, Subject: sub4; Training Loss: 0.24420952796936035, Val Loss: 1.2186782360076904\n",
      "Step: 4030, Subject: sub5; Training Loss: 0.39606648683547974, Val Loss: 0.8117218613624573\n",
      "Step: 4030, Subject: sub7; Training Loss: 0.35099124908447266, Val Loss: 0.9407753944396973\n",
      "Step: 4040, Subject: sub1; Training Loss: 0.22793090343475342, Val Loss: 0.9433176517486572\n",
      "Step: 4040, Subject: sub2; Training Loss: 0.1856788992881775, Val Loss: 0.7069012522697449\n",
      "Step: 4040, Subject: sub3; Training Loss: 0.4530142843723297, Val Loss: 0.8797346353530884\n",
      "Step: 4040, Subject: sub4; Training Loss: 0.22379595041275024, Val Loss: 0.7635982036590576\n",
      "Step: 4040, Subject: sub5; Training Loss: 0.28369051218032837, Val Loss: 0.6935364007949829\n",
      "Step: 4040, Subject: sub7; Training Loss: 0.3440445065498352, Val Loss: 0.8607724905014038\n",
      "Step: 4050, Subject: sub1; Training Loss: 0.21520225703716278, Val Loss: 1.0405793190002441\n",
      "Step: 4050, Subject: sub2; Training Loss: 0.24019677937030792, Val Loss: 0.8507111072540283\n",
      "Step: 4050, Subject: sub3; Training Loss: 0.4177394509315491, Val Loss: 0.722033679485321\n",
      "Step: 4050, Subject: sub4; Training Loss: 0.18827801942825317, Val Loss: 0.945361316204071\n",
      "Step: 4050, Subject: sub5; Training Loss: 0.3388653099536896, Val Loss: 0.7694211006164551\n",
      "Step: 4050, Subject: sub7; Training Loss: 0.30882328748703003, Val Loss: 1.0630793571472168\n",
      "Step: 4060, Subject: sub1; Training Loss: 0.2169570028781891, Val Loss: 0.6518535614013672\n",
      "Step: 4060, Subject: sub2; Training Loss: 0.13486824929714203, Val Loss: 0.9077078700065613\n",
      "Step: 4060, Subject: sub3; Training Loss: 0.401645302772522, Val Loss: 1.1288421154022217\n",
      "Step: 4060, Subject: sub4; Training Loss: 0.19939854741096497, Val Loss: 1.0265302658081055\n",
      "Step: 4060, Subject: sub5; Training Loss: 0.28277885913848877, Val Loss: 1.0204899311065674\n",
      "Step: 4060, Subject: sub7; Training Loss: 0.2648574113845825, Val Loss: 1.1347084045410156\n",
      "Step: 4070, Subject: sub1; Training Loss: 0.2478526383638382, Val Loss: 1.644644021987915\n",
      "Step: 4070, Subject: sub2; Training Loss: 0.17278403043746948, Val Loss: 0.8765143156051636\n",
      "Step: 4070, Subject: sub3; Training Loss: 0.5008150339126587, Val Loss: 0.9934211373329163\n",
      "Step: 4070, Subject: sub4; Training Loss: 0.19670994579792023, Val Loss: 0.962200939655304\n",
      "Step: 4070, Subject: sub5; Training Loss: 0.33839142322540283, Val Loss: 1.1840927600860596\n",
      "Step: 4070, Subject: sub7; Training Loss: 0.30153411626815796, Val Loss: 1.2516546249389648\n",
      "Step: 4080, Subject: sub1; Training Loss: 0.1579543650150299, Val Loss: 0.8270741701126099\n",
      "Step: 4080, Subject: sub2; Training Loss: 0.16190382838249207, Val Loss: 0.973426878452301\n",
      "Step: 4080, Subject: sub3; Training Loss: 0.5511264801025391, Val Loss: 0.8302056789398193\n",
      "Step: 4080, Subject: sub4; Training Loss: 0.265058696269989, Val Loss: 0.7914480566978455\n",
      "Step: 4080, Subject: sub5; Training Loss: 0.310697078704834, Val Loss: 0.8583047389984131\n",
      "Step: 4080, Subject: sub7; Training Loss: 0.2590011954307556, Val Loss: 1.135276198387146\n",
      "Step: 4090, Subject: sub1; Training Loss: 0.18938225507736206, Val Loss: 1.0155715942382812\n",
      "Step: 4090, Subject: sub2; Training Loss: 0.17296171188354492, Val Loss: 0.7303816080093384\n",
      "Step: 4090, Subject: sub3; Training Loss: 0.4461666941642761, Val Loss: 1.3701963424682617\n",
      "Step: 4090, Subject: sub4; Training Loss: 0.20398399233818054, Val Loss: 0.8583313226699829\n",
      "Step: 4090, Subject: sub5; Training Loss: 0.2603382468223572, Val Loss: 1.2207541465759277\n",
      "Step: 4090, Subject: sub7; Training Loss: 0.30585458874702454, Val Loss: 1.2852786779403687\n",
      "Step: 4100, Subject: sub1; Training Loss: 0.18981821835041046, Val Loss: 1.8445650339126587\n",
      "Step: 4100, Subject: sub2; Training Loss: 0.2158946692943573, Val Loss: 1.0427026748657227\n",
      "Step: 4100, Subject: sub3; Training Loss: 0.4280765950679779, Val Loss: 1.3451225757598877\n",
      "Step: 4100, Subject: sub4; Training Loss: 0.24882003664970398, Val Loss: 1.0230244398117065\n",
      "Step: 4100, Subject: sub5; Training Loss: 0.3024209141731262, Val Loss: 1.2449824810028076\n",
      "Step: 4100, Subject: sub7; Training Loss: 0.31383031606674194, Val Loss: 1.3228198289871216\n",
      "Step: 4110, Subject: sub1; Training Loss: 0.15932881832122803, Val Loss: 0.7206923961639404\n",
      "Step: 4110, Subject: sub2; Training Loss: 0.1994832307100296, Val Loss: 0.8324936628341675\n",
      "Step: 4110, Subject: sub3; Training Loss: 0.4672434329986572, Val Loss: 0.8797315955162048\n",
      "Step: 4110, Subject: sub4; Training Loss: 0.26898637413978577, Val Loss: 1.0269641876220703\n",
      "Step: 4110, Subject: sub5; Training Loss: 0.2870903015136719, Val Loss: 0.6564137935638428\n",
      "Step: 4110, Subject: sub7; Training Loss: 0.3292674124240875, Val Loss: 0.8267112970352173\n",
      "Step: 4120, Subject: sub1; Training Loss: 0.17602986097335815, Val Loss: 1.1965479850769043\n",
      "Step: 4120, Subject: sub2; Training Loss: 0.1605188101530075, Val Loss: 0.9233007431030273\n",
      "Step: 4120, Subject: sub3; Training Loss: 0.4142765402793884, Val Loss: 0.901881754398346\n",
      "Step: 4120, Subject: sub4; Training Loss: 0.18025869131088257, Val Loss: 1.0094654560089111\n",
      "Step: 4120, Subject: sub5; Training Loss: 0.36631494760513306, Val Loss: 1.0322473049163818\n",
      "Step: 4120, Subject: sub7; Training Loss: 0.30327367782592773, Val Loss: 1.1731973886489868\n",
      "Step: 4130, Subject: sub1; Training Loss: 0.1552285999059677, Val Loss: 0.9056291580200195\n",
      "Step: 4130, Subject: sub2; Training Loss: 0.13600632548332214, Val Loss: 0.8822486400604248\n",
      "Step: 4130, Subject: sub3; Training Loss: 0.41366273164749146, Val Loss: 0.8117305040359497\n",
      "Step: 4130, Subject: sub4; Training Loss: 0.3634757101535797, Val Loss: 1.113421082496643\n",
      "Step: 4130, Subject: sub5; Training Loss: 0.357064425945282, Val Loss: 0.6671684980392456\n",
      "Step: 4130, Subject: sub7; Training Loss: 0.3267384171485901, Val Loss: 0.9047274589538574\n",
      "Step: 4140, Subject: sub1; Training Loss: 0.14719104766845703, Val Loss: 1.3963301181793213\n",
      "Step: 4140, Subject: sub2; Training Loss: 0.13810347020626068, Val Loss: 0.9664629697799683\n",
      "Step: 4140, Subject: sub3; Training Loss: 0.44658875465393066, Val Loss: 1.1144765615463257\n",
      "Step: 4140, Subject: sub4; Training Loss: 0.2861303687095642, Val Loss: 0.8093597888946533\n",
      "Step: 4140, Subject: sub5; Training Loss: 0.26971521973609924, Val Loss: 0.7098551988601685\n",
      "Step: 4140, Subject: sub7; Training Loss: 0.3072853684425354, Val Loss: 0.9654970765113831\n",
      "Step: 4150, Subject: sub1; Training Loss: 0.15540413558483124, Val Loss: 1.359201192855835\n",
      "Step: 4150, Subject: sub2; Training Loss: 0.1574779748916626, Val Loss: 0.8992804288864136\n",
      "Step: 4150, Subject: sub3; Training Loss: 0.3860537111759186, Val Loss: 1.0913517475128174\n",
      "Step: 4150, Subject: sub4; Training Loss: 0.26802316308021545, Val Loss: 0.878819465637207\n",
      "Step: 4150, Subject: sub5; Training Loss: 0.3289508819580078, Val Loss: 1.096555233001709\n",
      "Step: 4150, Subject: sub7; Training Loss: 0.3141478896141052, Val Loss: 1.3213796615600586\n",
      "Step: 4160, Subject: sub1; Training Loss: 0.17617103457450867, Val Loss: 0.9203503131866455\n",
      "Step: 4160, Subject: sub2; Training Loss: 0.188568115234375, Val Loss: 0.8506019115447998\n",
      "Step: 4160, Subject: sub3; Training Loss: 0.4839310944080353, Val Loss: 0.8307855129241943\n",
      "Step: 4160, Subject: sub4; Training Loss: 0.21478471159934998, Val Loss: 0.9568378925323486\n",
      "Step: 4160, Subject: sub5; Training Loss: 0.3646569848060608, Val Loss: 0.8277727961540222\n",
      "Step: 4160, Subject: sub7; Training Loss: 0.24188894033432007, Val Loss: 1.0188589096069336\n",
      "Step: 4170, Subject: sub1; Training Loss: 0.2308797687292099, Val Loss: 0.9783803224563599\n",
      "Step: 4170, Subject: sub2; Training Loss: 0.176900252699852, Val Loss: 0.8329731225967407\n",
      "Step: 4170, Subject: sub3; Training Loss: 0.3596899211406708, Val Loss: 0.9007411003112793\n",
      "Step: 4170, Subject: sub4; Training Loss: 0.13535183668136597, Val Loss: 1.1558514833450317\n",
      "Step: 4170, Subject: sub5; Training Loss: 0.3169550895690918, Val Loss: 0.5698005557060242\n",
      "Step: 4170, Subject: sub7; Training Loss: 0.31026679277420044, Val Loss: 0.8178268671035767\n",
      "Step: 4180, Subject: sub1; Training Loss: 0.14281928539276123, Val Loss: 1.0325043201446533\n",
      "Step: 4180, Subject: sub2; Training Loss: 0.15874812006950378, Val Loss: 0.964225172996521\n",
      "Step: 4180, Subject: sub3; Training Loss: 0.3696751594543457, Val Loss: 0.8788633942604065\n",
      "Step: 4180, Subject: sub4; Training Loss: 0.21915900707244873, Val Loss: 1.0566140413284302\n",
      "Step: 4180, Subject: sub5; Training Loss: 0.24476051330566406, Val Loss: 0.9801616668701172\n",
      "Step: 4180, Subject: sub7; Training Loss: 0.29327675700187683, Val Loss: 1.0022203922271729\n",
      "Step: 4190, Subject: sub1; Training Loss: 0.1806710958480835, Val Loss: 1.1340949535369873\n",
      "Step: 4190, Subject: sub2; Training Loss: 0.14651376008987427, Val Loss: 0.8689405918121338\n",
      "Step: 4190, Subject: sub3; Training Loss: 0.42111730575561523, Val Loss: 0.7913385033607483\n",
      "Step: 4190, Subject: sub4; Training Loss: 0.22351062297821045, Val Loss: 1.09125554561615\n",
      "Step: 4190, Subject: sub5; Training Loss: 0.305141806602478, Val Loss: 0.6947156190872192\n",
      "Step: 4190, Subject: sub7; Training Loss: 0.30059337615966797, Val Loss: 0.8177876472473145\n",
      "Step: 4200, Subject: sub1; Training Loss: 0.1940406858921051, Val Loss: 0.9363440275192261\n",
      "Step: 4200, Subject: sub2; Training Loss: 0.15749892592430115, Val Loss: 0.9628420472145081\n",
      "Step: 4200, Subject: sub3; Training Loss: 0.4261178970336914, Val Loss: 1.0471879243850708\n",
      "Step: 4200, Subject: sub4; Training Loss: 0.19718429446220398, Val Loss: 1.1686313152313232\n",
      "Step: 4200, Subject: sub5; Training Loss: 0.2834147810935974, Val Loss: 1.040177822113037\n",
      "Step: 4200, Subject: sub7; Training Loss: 0.2992500066757202, Val Loss: 1.7177300453186035\n",
      "Step: 4210, Subject: sub1; Training Loss: 0.17462080717086792, Val Loss: 0.4387674331665039\n",
      "Saving model with validation loss: 0.4387674331665039\n",
      "\n",
      "Step: 4210, Subject: sub2; Training Loss: 0.13796687126159668, Val Loss: 0.865043044090271\n",
      "Step: 4210, Subject: sub3; Training Loss: 0.4930165410041809, Val Loss: 0.7094829082489014\n",
      "Step: 4210, Subject: sub4; Training Loss: 0.2768615782260895, Val Loss: 1.0270705223083496\n",
      "Step: 4210, Subject: sub5; Training Loss: 0.2551841735839844, Val Loss: 0.769894003868103\n",
      "Step: 4210, Subject: sub7; Training Loss: 0.2800065279006958, Val Loss: 1.1908386945724487\n",
      "Step: 4220, Subject: sub1; Training Loss: 0.23700548708438873, Val Loss: 1.0970425605773926\n",
      "Step: 4220, Subject: sub2; Training Loss: 0.18249325454235077, Val Loss: 0.879782497882843\n",
      "Step: 4220, Subject: sub3; Training Loss: 0.4476095736026764, Val Loss: 0.9538125395774841\n",
      "Step: 4220, Subject: sub4; Training Loss: 0.2446606159210205, Val Loss: 1.0967378616333008\n",
      "Step: 4220, Subject: sub5; Training Loss: 0.2899376153945923, Val Loss: 0.7870206832885742\n",
      "Step: 4220, Subject: sub7; Training Loss: 0.3130835294723511, Val Loss: 0.9227292537689209\n",
      "Step: 4230, Subject: sub1; Training Loss: 0.17816267907619476, Val Loss: 0.7981575131416321\n",
      "Step: 4230, Subject: sub2; Training Loss: 0.1826348602771759, Val Loss: 0.8793835639953613\n",
      "Step: 4230, Subject: sub3; Training Loss: 0.3896053433418274, Val Loss: 0.9553020000457764\n",
      "Step: 4230, Subject: sub4; Training Loss: 0.28587856888771057, Val Loss: 0.789525032043457\n",
      "Step: 4230, Subject: sub5; Training Loss: 0.31563714146614075, Val Loss: 1.1341341733932495\n",
      "Step: 4230, Subject: sub7; Training Loss: 0.3045199513435364, Val Loss: 1.2475723028182983\n",
      "Step: 4240, Subject: sub1; Training Loss: 0.16886594891548157, Val Loss: 0.9860326051712036\n",
      "Step: 4240, Subject: sub2; Training Loss: 0.18125289678573608, Val Loss: 0.8149875402450562\n",
      "Step: 4240, Subject: sub3; Training Loss: 0.3845643997192383, Val Loss: 1.0443003177642822\n",
      "Step: 4240, Subject: sub4; Training Loss: 0.19995972514152527, Val Loss: 1.0859787464141846\n",
      "Step: 4240, Subject: sub5; Training Loss: 0.3758689761161804, Val Loss: 1.049513816833496\n",
      "Step: 4240, Subject: sub7; Training Loss: 0.275782972574234, Val Loss: 1.1107255220413208\n",
      "Step: 4250, Subject: sub1; Training Loss: 0.16374915838241577, Val Loss: 1.3708505630493164\n",
      "Step: 4250, Subject: sub2; Training Loss: 0.24312981963157654, Val Loss: 0.7488307952880859\n",
      "Step: 4250, Subject: sub3; Training Loss: 0.4007461667060852, Val Loss: 0.7136087417602539\n",
      "Step: 4250, Subject: sub4; Training Loss: 0.2632941007614136, Val Loss: 0.9255892038345337\n",
      "Step: 4250, Subject: sub5; Training Loss: 0.2550441324710846, Val Loss: 0.9081132411956787\n",
      "Step: 4250, Subject: sub7; Training Loss: 0.32428139448165894, Val Loss: 1.2356047630310059\n",
      "Step: 4260, Subject: sub1; Training Loss: 0.21467530727386475, Val Loss: 1.076807975769043\n",
      "Step: 4260, Subject: sub2; Training Loss: 0.19341088831424713, Val Loss: 0.7512044906616211\n",
      "Step: 4260, Subject: sub3; Training Loss: 0.3877849578857422, Val Loss: 0.8729277849197388\n",
      "Step: 4260, Subject: sub4; Training Loss: 0.265001118183136, Val Loss: 0.8968162536621094\n",
      "Step: 4260, Subject: sub5; Training Loss: 0.3413277268409729, Val Loss: 1.0050697326660156\n",
      "Step: 4260, Subject: sub7; Training Loss: 0.3049475848674774, Val Loss: 0.8952531814575195\n",
      "Step: 4270, Subject: sub1; Training Loss: 0.195701003074646, Val Loss: 1.0182108879089355\n",
      "Step: 4270, Subject: sub2; Training Loss: 0.11513936519622803, Val Loss: 0.8723212480545044\n",
      "Step: 4270, Subject: sub3; Training Loss: 0.3059012293815613, Val Loss: 0.9697448015213013\n",
      "Step: 4270, Subject: sub4; Training Loss: 0.3022223711013794, Val Loss: 1.064359426498413\n",
      "Step: 4270, Subject: sub5; Training Loss: 0.361067533493042, Val Loss: 0.6144707202911377\n",
      "Step: 4270, Subject: sub7; Training Loss: 0.22663868963718414, Val Loss: 0.9410431981086731\n",
      "Step: 4280, Subject: sub1; Training Loss: 0.14769905805587769, Val Loss: 0.5515831112861633\n",
      "Step: 4280, Subject: sub2; Training Loss: 0.19386166334152222, Val Loss: 0.7711470127105713\n",
      "Step: 4280, Subject: sub3; Training Loss: 0.41873329877853394, Val Loss: 0.9099540114402771\n",
      "Step: 4280, Subject: sub4; Training Loss: 0.14861243963241577, Val Loss: 0.892932653427124\n",
      "Step: 4280, Subject: sub5; Training Loss: 0.3904390037059784, Val Loss: 0.6458985805511475\n",
      "Step: 4280, Subject: sub7; Training Loss: 0.280201256275177, Val Loss: 1.137390375137329\n",
      "Step: 4290, Subject: sub1; Training Loss: 0.15110190212726593, Val Loss: 0.8484026193618774\n",
      "Step: 4290, Subject: sub2; Training Loss: 0.14942723512649536, Val Loss: 0.8072819709777832\n",
      "Step: 4290, Subject: sub3; Training Loss: 0.4067910611629486, Val Loss: 1.4202041625976562\n",
      "Step: 4290, Subject: sub4; Training Loss: 0.2546352744102478, Val Loss: 0.9218037128448486\n",
      "Step: 4290, Subject: sub5; Training Loss: 0.35874611139297485, Val Loss: 0.8994693756103516\n",
      "Step: 4290, Subject: sub7; Training Loss: 0.30348557233810425, Val Loss: 1.0364733934402466\n",
      "Step: 4300, Subject: sub1; Training Loss: 0.13921935856342316, Val Loss: 1.0659111738204956\n",
      "Step: 4300, Subject: sub2; Training Loss: 0.13186168670654297, Val Loss: 0.7819088697433472\n",
      "Step: 4300, Subject: sub3; Training Loss: 0.36624857783317566, Val Loss: 0.8674681782722473\n",
      "Step: 4300, Subject: sub4; Training Loss: 0.1552133411169052, Val Loss: 0.9345126748085022\n",
      "Step: 4300, Subject: sub5; Training Loss: 0.3650236129760742, Val Loss: 0.8254282474517822\n",
      "Step: 4300, Subject: sub7; Training Loss: 0.3002798855304718, Val Loss: 1.083247184753418\n",
      "Step: 4310, Subject: sub1; Training Loss: 0.14880432188510895, Val Loss: 1.0201749801635742\n",
      "Step: 4310, Subject: sub2; Training Loss: 0.17098644375801086, Val Loss: 0.8860021233558655\n",
      "Step: 4310, Subject: sub3; Training Loss: 0.4173370599746704, Val Loss: 0.89790940284729\n",
      "Step: 4310, Subject: sub4; Training Loss: 0.20388516783714294, Val Loss: 0.7929598689079285\n",
      "Step: 4310, Subject: sub5; Training Loss: 0.24419811367988586, Val Loss: 1.0121439695358276\n",
      "Step: 4310, Subject: sub7; Training Loss: 0.2369362860918045, Val Loss: 1.2979737520217896\n",
      "Step: 4320, Subject: sub1; Training Loss: 0.14404860138893127, Val Loss: 0.9895486831665039\n",
      "Step: 4320, Subject: sub2; Training Loss: 0.12458531558513641, Val Loss: 0.8341697454452515\n",
      "Step: 4320, Subject: sub3; Training Loss: 0.46887511014938354, Val Loss: 1.2748643159866333\n",
      "Step: 4320, Subject: sub4; Training Loss: 0.2798256278038025, Val Loss: 0.8240121006965637\n",
      "Step: 4320, Subject: sub5; Training Loss: 0.29837295413017273, Val Loss: 0.8772428631782532\n",
      "Step: 4320, Subject: sub7; Training Loss: 0.2812589406967163, Val Loss: 1.1572964191436768\n",
      "Step: 4330, Subject: sub1; Training Loss: 0.24677366018295288, Val Loss: 1.1179707050323486\n",
      "Step: 4330, Subject: sub2; Training Loss: 0.12231887876987457, Val Loss: 0.8331728577613831\n",
      "Step: 4330, Subject: sub3; Training Loss: 0.42035743594169617, Val Loss: 0.8466992974281311\n",
      "Step: 4330, Subject: sub4; Training Loss: 0.22542184591293335, Val Loss: 0.9215242862701416\n",
      "Step: 4330, Subject: sub5; Training Loss: 0.2507933974266052, Val Loss: 0.8218085765838623\n",
      "Step: 4330, Subject: sub7; Training Loss: 0.2854686379432678, Val Loss: 1.025127649307251\n",
      "Step: 4340, Subject: sub1; Training Loss: 0.22426927089691162, Val Loss: 1.0717300176620483\n",
      "Step: 4340, Subject: sub2; Training Loss: 0.1451965570449829, Val Loss: 0.8040950298309326\n",
      "Step: 4340, Subject: sub3; Training Loss: 0.3401362895965576, Val Loss: 1.1412235498428345\n",
      "Step: 4340, Subject: sub4; Training Loss: 0.19725176692008972, Val Loss: 1.2627588510513306\n",
      "Step: 4340, Subject: sub5; Training Loss: 0.2830779552459717, Val Loss: 1.0072181224822998\n",
      "Step: 4340, Subject: sub7; Training Loss: 0.262503057718277, Val Loss: 0.8993909358978271\n",
      "Step: 4350, Subject: sub1; Training Loss: 0.1868862509727478, Val Loss: 0.9942505359649658\n",
      "Step: 4350, Subject: sub2; Training Loss: 0.17765995860099792, Val Loss: 0.9398181438446045\n",
      "Step: 4350, Subject: sub3; Training Loss: 0.3930504322052002, Val Loss: 1.2116299867630005\n",
      "Step: 4350, Subject: sub4; Training Loss: 0.29088467359542847, Val Loss: 1.1162972450256348\n",
      "Step: 4350, Subject: sub5; Training Loss: 0.3453245162963867, Val Loss: 0.9110552668571472\n",
      "Step: 4350, Subject: sub7; Training Loss: 0.24620738625526428, Val Loss: 1.09775972366333\n",
      "Step: 4360, Subject: sub1; Training Loss: 0.17622491717338562, Val Loss: 1.2005000114440918\n",
      "Step: 4360, Subject: sub2; Training Loss: 0.13941819965839386, Val Loss: 0.8110082149505615\n",
      "Step: 4360, Subject: sub3; Training Loss: 0.33587658405303955, Val Loss: 0.7592633962631226\n",
      "Step: 4360, Subject: sub4; Training Loss: 0.22992095351219177, Val Loss: 0.792149007320404\n",
      "Step: 4360, Subject: sub5; Training Loss: 0.3779694139957428, Val Loss: 1.0698796510696411\n",
      "Step: 4360, Subject: sub7; Training Loss: 0.3362540602684021, Val Loss: 1.3672237396240234\n",
      "Step: 4370, Subject: sub1; Training Loss: 0.153201162815094, Val Loss: 0.6992906928062439\n",
      "Step: 4370, Subject: sub2; Training Loss: 0.15349987149238586, Val Loss: 0.8716523051261902\n",
      "Step: 4370, Subject: sub3; Training Loss: 0.4025070071220398, Val Loss: 0.8654885292053223\n",
      "Step: 4370, Subject: sub4; Training Loss: 0.3349171578884125, Val Loss: 0.7696880102157593\n",
      "Step: 4370, Subject: sub5; Training Loss: 0.398872971534729, Val Loss: 1.030525803565979\n",
      "Step: 4370, Subject: sub7; Training Loss: 0.2624354660511017, Val Loss: 1.251500129699707\n",
      "Step: 4380, Subject: sub1; Training Loss: 0.14608991146087646, Val Loss: 0.5874766707420349\n",
      "Step: 4380, Subject: sub2; Training Loss: 0.1860458254814148, Val Loss: 0.9356849193572998\n",
      "Step: 4380, Subject: sub3; Training Loss: 0.3470369577407837, Val Loss: 1.0130815505981445\n",
      "Step: 4380, Subject: sub4; Training Loss: 0.17060279846191406, Val Loss: 1.0165696144104004\n",
      "Step: 4380, Subject: sub5; Training Loss: 0.3181477189064026, Val Loss: 0.9765663146972656\n",
      "Step: 4380, Subject: sub7; Training Loss: 0.30511730909347534, Val Loss: 1.2617933750152588\n",
      "Step: 4390, Subject: sub1; Training Loss: 0.20225507020950317, Val Loss: 0.5997568368911743\n",
      "Step: 4390, Subject: sub2; Training Loss: 0.1616744101047516, Val Loss: 0.8224440813064575\n",
      "Step: 4390, Subject: sub3; Training Loss: 0.3043801486492157, Val Loss: 0.7340513467788696\n",
      "Step: 4390, Subject: sub4; Training Loss: 0.25723105669021606, Val Loss: 0.7797809839248657\n",
      "Step: 4390, Subject: sub5; Training Loss: 0.31007492542266846, Val Loss: 0.9243859648704529\n",
      "Step: 4390, Subject: sub7; Training Loss: 0.32559239864349365, Val Loss: 1.0068897008895874\n",
      "Step: 4400, Subject: sub1; Training Loss: 0.15680918097496033, Val Loss: 1.2370095252990723\n",
      "Step: 4400, Subject: sub2; Training Loss: 0.17168016731739044, Val Loss: 0.7975607514381409\n",
      "Step: 4400, Subject: sub3; Training Loss: 0.46486198902130127, Val Loss: 1.070666790008545\n",
      "Step: 4400, Subject: sub4; Training Loss: 0.2898327112197876, Val Loss: 1.020240306854248\n",
      "Step: 4400, Subject: sub5; Training Loss: 0.23874858021736145, Val Loss: 0.9170693159103394\n",
      "Step: 4400, Subject: sub7; Training Loss: 0.39956873655319214, Val Loss: 1.2316937446594238\n",
      "Step: 4410, Subject: sub1; Training Loss: 0.15280917286872864, Val Loss: 0.9753158688545227\n",
      "Step: 4410, Subject: sub2; Training Loss: 0.1253858208656311, Val Loss: 0.8467423319816589\n",
      "Step: 4410, Subject: sub3; Training Loss: 0.41040971875190735, Val Loss: 0.8375179171562195\n",
      "Step: 4410, Subject: sub4; Training Loss: 0.27897781133651733, Val Loss: 0.6677814722061157\n",
      "Step: 4410, Subject: sub5; Training Loss: 0.2636469602584839, Val Loss: 1.0984930992126465\n",
      "Step: 4410, Subject: sub7; Training Loss: 0.25763997435569763, Val Loss: 1.1920616626739502\n",
      "Step: 4420, Subject: sub1; Training Loss: 0.22854572534561157, Val Loss: 1.053890347480774\n",
      "Step: 4420, Subject: sub2; Training Loss: 0.13005611300468445, Val Loss: 0.9160876274108887\n",
      "Step: 4420, Subject: sub3; Training Loss: 0.4236864447593689, Val Loss: 1.017403244972229\n",
      "Step: 4420, Subject: sub4; Training Loss: 0.23423686623573303, Val Loss: 0.8212165832519531\n",
      "Step: 4420, Subject: sub5; Training Loss: 0.3037374019622803, Val Loss: 0.6652361750602722\n",
      "Step: 4420, Subject: sub7; Training Loss: 0.32952171564102173, Val Loss: 0.9714557528495789\n",
      "Step: 4430, Subject: sub1; Training Loss: 0.19412630796432495, Val Loss: 1.2370139360427856\n",
      "Step: 4430, Subject: sub2; Training Loss: 0.20729543268680573, Val Loss: 1.0077424049377441\n",
      "Step: 4430, Subject: sub3; Training Loss: 0.43216538429260254, Val Loss: 1.2363401651382446\n",
      "Step: 4430, Subject: sub4; Training Loss: 0.26606103777885437, Val Loss: 1.0807712078094482\n",
      "Step: 4430, Subject: sub5; Training Loss: 0.38218438625335693, Val Loss: 1.1961712837219238\n",
      "Step: 4430, Subject: sub7; Training Loss: 0.2899336814880371, Val Loss: 1.3451178073883057\n",
      "Step: 4440, Subject: sub1; Training Loss: 0.2154226452112198, Val Loss: 0.7450934648513794\n",
      "Step: 4440, Subject: sub2; Training Loss: 0.15060144662857056, Val Loss: 0.8829769492149353\n",
      "Step: 4440, Subject: sub3; Training Loss: 0.35327017307281494, Val Loss: 0.6623021960258484\n",
      "Step: 4440, Subject: sub4; Training Loss: 0.2372504025697708, Val Loss: 0.8207371234893799\n",
      "Step: 4440, Subject: sub5; Training Loss: 0.3285481929779053, Val Loss: 1.049660325050354\n",
      "Step: 4440, Subject: sub7; Training Loss: 0.28361451625823975, Val Loss: 1.0640381574630737\n",
      "Step: 4450, Subject: sub1; Training Loss: 0.1727207899093628, Val Loss: 1.4259462356567383\n",
      "Step: 4450, Subject: sub2; Training Loss: 0.15996548533439636, Val Loss: 1.005194902420044\n",
      "Step: 4450, Subject: sub3; Training Loss: 0.46581727266311646, Val Loss: 0.8935142755508423\n",
      "Step: 4450, Subject: sub4; Training Loss: 0.22636084258556366, Val Loss: 1.0232188701629639\n",
      "Step: 4450, Subject: sub5; Training Loss: 0.2865738272666931, Val Loss: 0.6539482474327087\n",
      "Step: 4450, Subject: sub7; Training Loss: 0.3058893084526062, Val Loss: 0.8342493772506714\n",
      "Step: 4460, Subject: sub1; Training Loss: 0.18133488297462463, Val Loss: 1.384652853012085\n",
      "Step: 4460, Subject: sub2; Training Loss: 0.21696481108665466, Val Loss: 0.7950336337089539\n",
      "Step: 4460, Subject: sub3; Training Loss: 0.38264304399490356, Val Loss: 1.2343837022781372\n",
      "Step: 4460, Subject: sub4; Training Loss: 0.22866687178611755, Val Loss: 1.1917279958724976\n",
      "Step: 4460, Subject: sub5; Training Loss: 0.2357960194349289, Val Loss: 1.0834152698516846\n",
      "Step: 4460, Subject: sub7; Training Loss: 0.21735405921936035, Val Loss: 1.0101865530014038\n",
      "Step: 4470, Subject: sub1; Training Loss: 0.15667074918746948, Val Loss: 1.3967554569244385\n",
      "Step: 4470, Subject: sub2; Training Loss: 0.14865154027938843, Val Loss: 0.9982262849807739\n",
      "Step: 4470, Subject: sub3; Training Loss: 0.446585476398468, Val Loss: 0.9689056277275085\n",
      "Step: 4470, Subject: sub4; Training Loss: 0.2931884527206421, Val Loss: 0.9509994983673096\n",
      "Step: 4470, Subject: sub5; Training Loss: 0.36537718772888184, Val Loss: 1.1469656229019165\n",
      "Step: 4470, Subject: sub7; Training Loss: 0.2468298077583313, Val Loss: 1.1598358154296875\n",
      "Step: 4480, Subject: sub1; Training Loss: 0.1716471016407013, Val Loss: 0.9150799512863159\n",
      "Step: 4480, Subject: sub2; Training Loss: 0.11248399317264557, Val Loss: 0.7824209928512573\n",
      "Step: 4480, Subject: sub3; Training Loss: 0.3848003149032593, Val Loss: 1.0471150875091553\n",
      "Step: 4480, Subject: sub4; Training Loss: 0.1369692087173462, Val Loss: 0.8811812996864319\n",
      "Step: 4480, Subject: sub5; Training Loss: 0.2878074049949646, Val Loss: 0.9836896061897278\n",
      "Step: 4480, Subject: sub7; Training Loss: 0.29793405532836914, Val Loss: 1.2942681312561035\n",
      "Step: 4490, Subject: sub1; Training Loss: 0.17726482450962067, Val Loss: 0.6145608425140381\n",
      "Step: 4490, Subject: sub2; Training Loss: 0.16419567167758942, Val Loss: 0.9798609614372253\n",
      "Step: 4490, Subject: sub3; Training Loss: 0.4152728319168091, Val Loss: 0.6600836515426636\n",
      "Step: 4490, Subject: sub4; Training Loss: 0.23575806617736816, Val Loss: 0.8756986856460571\n",
      "Step: 4490, Subject: sub5; Training Loss: 0.2868344187736511, Val Loss: 0.8867621421813965\n",
      "Step: 4490, Subject: sub7; Training Loss: 0.312583863735199, Val Loss: 0.9206519722938538\n",
      "Step: 4500, Subject: sub1; Training Loss: 0.17543496191501617, Val Loss: 1.2927308082580566\n",
      "Step: 4500, Subject: sub2; Training Loss: 0.19667650759220123, Val Loss: 0.882248044013977\n",
      "Step: 4500, Subject: sub3; Training Loss: 0.4067656397819519, Val Loss: 1.3188061714172363\n",
      "Step: 4500, Subject: sub4; Training Loss: 0.21104425191879272, Val Loss: 1.0161950588226318\n",
      "Step: 4500, Subject: sub5; Training Loss: 0.32308483123779297, Val Loss: 1.080068588256836\n",
      "Step: 4500, Subject: sub7; Training Loss: 0.24053744971752167, Val Loss: 1.4184539318084717\n",
      "Step: 4510, Subject: sub1; Training Loss: 0.14157110452651978, Val Loss: 0.9573080539703369\n",
      "Step: 4510, Subject: sub2; Training Loss: 0.1105705052614212, Val Loss: 0.8885462284088135\n",
      "Step: 4510, Subject: sub3; Training Loss: 0.40155357122421265, Val Loss: 0.8258973956108093\n",
      "Step: 4510, Subject: sub4; Training Loss: 0.1407846063375473, Val Loss: 0.8868653178215027\n",
      "Step: 4510, Subject: sub5; Training Loss: 0.294380784034729, Val Loss: 0.8830329775810242\n",
      "Step: 4510, Subject: sub7; Training Loss: 0.2809344232082367, Val Loss: 1.1914398670196533\n",
      "Step: 4520, Subject: sub1; Training Loss: 0.17409217357635498, Val Loss: 0.929591953754425\n",
      "Step: 4520, Subject: sub2; Training Loss: 0.14757059514522552, Val Loss: 0.7707470059394836\n",
      "Step: 4520, Subject: sub3; Training Loss: 0.37253016233444214, Val Loss: 0.6490026712417603\n",
      "Step: 4520, Subject: sub4; Training Loss: 0.1802619993686676, Val Loss: 0.776338517665863\n",
      "Step: 4520, Subject: sub5; Training Loss: 0.33552050590515137, Val Loss: 0.8161858916282654\n",
      "Step: 4520, Subject: sub7; Training Loss: 0.2547980546951294, Val Loss: 1.0465593338012695\n",
      "Step: 4530, Subject: sub1; Training Loss: 0.20401525497436523, Val Loss: 0.5976595282554626\n",
      "Step: 4530, Subject: sub2; Training Loss: 0.1590154469013214, Val Loss: 0.819817066192627\n",
      "Step: 4530, Subject: sub3; Training Loss: 0.31850665807724, Val Loss: 0.9159352779388428\n",
      "Step: 4530, Subject: sub4; Training Loss: 0.21321463584899902, Val Loss: 0.8642315864562988\n",
      "Step: 4530, Subject: sub5; Training Loss: 0.2535625696182251, Val Loss: 1.172581672668457\n",
      "Step: 4530, Subject: sub7; Training Loss: 0.3025912642478943, Val Loss: 1.2382315397262573\n",
      "Step: 4540, Subject: sub1; Training Loss: 0.18306857347488403, Val Loss: 1.178374171257019\n",
      "Step: 4540, Subject: sub2; Training Loss: 0.14811426401138306, Val Loss: 0.8494863510131836\n",
      "Step: 4540, Subject: sub3; Training Loss: 0.3762984275817871, Val Loss: 1.0072799921035767\n",
      "Step: 4540, Subject: sub4; Training Loss: 0.2491210252046585, Val Loss: 0.689456582069397\n",
      "Step: 4540, Subject: sub5; Training Loss: 0.41193416714668274, Val Loss: 0.9116904735565186\n",
      "Step: 4540, Subject: sub7; Training Loss: 0.27025824785232544, Val Loss: 1.104411005973816\n",
      "Step: 4550, Subject: sub1; Training Loss: 0.13748496770858765, Val Loss: 1.250929832458496\n",
      "Step: 4550, Subject: sub2; Training Loss: 0.18106207251548767, Val Loss: 0.8651821613311768\n",
      "Step: 4550, Subject: sub3; Training Loss: 0.35859861969947815, Val Loss: 0.9454482793807983\n",
      "Step: 4550, Subject: sub4; Training Loss: 0.18157730996608734, Val Loss: 0.9706168174743652\n",
      "Step: 4550, Subject: sub5; Training Loss: 0.289227694272995, Val Loss: 1.0535932779312134\n",
      "Step: 4550, Subject: sub7; Training Loss: 0.19768518209457397, Val Loss: 1.201196312904358\n",
      "Step: 4560, Subject: sub1; Training Loss: 0.17223139107227325, Val Loss: 1.1211795806884766\n",
      "Step: 4560, Subject: sub2; Training Loss: 0.11316311359405518, Val Loss: 0.897334635257721\n",
      "Step: 4560, Subject: sub3; Training Loss: 0.4042202830314636, Val Loss: 0.775175929069519\n",
      "Step: 4560, Subject: sub4; Training Loss: 0.2317717969417572, Val Loss: 0.835120677947998\n",
      "Step: 4560, Subject: sub5; Training Loss: 0.3424009084701538, Val Loss: 0.836803674697876\n",
      "Step: 4560, Subject: sub7; Training Loss: 0.25321027636528015, Val Loss: 1.5464640855789185\n",
      "Step: 4570, Subject: sub1; Training Loss: 0.15155544877052307, Val Loss: 1.1810096502304077\n",
      "Step: 4570, Subject: sub2; Training Loss: 0.14069761335849762, Val Loss: 0.8373545408248901\n",
      "Step: 4570, Subject: sub3; Training Loss: 0.40038377046585083, Val Loss: 0.7046802043914795\n",
      "Step: 4570, Subject: sub4; Training Loss: 0.22547388076782227, Val Loss: 0.9363169074058533\n",
      "Step: 4570, Subject: sub5; Training Loss: 0.33010411262512207, Val Loss: 1.1436063051223755\n",
      "Step: 4570, Subject: sub7; Training Loss: 0.3231887221336365, Val Loss: 1.344157099723816\n",
      "Step: 4580, Subject: sub1; Training Loss: 0.17899881303310394, Val Loss: 0.5891155004501343\n",
      "Step: 4580, Subject: sub2; Training Loss: 0.1978335678577423, Val Loss: 0.8317666053771973\n",
      "Step: 4580, Subject: sub3; Training Loss: 0.383320689201355, Val Loss: 0.6346879005432129\n",
      "Step: 4580, Subject: sub4; Training Loss: 0.19768516719341278, Val Loss: 0.8102412223815918\n",
      "Step: 4580, Subject: sub5; Training Loss: 0.25572073459625244, Val Loss: 0.8872190117835999\n",
      "Step: 4580, Subject: sub7; Training Loss: 0.2367289662361145, Val Loss: 0.9557472467422485\n",
      "Step: 4590, Subject: sub1; Training Loss: 0.2164759337902069, Val Loss: 1.186608076095581\n",
      "Step: 4590, Subject: sub2; Training Loss: 0.118454709649086, Val Loss: 0.8490501642227173\n",
      "Step: 4590, Subject: sub3; Training Loss: 0.3607887029647827, Val Loss: 0.7889177799224854\n",
      "Step: 4590, Subject: sub4; Training Loss: 0.19908928871154785, Val Loss: 1.0957915782928467\n",
      "Step: 4590, Subject: sub5; Training Loss: 0.3415362238883972, Val Loss: 0.8836831450462341\n",
      "Step: 4590, Subject: sub7; Training Loss: 0.20017148554325104, Val Loss: 0.7753898501396179\n",
      "Step: 4600, Subject: sub1; Training Loss: 0.15455177426338196, Val Loss: 1.3342204093933105\n",
      "Step: 4600, Subject: sub2; Training Loss: 0.17073848843574524, Val Loss: 0.8173602223396301\n",
      "Step: 4600, Subject: sub3; Training Loss: 0.33908769488334656, Val Loss: 0.8940945863723755\n",
      "Step: 4600, Subject: sub4; Training Loss: 0.2550050914287567, Val Loss: 1.2316926717758179\n",
      "Step: 4600, Subject: sub5; Training Loss: 0.330533504486084, Val Loss: 1.0285230875015259\n",
      "Step: 4600, Subject: sub7; Training Loss: 0.23374249041080475, Val Loss: 1.0557987689971924\n",
      "Step: 4610, Subject: sub1; Training Loss: 0.11090593039989471, Val Loss: 0.820297122001648\n",
      "Step: 4610, Subject: sub2; Training Loss: 0.1219189390540123, Val Loss: 0.8369556665420532\n",
      "Step: 4610, Subject: sub3; Training Loss: 0.405955046415329, Val Loss: 0.661107063293457\n",
      "Step: 4610, Subject: sub4; Training Loss: 0.20063352584838867, Val Loss: 1.129564642906189\n",
      "Step: 4610, Subject: sub5; Training Loss: 0.33424943685531616, Val Loss: 0.597839891910553\n",
      "Step: 4610, Subject: sub7; Training Loss: 0.3434552550315857, Val Loss: 1.2858483791351318\n",
      "Step: 4620, Subject: sub1; Training Loss: 0.1515655815601349, Val Loss: 1.2463067770004272\n",
      "Step: 4620, Subject: sub2; Training Loss: 0.1380045861005783, Val Loss: 0.8075810670852661\n",
      "Step: 4620, Subject: sub3; Training Loss: 0.38525140285491943, Val Loss: 0.8059661388397217\n",
      "Step: 4620, Subject: sub4; Training Loss: 0.24063056707382202, Val Loss: 0.8582455515861511\n",
      "Step: 4620, Subject: sub5; Training Loss: 0.30639708042144775, Val Loss: 0.9520551562309265\n",
      "Step: 4620, Subject: sub7; Training Loss: 0.27598631381988525, Val Loss: 1.4379085302352905\n",
      "Step: 4630, Subject: sub1; Training Loss: 0.14604851603507996, Val Loss: 1.6259889602661133\n",
      "Step: 4630, Subject: sub2; Training Loss: 0.12676168978214264, Val Loss: 0.8710513114929199\n",
      "Step: 4630, Subject: sub3; Training Loss: 0.3953641653060913, Val Loss: 0.8048263192176819\n",
      "Step: 4630, Subject: sub4; Training Loss: 0.14377474784851074, Val Loss: 0.788100004196167\n",
      "Step: 4630, Subject: sub5; Training Loss: 0.282161682844162, Val Loss: 1.259143590927124\n",
      "Step: 4630, Subject: sub7; Training Loss: 0.23359720408916473, Val Loss: 1.3274022340774536\n",
      "Step: 4640, Subject: sub1; Training Loss: 0.21093381941318512, Val Loss: 0.8799654245376587\n",
      "Step: 4640, Subject: sub2; Training Loss: 0.17519301176071167, Val Loss: 0.8921300172805786\n",
      "Step: 4640, Subject: sub3; Training Loss: 0.35495930910110474, Val Loss: 0.9492927193641663\n",
      "Step: 4640, Subject: sub4; Training Loss: 0.24205425381660461, Val Loss: 0.9100245833396912\n",
      "Step: 4640, Subject: sub5; Training Loss: 0.3242867887020111, Val Loss: 0.6517528891563416\n",
      "Step: 4640, Subject: sub7; Training Loss: 0.25036388635635376, Val Loss: 0.941865861415863\n",
      "Step: 4650, Subject: sub1; Training Loss: 0.1734270453453064, Val Loss: 0.9030006527900696\n",
      "Step: 4650, Subject: sub2; Training Loss: 0.15723828971385956, Val Loss: 0.8123199343681335\n",
      "Step: 4650, Subject: sub3; Training Loss: 0.35425031185150146, Val Loss: 0.8692407011985779\n",
      "Step: 4650, Subject: sub4; Training Loss: 0.2536551356315613, Val Loss: 1.1303329467773438\n",
      "Step: 4650, Subject: sub5; Training Loss: 0.3286360502243042, Val Loss: 0.652083158493042\n",
      "Step: 4650, Subject: sub7; Training Loss: 0.23910290002822876, Val Loss: 0.937353253364563\n",
      "Step: 4660, Subject: sub1; Training Loss: 0.1748199462890625, Val Loss: 0.8104269504547119\n",
      "Step: 4660, Subject: sub2; Training Loss: 0.16848774254322052, Val Loss: 0.952961802482605\n",
      "Step: 4660, Subject: sub3; Training Loss: 0.3244536519050598, Val Loss: 0.9181960821151733\n",
      "Step: 4660, Subject: sub4; Training Loss: 0.19149649143218994, Val Loss: 0.9053308963775635\n",
      "Step: 4660, Subject: sub5; Training Loss: 0.32617583870887756, Val Loss: 0.8546440601348877\n",
      "Step: 4660, Subject: sub7; Training Loss: 0.23114939033985138, Val Loss: 1.2929534912109375\n",
      "Step: 4670, Subject: sub1; Training Loss: 0.19110244512557983, Val Loss: 1.0570179224014282\n",
      "Step: 4670, Subject: sub2; Training Loss: 0.12626700103282928, Val Loss: 0.7252899408340454\n",
      "Step: 4670, Subject: sub3; Training Loss: 0.48890531063079834, Val Loss: 1.4459176063537598\n",
      "Step: 4670, Subject: sub4; Training Loss: 0.24808882176876068, Val Loss: 0.8665926456451416\n",
      "Step: 4670, Subject: sub5; Training Loss: 0.2437148541212082, Val Loss: 0.8795486688613892\n",
      "Step: 4670, Subject: sub7; Training Loss: 0.22102530300617218, Val Loss: 1.125615119934082\n",
      "Step: 4680, Subject: sub1; Training Loss: 0.16519665718078613, Val Loss: 1.8668662309646606\n",
      "Step: 4680, Subject: sub2; Training Loss: 0.12647362053394318, Val Loss: 0.8409504890441895\n",
      "Step: 4680, Subject: sub3; Training Loss: 0.36337223649024963, Val Loss: 0.9259052276611328\n",
      "Step: 4680, Subject: sub4; Training Loss: 0.20009729266166687, Val Loss: 0.8506565690040588\n",
      "Step: 4680, Subject: sub5; Training Loss: 0.30876076221466064, Val Loss: 0.9940603971481323\n",
      "Step: 4680, Subject: sub7; Training Loss: 0.298766165971756, Val Loss: 1.2641230821609497\n",
      "Step: 4690, Subject: sub1; Training Loss: 0.186494380235672, Val Loss: 0.9292206168174744\n",
      "Step: 4690, Subject: sub2; Training Loss: 0.15303990244865417, Val Loss: 0.9820914268493652\n",
      "Step: 4690, Subject: sub3; Training Loss: 0.35129401087760925, Val Loss: 0.6864595413208008\n",
      "Step: 4690, Subject: sub4; Training Loss: 0.23275455832481384, Val Loss: 0.9341058135032654\n",
      "Step: 4690, Subject: sub5; Training Loss: 0.47290417551994324, Val Loss: 0.8097009658813477\n",
      "Step: 4690, Subject: sub7; Training Loss: 0.20675024390220642, Val Loss: 1.4302834272384644\n",
      "Step: 4700, Subject: sub1; Training Loss: 0.13971129059791565, Val Loss: 1.0006883144378662\n",
      "Step: 4700, Subject: sub2; Training Loss: 0.1936539262533188, Val Loss: 0.8475673794746399\n",
      "Step: 4700, Subject: sub3; Training Loss: 0.34881553053855896, Val Loss: 0.8906775712966919\n",
      "Step: 4700, Subject: sub4; Training Loss: 0.18615533411502838, Val Loss: 0.9132115840911865\n",
      "Step: 4700, Subject: sub5; Training Loss: 0.3147955536842346, Val Loss: 0.9852805733680725\n",
      "Step: 4700, Subject: sub7; Training Loss: 0.31873852014541626, Val Loss: 1.0942646265029907\n",
      "Step: 4710, Subject: sub1; Training Loss: 0.17141860723495483, Val Loss: 0.8522895574569702\n",
      "Step: 4710, Subject: sub2; Training Loss: 0.14069637656211853, Val Loss: 0.9685671329498291\n",
      "Step: 4710, Subject: sub3; Training Loss: 0.34868115186691284, Val Loss: 1.1088624000549316\n",
      "Step: 4710, Subject: sub4; Training Loss: 0.2717192769050598, Val Loss: 0.9640952348709106\n",
      "Step: 4710, Subject: sub5; Training Loss: 0.3771580457687378, Val Loss: 0.6550871729850769\n",
      "Step: 4710, Subject: sub7; Training Loss: 0.3634524941444397, Val Loss: 0.841239333152771\n",
      "Step: 4720, Subject: sub1; Training Loss: 0.1619437038898468, Val Loss: 1.1422513723373413\n",
      "Step: 4720, Subject: sub2; Training Loss: 0.16155767440795898, Val Loss: 0.8943281769752502\n",
      "Step: 4720, Subject: sub3; Training Loss: 0.40370064973831177, Val Loss: 1.0192548036575317\n",
      "Step: 4720, Subject: sub4; Training Loss: 0.23784838616847992, Val Loss: 0.8188296556472778\n",
      "Step: 4720, Subject: sub5; Training Loss: 0.312180757522583, Val Loss: 0.7953640818595886\n",
      "Step: 4720, Subject: sub7; Training Loss: 0.31660881638526917, Val Loss: 1.241295576095581\n",
      "Step: 4730, Subject: sub1; Training Loss: 0.14855527877807617, Val Loss: 0.99098801612854\n",
      "Step: 4730, Subject: sub2; Training Loss: 0.12744837999343872, Val Loss: 0.9678623676300049\n",
      "Step: 4730, Subject: sub3; Training Loss: 0.3534039855003357, Val Loss: 1.0376057624816895\n",
      "Step: 4730, Subject: sub4; Training Loss: 0.1674480438232422, Val Loss: 0.9202694296836853\n",
      "Step: 4730, Subject: sub5; Training Loss: 0.2717244625091553, Val Loss: 0.9227403402328491\n",
      "Step: 4730, Subject: sub7; Training Loss: 0.2779739499092102, Val Loss: 1.213132381439209\n",
      "Step: 4740, Subject: sub1; Training Loss: 0.19614671170711517, Val Loss: 1.1116095781326294\n",
      "Step: 4740, Subject: sub2; Training Loss: 0.1585092544555664, Val Loss: 0.7746487855911255\n",
      "Step: 4740, Subject: sub3; Training Loss: 0.3918088674545288, Val Loss: 0.8742042779922485\n",
      "Step: 4740, Subject: sub4; Training Loss: 0.2120692878961563, Val Loss: 0.8528382778167725\n",
      "Step: 4740, Subject: sub5; Training Loss: 0.24209854006767273, Val Loss: 0.8916335105895996\n",
      "Step: 4740, Subject: sub7; Training Loss: 0.25006210803985596, Val Loss: 1.0230814218521118\n",
      "Step: 4750, Subject: sub1; Training Loss: 0.14247335493564606, Val Loss: 1.2256619930267334\n",
      "Step: 4750, Subject: sub2; Training Loss: 0.13419680297374725, Val Loss: 0.789099931716919\n",
      "Step: 4750, Subject: sub3; Training Loss: 0.34257668256759644, Val Loss: 1.0302094221115112\n",
      "Step: 4750, Subject: sub4; Training Loss: 0.23786357045173645, Val Loss: 0.8446537852287292\n",
      "Step: 4750, Subject: sub5; Training Loss: 0.32171833515167236, Val Loss: 0.7693699598312378\n",
      "Step: 4750, Subject: sub7; Training Loss: 0.22248145937919617, Val Loss: 0.9864827990531921\n",
      "Step: 4760, Subject: sub1; Training Loss: 0.20618772506713867, Val Loss: 1.2280337810516357\n",
      "Step: 4760, Subject: sub2; Training Loss: 0.10970604419708252, Val Loss: 0.8522336483001709\n",
      "Step: 4760, Subject: sub3; Training Loss: 0.34322068095207214, Val Loss: 0.8400049209594727\n",
      "Step: 4760, Subject: sub4; Training Loss: 0.19677585363388062, Val Loss: 0.9173548221588135\n",
      "Step: 4760, Subject: sub5; Training Loss: 0.30965861678123474, Val Loss: 0.9158439040184021\n",
      "Step: 4760, Subject: sub7; Training Loss: 0.28444594144821167, Val Loss: 0.8490931391716003\n",
      "Step: 4770, Subject: sub1; Training Loss: 0.1523146778345108, Val Loss: 0.9233875274658203\n",
      "Step: 4770, Subject: sub2; Training Loss: 0.16081738471984863, Val Loss: 1.0204017162322998\n",
      "Step: 4770, Subject: sub3; Training Loss: 0.3813570737838745, Val Loss: 0.817240834236145\n",
      "Step: 4770, Subject: sub4; Training Loss: 0.2647618353366852, Val Loss: 1.0951838493347168\n",
      "Step: 4770, Subject: sub5; Training Loss: 0.3414314389228821, Val Loss: 0.6815038919448853\n",
      "Step: 4770, Subject: sub7; Training Loss: 0.25424009561538696, Val Loss: 0.8845359086990356\n",
      "Step: 4780, Subject: sub1; Training Loss: 0.15864455699920654, Val Loss: 1.6243064403533936\n",
      "Step: 4780, Subject: sub2; Training Loss: 0.15307722985744476, Val Loss: 0.8401836156845093\n",
      "Step: 4780, Subject: sub3; Training Loss: 0.4890698194503784, Val Loss: 0.7499391436576843\n",
      "Step: 4780, Subject: sub4; Training Loss: 0.20397010445594788, Val Loss: 0.7963376045227051\n",
      "Step: 4780, Subject: sub5; Training Loss: 0.34567704796791077, Val Loss: 1.3417303562164307\n",
      "Step: 4780, Subject: sub7; Training Loss: 0.21895447373390198, Val Loss: 1.6074492931365967\n",
      "Step: 4790, Subject: sub1; Training Loss: 0.15650051832199097, Val Loss: 1.5366121530532837\n",
      "Step: 4790, Subject: sub2; Training Loss: 0.1719931662082672, Val Loss: 0.7967945337295532\n",
      "Step: 4790, Subject: sub3; Training Loss: 0.3635420501232147, Val Loss: 0.6711281538009644\n",
      "Step: 4790, Subject: sub4; Training Loss: 0.20596446096897125, Val Loss: 0.9070355296134949\n",
      "Step: 4790, Subject: sub5; Training Loss: 0.291448712348938, Val Loss: 0.6640009880065918\n",
      "Step: 4790, Subject: sub7; Training Loss: 0.1818268597126007, Val Loss: 1.2836856842041016\n",
      "Step: 4800, Subject: sub1; Training Loss: 0.15022118389606476, Val Loss: 1.1067763566970825\n",
      "Step: 4800, Subject: sub2; Training Loss: 0.1228281706571579, Val Loss: 0.9732654690742493\n",
      "Step: 4800, Subject: sub3; Training Loss: 0.3305481970310211, Val Loss: 0.7496978640556335\n",
      "Step: 4800, Subject: sub4; Training Loss: 0.183444082736969, Val Loss: 1.0069539546966553\n",
      "Step: 4800, Subject: sub5; Training Loss: 0.3965292274951935, Val Loss: 0.4987034797668457\n",
      "Step: 4800, Subject: sub7; Training Loss: 0.27381181716918945, Val Loss: 0.9085855484008789\n",
      "Step: 4810, Subject: sub1; Training Loss: 0.16941148042678833, Val Loss: 0.9397478103637695\n",
      "Step: 4810, Subject: sub2; Training Loss: 0.18213890492916107, Val Loss: 0.8589348196983337\n",
      "Step: 4810, Subject: sub3; Training Loss: 0.30108267068862915, Val Loss: 0.9519880414009094\n",
      "Step: 4810, Subject: sub4; Training Loss: 0.20329053699970245, Val Loss: 0.9606720209121704\n",
      "Step: 4810, Subject: sub5; Training Loss: 0.2956065535545349, Val Loss: 0.9335397481918335\n",
      "Step: 4810, Subject: sub7; Training Loss: 0.3458472490310669, Val Loss: 1.3046774864196777\n",
      "Step: 4820, Subject: sub1; Training Loss: 0.13680213689804077, Val Loss: 1.461841106414795\n",
      "Step: 4820, Subject: sub2; Training Loss: 0.20265057682991028, Val Loss: 0.8838547468185425\n",
      "Step: 4820, Subject: sub3; Training Loss: 0.362352579832077, Val Loss: 0.8829666972160339\n",
      "Step: 4820, Subject: sub4; Training Loss: 0.30328369140625, Val Loss: 1.1710023880004883\n",
      "Step: 4820, Subject: sub5; Training Loss: 0.4276517629623413, Val Loss: 0.8900308012962341\n",
      "Step: 4820, Subject: sub7; Training Loss: 0.2967602014541626, Val Loss: 1.0095429420471191\n",
      "Step: 4830, Subject: sub1; Training Loss: 0.1438942700624466, Val Loss: 0.7345131635665894\n",
      "Step: 4830, Subject: sub2; Training Loss: 0.14773324131965637, Val Loss: 0.8369942903518677\n",
      "Step: 4830, Subject: sub3; Training Loss: 0.3246528208255768, Val Loss: 0.9655546545982361\n",
      "Step: 4830, Subject: sub4; Training Loss: 0.15798251330852509, Val Loss: 0.9541753530502319\n",
      "Step: 4830, Subject: sub5; Training Loss: 0.2984647750854492, Val Loss: 0.6443684101104736\n",
      "Step: 4830, Subject: sub7; Training Loss: 0.29065048694610596, Val Loss: 1.2495007514953613\n",
      "Step: 4840, Subject: sub1; Training Loss: 0.18015027046203613, Val Loss: 1.1234005689620972\n",
      "Step: 4840, Subject: sub2; Training Loss: 0.11110164225101471, Val Loss: 0.9207303524017334\n",
      "Step: 4840, Subject: sub3; Training Loss: 0.39062684774398804, Val Loss: 0.837975025177002\n",
      "Step: 4840, Subject: sub4; Training Loss: 0.14899328351020813, Val Loss: 0.9089898467063904\n",
      "Step: 4840, Subject: sub5; Training Loss: 0.28847694396972656, Val Loss: 1.1164031028747559\n",
      "Step: 4840, Subject: sub7; Training Loss: 0.3016752302646637, Val Loss: 1.1316982507705688\n",
      "Step: 4850, Subject: sub1; Training Loss: 0.16855400800704956, Val Loss: 0.5524132251739502\n",
      "Step: 4850, Subject: sub2; Training Loss: 0.14392459392547607, Val Loss: 0.8865910172462463\n",
      "Step: 4850, Subject: sub3; Training Loss: 0.40477579832077026, Val Loss: 0.9855366945266724\n",
      "Step: 4850, Subject: sub4; Training Loss: 0.1660279631614685, Val Loss: 1.0658899545669556\n",
      "Step: 4850, Subject: sub5; Training Loss: 0.28635597229003906, Val Loss: 0.7541120648384094\n",
      "Step: 4850, Subject: sub7; Training Loss: 0.3547091782093048, Val Loss: 1.0325069427490234\n",
      "Step: 4860, Subject: sub1; Training Loss: 0.15534961223602295, Val Loss: 0.7016751766204834\n",
      "Step: 4860, Subject: sub2; Training Loss: 0.1183261126279831, Val Loss: 0.7993699312210083\n",
      "Step: 4860, Subject: sub3; Training Loss: 0.33275771141052246, Val Loss: 0.9396312236785889\n",
      "Step: 4860, Subject: sub4; Training Loss: 0.20391982793807983, Val Loss: 0.7864474058151245\n",
      "Step: 4860, Subject: sub5; Training Loss: 0.30068206787109375, Val Loss: 0.6329153776168823\n",
      "Step: 4860, Subject: sub7; Training Loss: 0.25378137826919556, Val Loss: 1.1807280778884888\n",
      "Step: 4870, Subject: sub1; Training Loss: 0.12381113320589066, Val Loss: 1.1369141340255737\n",
      "Step: 4870, Subject: sub2; Training Loss: 0.10948508977890015, Val Loss: 0.8219587206840515\n",
      "Step: 4870, Subject: sub3; Training Loss: 0.38728076219558716, Val Loss: 0.6898255944252014\n",
      "Step: 4870, Subject: sub4; Training Loss: 0.21364596486091614, Val Loss: 0.9126782417297363\n",
      "Step: 4870, Subject: sub5; Training Loss: 0.2869057059288025, Val Loss: 0.7733663320541382\n",
      "Step: 4870, Subject: sub7; Training Loss: 0.205448180437088, Val Loss: 1.01616370677948\n",
      "Step: 4880, Subject: sub1; Training Loss: 0.1643734872341156, Val Loss: 1.2306888103485107\n",
      "Step: 4880, Subject: sub2; Training Loss: 0.11166581511497498, Val Loss: 0.9602092504501343\n",
      "Step: 4880, Subject: sub3; Training Loss: 0.3635668158531189, Val Loss: 0.6981762051582336\n",
      "Step: 4880, Subject: sub4; Training Loss: 0.2829074263572693, Val Loss: 1.0774762630462646\n",
      "Step: 4880, Subject: sub5; Training Loss: 0.3119167685508728, Val Loss: 1.0699987411499023\n",
      "Step: 4880, Subject: sub7; Training Loss: 0.2708442211151123, Val Loss: 1.011480689048767\n",
      "Step: 4890, Subject: sub1; Training Loss: 0.13098306953907013, Val Loss: 1.1321440935134888\n",
      "Step: 4890, Subject: sub2; Training Loss: 0.12875154614448547, Val Loss: 0.8062841892242432\n",
      "Step: 4890, Subject: sub3; Training Loss: 0.36584097146987915, Val Loss: 0.8141680359840393\n",
      "Step: 4890, Subject: sub4; Training Loss: 0.31854480504989624, Val Loss: 1.2672290802001953\n",
      "Step: 4890, Subject: sub5; Training Loss: 0.25019922852516174, Val Loss: 0.9089558124542236\n",
      "Step: 4890, Subject: sub7; Training Loss: 0.3232665956020355, Val Loss: 0.8586593270301819\n",
      "Step: 4900, Subject: sub1; Training Loss: 0.1359102874994278, Val Loss: 0.7110576033592224\n",
      "Step: 4900, Subject: sub2; Training Loss: 0.12050558626651764, Val Loss: 0.8263519406318665\n",
      "Step: 4900, Subject: sub3; Training Loss: 0.3837434947490692, Val Loss: 0.6698904037475586\n",
      "Step: 4900, Subject: sub4; Training Loss: 0.215898796916008, Val Loss: 0.9908201694488525\n",
      "Step: 4900, Subject: sub5; Training Loss: 0.2370302528142929, Val Loss: 0.7622514963150024\n",
      "Step: 4900, Subject: sub7; Training Loss: 0.24917177855968475, Val Loss: 1.1958729028701782\n",
      "Step: 4910, Subject: sub1; Training Loss: 0.14912304282188416, Val Loss: 1.012775182723999\n",
      "Step: 4910, Subject: sub2; Training Loss: 0.15568116307258606, Val Loss: 0.8159561157226562\n",
      "Step: 4910, Subject: sub3; Training Loss: 0.39456143975257874, Val Loss: 0.794837474822998\n",
      "Step: 4910, Subject: sub4; Training Loss: 0.17200367152690887, Val Loss: 0.8548183441162109\n",
      "Step: 4910, Subject: sub5; Training Loss: 0.26715731620788574, Val Loss: 0.8118659257888794\n",
      "Step: 4910, Subject: sub7; Training Loss: 0.3014904856681824, Val Loss: 1.1785365343093872\n",
      "Step: 4920, Subject: sub1; Training Loss: 0.1607464849948883, Val Loss: 1.1559908390045166\n",
      "Step: 4920, Subject: sub2; Training Loss: 0.1327120065689087, Val Loss: 0.8782132267951965\n",
      "Step: 4920, Subject: sub3; Training Loss: 0.33501291275024414, Val Loss: 1.0633777379989624\n",
      "Step: 4920, Subject: sub4; Training Loss: 0.22950780391693115, Val Loss: 0.7928005456924438\n",
      "Step: 4920, Subject: sub5; Training Loss: 0.3135830760002136, Val Loss: 0.6857290267944336\n",
      "Step: 4920, Subject: sub7; Training Loss: 0.21728157997131348, Val Loss: 0.9410596489906311\n",
      "Step: 4930, Subject: sub1; Training Loss: 0.17990994453430176, Val Loss: 1.1487576961517334\n",
      "Step: 4930, Subject: sub2; Training Loss: 0.1246441975235939, Val Loss: 0.7739720344543457\n",
      "Step: 4930, Subject: sub3; Training Loss: 0.36004889011383057, Val Loss: 0.7882423400878906\n",
      "Step: 4930, Subject: sub4; Training Loss: 0.21568642556667328, Val Loss: 1.1233558654785156\n",
      "Step: 4930, Subject: sub5; Training Loss: 0.2719677686691284, Val Loss: 0.7265306711196899\n",
      "Step: 4930, Subject: sub7; Training Loss: 0.23825420439243317, Val Loss: 1.061692237854004\n",
      "Step: 4940, Subject: sub1; Training Loss: 0.13548707962036133, Val Loss: 1.0733129978179932\n",
      "Step: 4940, Subject: sub2; Training Loss: 0.21259573101997375, Val Loss: 0.807119607925415\n",
      "Step: 4940, Subject: sub3; Training Loss: 0.3317079544067383, Val Loss: 0.7782198190689087\n",
      "Step: 4940, Subject: sub4; Training Loss: 0.21733753383159637, Val Loss: 0.9252901077270508\n",
      "Step: 4940, Subject: sub5; Training Loss: 0.34531497955322266, Val Loss: 0.5941498279571533\n",
      "Step: 4940, Subject: sub7; Training Loss: 0.2771089971065521, Val Loss: 0.8986764550209045\n",
      "Step: 4950, Subject: sub1; Training Loss: 0.17728203535079956, Val Loss: 0.9621806740760803\n",
      "Step: 4950, Subject: sub2; Training Loss: 0.14292538166046143, Val Loss: 0.8269754648208618\n",
      "Step: 4950, Subject: sub3; Training Loss: 0.30683040618896484, Val Loss: 0.7295591235160828\n",
      "Step: 4950, Subject: sub4; Training Loss: 0.20024624466896057, Val Loss: 0.7109240293502808\n",
      "Step: 4950, Subject: sub5; Training Loss: 0.2196982204914093, Val Loss: 1.0498576164245605\n",
      "Step: 4950, Subject: sub7; Training Loss: 0.2660576105117798, Val Loss: 1.1071434020996094\n",
      "Step: 4960, Subject: sub1; Training Loss: 0.1976976990699768, Val Loss: 0.9014539122581482\n",
      "Step: 4960, Subject: sub2; Training Loss: 0.10783439129590988, Val Loss: 0.9914655685424805\n",
      "Step: 4960, Subject: sub3; Training Loss: 0.361965149641037, Val Loss: 0.85859614610672\n",
      "Step: 4960, Subject: sub4; Training Loss: 0.21480320394039154, Val Loss: 0.8829129338264465\n",
      "Step: 4960, Subject: sub5; Training Loss: 0.245150625705719, Val Loss: 0.5789504647254944\n",
      "Step: 4960, Subject: sub7; Training Loss: 0.2573935389518738, Val Loss: 1.0466632843017578\n",
      "Step: 4970, Subject: sub1; Training Loss: 0.1548539251089096, Val Loss: 0.8143519163131714\n",
      "Step: 4970, Subject: sub2; Training Loss: 0.09822417795658112, Val Loss: 0.8974975347518921\n",
      "Step: 4970, Subject: sub3; Training Loss: 0.3158795237541199, Val Loss: 0.8480281829833984\n",
      "Step: 4970, Subject: sub4; Training Loss: 0.25175368785858154, Val Loss: 0.8271648287773132\n",
      "Step: 4970, Subject: sub5; Training Loss: 0.24564307928085327, Val Loss: 0.826227068901062\n",
      "Step: 4970, Subject: sub7; Training Loss: 0.20509567856788635, Val Loss: 0.8738319873809814\n",
      "Step: 4980, Subject: sub1; Training Loss: 0.21683107316493988, Val Loss: 0.4409767687320709\n",
      "Step: 4980, Subject: sub2; Training Loss: 0.15615598857402802, Val Loss: 0.8834884166717529\n",
      "Step: 4980, Subject: sub3; Training Loss: 0.42458218336105347, Val Loss: 0.6664893627166748\n",
      "Step: 4980, Subject: sub4; Training Loss: 0.2069656401872635, Val Loss: 0.9689455032348633\n",
      "Step: 4980, Subject: sub5; Training Loss: 0.2632838785648346, Val Loss: 1.1218993663787842\n",
      "Step: 4980, Subject: sub7; Training Loss: 0.22163239121437073, Val Loss: 1.167848825454712\n",
      "Step: 4990, Subject: sub1; Training Loss: 0.14534598588943481, Val Loss: 0.8949599266052246\n",
      "Step: 4990, Subject: sub2; Training Loss: 0.17900225520133972, Val Loss: 0.767212986946106\n",
      "Step: 4990, Subject: sub3; Training Loss: 0.3792075514793396, Val Loss: 0.8252961039543152\n",
      "Step: 4990, Subject: sub4; Training Loss: 0.20481303334236145, Val Loss: 0.8872658610343933\n",
      "Step: 4990, Subject: sub5; Training Loss: 0.3237016797065735, Val Loss: 0.9985836148262024\n",
      "Step: 4990, Subject: sub7; Training Loss: 0.24358955025672913, Val Loss: 0.8880026340484619\n",
      "Step: 5000, Subject: sub1; Training Loss: 0.16994285583496094, Val Loss: 0.5755554437637329\n",
      "Step: 5000, Subject: sub2; Training Loss: 0.16220757365226746, Val Loss: 0.8639730215072632\n",
      "Step: 5000, Subject: sub3; Training Loss: 0.33399003744125366, Val Loss: 0.83655846118927\n",
      "Step: 5000, Subject: sub4; Training Loss: 0.18884879350662231, Val Loss: 0.8888986706733704\n",
      "Step: 5000, Subject: sub5; Training Loss: 0.3467610478401184, Val Loss: 0.7570074796676636\n",
      "Step: 5000, Subject: sub7; Training Loss: 0.2794122099876404, Val Loss: 1.1018648147583008\n"
     ]
    }
   ],
   "source": [
    "mtl_body = MtlNetwork_body2(num_classes = 85)\n",
    "train_mtl_nn(mtl_body, training_data, training_steps_per_subject = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl_body.save_weights(\"nn_weights/generalized-sub6/shared_body\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

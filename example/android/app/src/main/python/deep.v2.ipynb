{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 12:43:32.953325: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import subject_data as sd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import random\n",
    "\n",
    "from random import seed, randint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 85\n",
    "num_features = 100*100*40\n",
    "\n",
    "# Training parameters. Sunday, May 24, 2020 \n",
    "learning_rate = 0.001 # start with 0.001\n",
    "training_steps = 50000\n",
    "batch_size = 16\n",
    "display_step = 50\n",
    "\n",
    "sam = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = (100,100)\n",
    "def get_subject_training_data(subjects, beg, tot):\n",
    "\n",
    "    sub1_data, sub1_ppg = sd.get_sub1_data(beg, tot)\n",
    "    sub2_data, sub2_ppg = sd.get_sub2_data(beg, tot)\n",
    "    sub3_data, sub3_ppg = sd.get_sub3_data(beg, tot)\n",
    "    sub4_data, sub4_ppg = sd.get_sub4_data(beg, tot)\n",
    "    sub5_data, sub5_ppg = sd.get_sub5_data(beg, tot)\n",
    "    sub6_data, sub6_ppg = sd.get_sub6_data(beg, tot)\n",
    "    sub7_data, sub7_ppg = sd.get_sub7_data(beg, tot)\n",
    "\n",
    "    frame_cons = 40 # how many frame to consider at a time\n",
    "    random.seed(1)\n",
    "    rv = np.arange(0, tot - frame_cons, 1)\n",
    "    np.random.shuffle(rv)\n",
    "\n",
    "    def gen_pulR(ppg):\n",
    "        return np.reshape(ppg, [ppg.shape[0],1])\n",
    "\n",
    "    rv =  np.array(rv)\n",
    "    pulR1 = gen_pulR(sub1_ppg)\n",
    "    pulR2 = gen_pulR(sub2_ppg)\n",
    "    pulR3 = gen_pulR(sub3_ppg)\n",
    "    pulR4 = gen_pulR(sub4_ppg)\n",
    "    pulR5 = gen_pulR(sub5_ppg)\n",
    "    pulR6 = gen_pulR(sub6_ppg)\n",
    "    pulR7 = gen_pulR(sub7_ppg)\n",
    "\n",
    "    trainX = {1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
    "    trainY = {1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
    "\n",
    "    sub1_data = sub1_data[:,:,:,np.newaxis]\n",
    "    sub2_data = sub2_data[:,:,:,np.newaxis]\n",
    "    sub3_data = sub3_data[:,:,:,np.newaxis]\n",
    "    sub4_data = sub4_data[:,:,:,np.newaxis]\n",
    "    sub5_data = sub5_data[:,:,:,np.newaxis]\n",
    "    sub6_data = sub6_data[:,:,:,np.newaxis]\n",
    "    sub7_data = sub7_data[:,:,:,np.newaxis]\n",
    "\n",
    "    def im_get(data, i):\n",
    "        img = data[i:i+frame_cons,:,:,0]\n",
    "        if img.shape[0] < frame_cons:\n",
    "            return None\n",
    "        img = np.reshape(img, [frame_cons, *im_size])\n",
    "        img = np.moveaxis(img, 0,-1)\n",
    "        return img\n",
    "\n",
    "    for j, i in enumerate(rv):\n",
    "        \n",
    "        img1 = im_get(sub1_data, i)\n",
    "        img2 = im_get(sub2_data, i)\n",
    "        img3 = im_get(sub3_data, i)\n",
    "        img4 = im_get(sub4_data, i)\n",
    "        img5 = im_get(sub5_data, i)\n",
    "        img6 = im_get(sub6_data, i)\n",
    "        img7 = im_get(sub7_data, i)\n",
    "\n",
    "        if img1 is None or img2 is None or img3 is None or img4 is None or img5 is None or img6 is None or img7 is None:\n",
    "            continue\n",
    "\n",
    "        trainX[1].append(img1)\n",
    "        trainX[2].append(img2)\n",
    "        trainX[3].append(img3)\n",
    "        trainX[4].append(img4)\n",
    "        trainX[5].append(img5)\n",
    "        trainX[6].append(img6)\n",
    "        trainX[7].append(img7)\n",
    "        \n",
    "        p_point = np.int(np.round(i*64/30))\n",
    "        \n",
    "        ppg = pulR1[p_point: p_point+85, 0]\n",
    "        trainY[1].append(ppg)\n",
    "        ppg = pulR2[p_point: p_point+85, 0]\n",
    "        trainY[2].append(ppg)\n",
    "        ppg = pulR3[p_point: p_point+85, 0]\n",
    "        trainY[3].append(ppg)\n",
    "        ppg = pulR4[p_point: p_point+85, 0]\n",
    "        trainY[4].append(ppg)\n",
    "        ppg = pulR5[p_point: p_point+85, 0]\n",
    "        trainY[5].append(ppg)\n",
    "        ppg = pulR6[p_point: p_point+85, 0]\n",
    "        trainY[6].append(ppg)\n",
    "        ppg = pulR7[p_point: p_point+85, 0]\n",
    "        trainY[7].append(ppg)\n",
    "\n",
    "    to_delete = []\n",
    "    for key in trainX:\n",
    "        try:\n",
    "            trainX[key] = np.array(trainX[key], dtype = np.float32)\n",
    "            trainX[key] = (trainX[key]-trainX[key].min())\n",
    "            trainX[key] = trainX[key]/ trainX[key].max()\n",
    "            \n",
    "            trainY[key] = np.array(trainY[key], dtype = np.float32)\n",
    "            trainY[key] = trainY[key] - trainY[key].min(axis = 1)[:, np.newaxis]\n",
    "            trainY[key] = (trainY[key]/(trainY[key].max(axis = 1)[:, np.newaxis]+ 10**-5))*2-1\n",
    "        except:\n",
    "            print(f'Beg: {beg}, Tot: {tot}: had to delete subject {key}')\n",
    "            to_delete.append(key)\n",
    "    for key in to_delete:\n",
    "        del trainX[key]\n",
    "        del trainY[key]\n",
    "\n",
    "\n",
    "    training_data = []\n",
    "    testing_data = []\n",
    "    trXs = []\n",
    "    trYs = []\n",
    "    teXs = []\n",
    "    teYs = []\n",
    "    for key in trainX:\n",
    "        if key in subjects:\n",
    "\n",
    "            trX, teX, trY, teY = train_test_split(trainX[key], trainY[key], \n",
    "                                                test_size = .1, random_state = 42)\n",
    "            trXs.append(trX)\n",
    "            trYs.append(trY)\n",
    "            teXs.append(teX)\n",
    "            teYs.append(teY)\n",
    "\n",
    "            train_data = tf.data.Dataset.from_tensor_slices((trX, trY))\n",
    "            train_data = train_data.repeat().shuffle(buffer_size=100,\n",
    "                                                    seed= 8).batch(batch_size).prefetch(1)        \n",
    "            test_data = tf.data.Dataset.from_tensor_slices((teX, teY))\n",
    "            test_data = test_data.repeat().shuffle(buffer_size=100,\n",
    "                                                    seed= 8).batch(batch_size).prefetch(1)\n",
    "\n",
    "            training_data.append(train_data)\n",
    "            testing_data.append(test_data)\n",
    "\n",
    "    return training_data, testing_data, trXs, trYs, teXs, teYs\n",
    "\n",
    "\n",
    "# training_data, testing_data = get_subject_training_data(100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RootMeanSquareLoss(x,y):\n",
    "    \n",
    "    # pdb.set_trace()  \n",
    "    loss = tf.keras.losses.MSE(y_true = y, y_pred =x)  # initial one\n",
    "    #return tf.reduce_mean(loss)  # some other shape similarity\n",
    "     \n",
    "    loss2 = tf.reduce_mean((tf.math.abs(tf.math.sign(y))-tf.math.sign(tf.math.multiply(x,y))),axis = -1)\n",
    "    # print(loss2.shape)\n",
    "    \n",
    "    # print(tf.reduce_mean(loss), tf.reduce_mean(loss2))\n",
    "    return loss + 0.5*loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(optimizer, neural_net, x,y):    # for the second network varies in head\n",
    "    with tf.GradientTape() as g:\n",
    "        pred =  neural_net(x, training = True) \n",
    "        loss =  RootMeanSquareLoss(y, pred)  # change for mtl\n",
    "    \n",
    "        trainable_variables =  neural_net.trainable_variables\n",
    "        gradients =  g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "            \n",
    "def Val_loss (neural_net, testX, testY):\n",
    "    pred = neural_net(testX, training = False)\n",
    "    loss = RootMeanSquareLoss(testY, pred)\n",
    "    val_loss.append(tf.reduce_mean(loss))\n",
    "    \n",
    "\n",
    "val_loss_chunk_size = 16\n",
    "\n",
    "def train_personalized_network(subject, training_steps, frames_used = 3000, learning_rate = 0.0008):\n",
    "\n",
    "    if not os.path.exists('personalized_weights'):\n",
    "        os.mkdir('personalized_weights')\n",
    "    if not os.path.exists(f'personalized_weights/subject_{subject}'):\n",
    "        os.mkdir(f'personalized_weights/subject_{subject}')\n",
    "    \n",
    "    mtl_body =  MtlNetwork_body()\n",
    "    head1 =  MtlNetwork_head(num_classes)\n",
    "    head2 = MtlNetwork_head(num_classes)\n",
    "    neural_net1 =  tf.keras.Sequential([mtl_body, head1])\n",
    "    neural_net2 =  tf.keras.Sequential([mtl_body, head2])\n",
    "    print('Initialized networks')\n",
    "\n",
    "    optimizer  = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "    train_data, test_data, trXs, trYs, teXs, teYs = get_subject_training_data([subject], 0, frames_used)\n",
    "    train_data = train_data[0]\n",
    "    test_data = test_data[0]\n",
    "    trX = trXs[0]\n",
    "    trY = trYs[0]\n",
    "    teX = teXs[0]\n",
    "    teY = teYs[0]\n",
    "    print('Got training data')\n",
    "    \n",
    "    min_val_loss = float('inf')\n",
    "\n",
    "    print('Starting training')\n",
    "    for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1): \n",
    "        run_optimization(optimizer, neural_net1, batch_x, batch_y)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            \n",
    "            pred = neural_net1(batch_x, training=True)\n",
    "            loss = RootMeanSquareLoss(batch_y, pred)\n",
    "            train_loss.append(tf.reduce_mean(loss))\n",
    "            \n",
    "            tp = np.random.randint(450)\n",
    "            test_X = teX[tp + 0: tp + val_loss_chunk_size]\n",
    "            test_Y = teY[tp + 0: tp + val_loss_chunk_size]\n",
    "            while len(test_X) < val_loss_chunk_size:\n",
    "                tp = np.random.randint(450)\n",
    "                test_X = np.concatenate((test_X, teX[tp + 0: tp + val_loss_chunk_size]), axis = 0)\n",
    "                test_Y = np.concatenate((test_Y, teY[tp + 0: tp + val_loss_chunk_size]), axis = 0)\n",
    "\n",
    "            Val_loss(neural_net1, test_X, test_Y)\n",
    "            current_val_loss = val_loss[-1]\n",
    "            print(\"step: %i, loss: %f val Loss: %f\" % (step, tf.reduce_mean(loss), current_val_loss))\n",
    "            \n",
    "            # Save the model weights if the current validation loss is lower than the previous minimum validation loss\n",
    "            if current_val_loss < min_val_loss:\n",
    "                \n",
    "                min_val_loss = current_val_loss                \n",
    "                print(f'Saving model with validation loss: {min_val_loss}\\n')\n",
    "                neural_net1.save_weights(f'personalized_weights/subject_{subject}/model_weights')\n",
    "    \n",
    "    return neural_net1, neural_net2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelhmorton/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got training data\n",
      "Starting training\n",
      "step: 10, loss: 0.792501 val Loss: 0.803967\n",
      "Saving model with validation loss: 0.803966760635376\n",
      "\n",
      "step: 20, loss: 0.752462 val Loss: 0.747887\n",
      "Saving model with validation loss: 0.7478874325752258\n",
      "\n",
      "step: 30, loss: 0.759078 val Loss: 0.766695\n",
      "step: 40, loss: 0.725795 val Loss: 0.760959\n",
      "step: 50, loss: 0.721435 val Loss: 0.714582\n",
      "Saving model with validation loss: 0.7145822048187256\n",
      "\n",
      "step: 60, loss: 0.743580 val Loss: 0.790933\n",
      "step: 70, loss: 0.730114 val Loss: 0.784839\n",
      "step: 80, loss: 0.768000 val Loss: 0.748211\n",
      "step: 90, loss: 0.700577 val Loss: 0.711948\n",
      "Saving model with validation loss: 0.7119482755661011\n",
      "\n",
      "step: 100, loss: 0.763246 val Loss: 0.757623\n",
      "step: 110, loss: 0.754867 val Loss: 0.790884\n",
      "step: 120, loss: 0.766159 val Loss: 0.813793\n",
      "step: 130, loss: 0.803105 val Loss: 0.716595\n",
      "step: 140, loss: 0.796327 val Loss: 0.717831\n",
      "step: 150, loss: 0.779306 val Loss: 0.817168\n",
      "step: 160, loss: 0.760489 val Loss: 0.760406\n",
      "step: 170, loss: 0.752534 val Loss: 0.770582\n",
      "step: 180, loss: 0.741830 val Loss: 0.714915\n",
      "step: 190, loss: 0.815162 val Loss: 0.726507\n",
      "step: 200, loss: 0.740463 val Loss: 0.769815\n",
      "step: 210, loss: 0.730460 val Loss: 0.741721\n",
      "step: 220, loss: 0.703751 val Loss: 0.734190\n",
      "step: 230, loss: 0.733021 val Loss: 0.778143\n",
      "step: 240, loss: 0.713118 val Loss: 0.789317\n",
      "step: 250, loss: 0.723484 val Loss: 0.788758\n",
      "step: 260, loss: 0.765939 val Loss: 0.750046\n",
      "step: 270, loss: 0.853484 val Loss: 0.817460\n",
      "step: 280, loss: 0.788550 val Loss: 0.749897\n",
      "step: 290, loss: 0.750864 val Loss: 0.684541\n",
      "Saving model with validation loss: 0.684540867805481\n",
      "\n",
      "step: 300, loss: 0.730976 val Loss: 0.756672\n",
      "step: 310, loss: 0.773149 val Loss: 0.777490\n",
      "step: 320, loss: 0.699460 val Loss: 0.784406\n",
      "step: 330, loss: 0.860055 val Loss: 0.750401\n",
      "step: 340, loss: 0.753260 val Loss: 0.729794\n",
      "step: 350, loss: 0.790960 val Loss: 0.772480\n",
      "step: 360, loss: 0.821365 val Loss: 0.781393\n",
      "step: 370, loss: 0.713862 val Loss: 0.707797\n",
      "step: 380, loss: 0.778861 val Loss: 0.767729\n",
      "step: 390, loss: 0.742977 val Loss: 0.751776\n",
      "step: 400, loss: 0.737778 val Loss: 0.756901\n",
      "step: 410, loss: 0.730852 val Loss: 0.780281\n",
      "step: 420, loss: 0.734705 val Loss: 0.709579\n",
      "step: 430, loss: 0.696971 val Loss: 0.833882\n",
      "step: 440, loss: 0.756811 val Loss: 0.725103\n",
      "step: 450, loss: 0.725292 val Loss: 0.733839\n",
      "step: 460, loss: 0.843225 val Loss: 0.754986\n",
      "step: 470, loss: 0.696149 val Loss: 0.758513\n",
      "step: 480, loss: 0.776942 val Loss: 0.740661\n",
      "step: 490, loss: 0.758219 val Loss: 0.744950\n",
      "step: 500, loss: 0.771019 val Loss: 0.794716\n",
      "step: 510, loss: 0.752542 val Loss: 0.748279\n",
      "step: 520, loss: 0.819220 val Loss: 0.781256\n",
      "step: 530, loss: 0.722480 val Loss: 0.762590\n",
      "step: 540, loss: 0.777394 val Loss: 0.745517\n",
      "step: 550, loss: 0.714013 val Loss: 0.818778\n",
      "step: 560, loss: 0.759962 val Loss: 0.780466\n",
      "step: 570, loss: 0.753167 val Loss: 0.749247\n",
      "step: 580, loss: 0.750041 val Loss: 0.778087\n",
      "step: 590, loss: 0.722847 val Loss: 0.734407\n",
      "step: 600, loss: 0.813192 val Loss: 0.697785\n",
      "step: 610, loss: 0.895564 val Loss: 0.796843\n",
      "step: 620, loss: 0.765984 val Loss: 0.792501\n",
      "step: 630, loss: 0.717031 val Loss: 0.769249\n",
      "step: 640, loss: 0.802306 val Loss: 0.773635\n",
      "step: 650, loss: 0.793589 val Loss: 0.780658\n",
      "step: 660, loss: 0.717985 val Loss: 0.732077\n",
      "step: 670, loss: 0.737517 val Loss: 0.808694\n",
      "step: 680, loss: 0.767015 val Loss: 0.720387\n",
      "step: 690, loss: 0.796537 val Loss: 0.789848\n",
      "step: 700, loss: 0.756286 val Loss: 0.748959\n",
      "step: 710, loss: 0.709867 val Loss: 0.733703\n",
      "step: 720, loss: 0.781659 val Loss: 0.803063\n",
      "step: 730, loss: 0.759812 val Loss: 0.687897\n",
      "step: 740, loss: 0.785791 val Loss: 0.686086\n",
      "step: 750, loss: 0.711061 val Loss: 0.741229\n",
      "step: 760, loss: 0.779123 val Loss: 0.773976\n",
      "step: 770, loss: 0.738377 val Loss: 0.779103\n",
      "step: 780, loss: 0.826915 val Loss: 0.683487\n",
      "Saving model with validation loss: 0.683487057685852\n",
      "\n",
      "step: 790, loss: 0.691797 val Loss: 0.772001\n",
      "step: 800, loss: 0.744603 val Loss: 0.724825\n",
      "step: 810, loss: 0.754503 val Loss: 0.805750\n",
      "step: 820, loss: 0.776914 val Loss: 0.756713\n",
      "step: 830, loss: 0.830462 val Loss: 0.739516\n",
      "step: 840, loss: 0.760966 val Loss: 0.770088\n",
      "step: 850, loss: 0.700025 val Loss: 0.768145\n",
      "step: 860, loss: 0.677304 val Loss: 0.772720\n",
      "step: 870, loss: 0.669178 val Loss: 0.770899\n",
      "step: 880, loss: 0.771731 val Loss: 0.819056\n",
      "step: 890, loss: 0.749032 val Loss: 0.833660\n",
      "step: 900, loss: 0.695502 val Loss: 0.740428\n",
      "step: 910, loss: 0.778345 val Loss: 0.758188\n",
      "step: 920, loss: 0.689539 val Loss: 0.791440\n",
      "step: 930, loss: 0.712468 val Loss: 0.755791\n",
      "step: 940, loss: 0.802377 val Loss: 0.770675\n",
      "step: 950, loss: 0.655733 val Loss: 0.718592\n",
      "step: 960, loss: 0.758687 val Loss: 0.816677\n",
      "step: 970, loss: 0.739628 val Loss: 0.757187\n",
      "step: 980, loss: 0.752640 val Loss: 0.777341\n",
      "step: 990, loss: 0.692579 val Loss: 0.735535\n",
      "step: 1000, loss: 0.797071 val Loss: 0.784157\n",
      "step: 1010, loss: 0.744698 val Loss: 0.745065\n",
      "step: 1020, loss: 0.771142 val Loss: 0.810195\n",
      "step: 1030, loss: 0.745064 val Loss: 0.698058\n",
      "step: 1040, loss: 0.744316 val Loss: 0.823274\n",
      "step: 1050, loss: 0.736291 val Loss: 0.803302\n",
      "step: 1060, loss: 0.730220 val Loss: 0.712225\n",
      "step: 1070, loss: 0.738557 val Loss: 0.739495\n",
      "step: 1080, loss: 0.762622 val Loss: 0.758255\n",
      "step: 1090, loss: 0.653433 val Loss: 0.732581\n",
      "step: 1100, loss: 0.685367 val Loss: 0.786584\n",
      "step: 1110, loss: 0.829982 val Loss: 0.784100\n",
      "step: 1120, loss: 0.767800 val Loss: 0.662141\n",
      "Saving model with validation loss: 0.6621414422988892\n",
      "\n",
      "step: 1130, loss: 0.754184 val Loss: 0.680625\n",
      "step: 1140, loss: 0.780702 val Loss: 0.736929\n",
      "step: 1150, loss: 0.747291 val Loss: 0.765596\n",
      "step: 1160, loss: 0.771142 val Loss: 0.670116\n",
      "step: 1170, loss: 0.795408 val Loss: 0.747674\n",
      "step: 1180, loss: 0.731906 val Loss: 0.791722\n",
      "step: 1190, loss: 0.780825 val Loss: 0.689868\n",
      "step: 1200, loss: 0.736410 val Loss: 0.797474\n",
      "step: 1210, loss: 0.680351 val Loss: 0.810100\n",
      "step: 1220, loss: 0.720095 val Loss: 0.783659\n",
      "step: 1230, loss: 0.734232 val Loss: 0.794997\n",
      "step: 1240, loss: 0.773940 val Loss: 0.725741\n",
      "step: 1250, loss: 0.760815 val Loss: 0.776956\n",
      "step: 1260, loss: 0.720966 val Loss: 0.690637\n",
      "step: 1270, loss: 0.803637 val Loss: 0.741750\n",
      "step: 1280, loss: 0.736496 val Loss: 0.767570\n",
      "step: 1290, loss: 0.743010 val Loss: 0.738809\n",
      "step: 1300, loss: 0.709538 val Loss: 0.750372\n",
      "step: 1310, loss: 0.791178 val Loss: 0.760560\n",
      "step: 1320, loss: 0.716513 val Loss: 0.794592\n",
      "step: 1330, loss: 0.778805 val Loss: 0.783155\n",
      "step: 1340, loss: 0.798993 val Loss: 0.732391\n",
      "step: 1350, loss: 0.797651 val Loss: 0.738956\n",
      "step: 1360, loss: 0.761133 val Loss: 0.671266\n",
      "step: 1370, loss: 0.744127 val Loss: 0.734857\n",
      "step: 1380, loss: 0.788095 val Loss: 0.739127\n",
      "step: 1390, loss: 0.750497 val Loss: 0.741210\n",
      "step: 1400, loss: 0.752499 val Loss: 0.758630\n",
      "step: 1410, loss: 0.793603 val Loss: 0.755354\n",
      "step: 1420, loss: 0.786040 val Loss: 0.769200\n",
      "step: 1430, loss: 0.773288 val Loss: 0.737969\n",
      "step: 1440, loss: 0.796774 val Loss: 0.723986\n",
      "step: 1450, loss: 0.715335 val Loss: 0.749645\n",
      "step: 1460, loss: 0.738062 val Loss: 0.664535\n",
      "step: 1470, loss: 0.615727 val Loss: 0.797019\n",
      "step: 1480, loss: 0.859569 val Loss: 0.750545\n",
      "step: 1490, loss: 0.706382 val Loss: 0.730618\n",
      "step: 1500, loss: 0.718097 val Loss: 0.753586\n",
      "step: 1510, loss: 0.783147 val Loss: 0.774830\n",
      "step: 1520, loss: 0.794005 val Loss: 0.751220\n",
      "step: 1530, loss: 0.769604 val Loss: 0.739202\n",
      "step: 1540, loss: 0.733820 val Loss: 0.720939\n",
      "step: 1550, loss: 0.799923 val Loss: 0.700906\n",
      "step: 1560, loss: 0.782318 val Loss: 0.749602\n",
      "step: 1570, loss: 0.790819 val Loss: 0.736055\n",
      "step: 1580, loss: 0.781612 val Loss: 0.757690\n",
      "step: 1590, loss: 0.739160 val Loss: 0.754866\n",
      "step: 1600, loss: 0.795348 val Loss: 0.770648\n",
      "step: 1610, loss: 0.736636 val Loss: 0.725606\n",
      "step: 1620, loss: 0.807758 val Loss: 0.741941\n",
      "step: 1630, loss: 0.790927 val Loss: 0.687276\n",
      "step: 1640, loss: 0.777508 val Loss: 0.728281\n",
      "step: 1650, loss: 0.696053 val Loss: 0.744245\n",
      "step: 1660, loss: 0.780366 val Loss: 0.697991\n",
      "step: 1670, loss: 0.778340 val Loss: 0.787181\n",
      "step: 1680, loss: 0.787737 val Loss: 0.759332\n",
      "step: 1690, loss: 0.711982 val Loss: 0.778153\n",
      "step: 1700, loss: 0.702056 val Loss: 0.784525\n",
      "step: 1710, loss: 0.747414 val Loss: 0.759122\n",
      "step: 1720, loss: 0.782340 val Loss: 0.751481\n",
      "step: 1730, loss: 0.704739 val Loss: 0.674368\n",
      "step: 1740, loss: 0.726048 val Loss: 0.756423\n",
      "step: 1750, loss: 0.752432 val Loss: 0.701152\n",
      "step: 1760, loss: 0.759400 val Loss: 0.751868\n",
      "step: 1770, loss: 0.729266 val Loss: 0.770353\n",
      "step: 1780, loss: 0.757643 val Loss: 0.786785\n",
      "step: 1790, loss: 0.758165 val Loss: 0.682376\n",
      "step: 1800, loss: 0.698914 val Loss: 0.764630\n",
      "step: 1810, loss: 0.735609 val Loss: 0.793331\n",
      "step: 1820, loss: 0.776285 val Loss: 0.740334\n",
      "step: 1830, loss: 0.818478 val Loss: 0.732072\n",
      "step: 1840, loss: 0.787391 val Loss: 0.794603\n",
      "step: 1850, loss: 0.760877 val Loss: 0.737944\n",
      "step: 1860, loss: 0.761335 val Loss: 0.740886\n",
      "step: 1870, loss: 0.676466 val Loss: 0.759314\n",
      "step: 1880, loss: 0.834673 val Loss: 0.759113\n",
      "step: 1890, loss: 0.823396 val Loss: 0.732477\n",
      "step: 1900, loss: 0.731486 val Loss: 0.758763\n",
      "step: 1910, loss: 0.754113 val Loss: 0.752394\n",
      "step: 1920, loss: 0.667713 val Loss: 0.789397\n",
      "step: 1930, loss: 0.731346 val Loss: 0.742055\n",
      "step: 1940, loss: 0.814049 val Loss: 0.813810\n",
      "step: 1950, loss: 0.768769 val Loss: 0.768493\n",
      "step: 1960, loss: 0.800434 val Loss: 0.675444\n",
      "step: 1970, loss: 0.648069 val Loss: 0.808580\n",
      "step: 1980, loss: 0.767416 val Loss: 0.755857\n",
      "step: 1990, loss: 0.726105 val Loss: 0.796052\n",
      "step: 2000, loss: 0.807825 val Loss: 0.784973\n",
      "step: 2010, loss: 0.771143 val Loss: 0.692820\n",
      "step: 2020, loss: 0.731177 val Loss: 0.768580\n",
      "step: 2030, loss: 0.752488 val Loss: 0.751644\n",
      "step: 2040, loss: 0.755720 val Loss: 0.718420\n",
      "step: 2050, loss: 0.820904 val Loss: 0.761085\n",
      "step: 2060, loss: 0.847317 val Loss: 0.696513\n",
      "step: 2070, loss: 0.704910 val Loss: 0.680657\n",
      "step: 2080, loss: 0.712869 val Loss: 0.781210\n",
      "step: 2090, loss: 0.640035 val Loss: 0.754934\n",
      "step: 2100, loss: 0.745197 val Loss: 0.787017\n",
      "step: 2110, loss: 0.820610 val Loss: 0.664110\n",
      "step: 2120, loss: 0.699176 val Loss: 0.756953\n",
      "step: 2130, loss: 0.829340 val Loss: 0.674272\n",
      "step: 2140, loss: 0.708326 val Loss: 0.809423\n",
      "step: 2150, loss: 0.799549 val Loss: 0.683520\n",
      "step: 2160, loss: 0.740761 val Loss: 0.839142\n",
      "step: 2170, loss: 0.792317 val Loss: 0.711114\n",
      "step: 2180, loss: 0.721823 val Loss: 0.788279\n",
      "step: 2190, loss: 0.768222 val Loss: 0.784882\n",
      "step: 2200, loss: 0.691751 val Loss: 0.677575\n",
      "step: 2210, loss: 0.694987 val Loss: 0.820634\n",
      "step: 2220, loss: 0.661743 val Loss: 0.755360\n",
      "step: 2230, loss: 0.784395 val Loss: 0.707658\n",
      "step: 2240, loss: 0.751517 val Loss: 0.749838\n",
      "step: 2250, loss: 0.747112 val Loss: 0.778004\n",
      "step: 2260, loss: 0.721555 val Loss: 0.746991\n",
      "step: 2270, loss: 0.733564 val Loss: 0.758711\n",
      "step: 2280, loss: 0.732489 val Loss: 0.803553\n",
      "step: 2290, loss: 0.751636 val Loss: 0.808785\n",
      "step: 2300, loss: 0.705553 val Loss: 0.769729\n",
      "step: 2310, loss: 0.844212 val Loss: 0.668125\n",
      "step: 2320, loss: 0.684258 val Loss: 0.685069\n",
      "step: 2330, loss: 0.769031 val Loss: 0.749823\n",
      "step: 2340, loss: 0.707396 val Loss: 0.745727\n",
      "step: 2350, loss: 0.797104 val Loss: 0.691998\n",
      "step: 2360, loss: 0.715298 val Loss: 0.749343\n",
      "step: 2370, loss: 0.719038 val Loss: 0.753346\n",
      "step: 2380, loss: 0.789731 val Loss: 0.719475\n",
      "step: 2390, loss: 0.764452 val Loss: 0.800279\n",
      "step: 2400, loss: 0.796191 val Loss: 0.753726\n",
      "step: 2410, loss: 0.761489 val Loss: 0.750905\n",
      "step: 2420, loss: 0.745967 val Loss: 0.789842\n",
      "step: 2430, loss: 0.793778 val Loss: 0.676482\n",
      "step: 2440, loss: 0.760400 val Loss: 0.736680\n",
      "step: 2450, loss: 0.741205 val Loss: 0.759705\n",
      "step: 2460, loss: 0.796903 val Loss: 0.685601\n",
      "step: 2470, loss: 0.808506 val Loss: 0.745222\n",
      "step: 2480, loss: 0.760619 val Loss: 0.752272\n",
      "step: 2490, loss: 0.792668 val Loss: 0.783224\n",
      "step: 2500, loss: 0.897560 val Loss: 0.725577\n",
      "step: 2510, loss: 0.753197 val Loss: 0.744048\n",
      "step: 2520, loss: 0.845545 val Loss: 0.789547\n",
      "step: 2530, loss: 0.708503 val Loss: 0.739264\n",
      "step: 2540, loss: 0.777536 val Loss: 0.744269\n",
      "step: 2550, loss: 0.681729 val Loss: 0.755872\n",
      "step: 2560, loss: 0.753966 val Loss: 0.789779\n",
      "step: 2570, loss: 0.715057 val Loss: 0.800614\n",
      "step: 2580, loss: 0.678413 val Loss: 0.747989\n",
      "step: 2590, loss: 0.705374 val Loss: 0.790350\n",
      "step: 2600, loss: 0.883026 val Loss: 0.747500\n",
      "step: 2610, loss: 0.805538 val Loss: 0.750415\n",
      "step: 2620, loss: 0.776349 val Loss: 0.760390\n",
      "step: 2630, loss: 0.782770 val Loss: 0.688781\n",
      "step: 2640, loss: 0.746243 val Loss: 0.666537\n",
      "step: 2650, loss: 0.758391 val Loss: 0.664424\n",
      "step: 2660, loss: 0.807226 val Loss: 0.742700\n",
      "step: 2670, loss: 0.750833 val Loss: 0.791660\n",
      "step: 2680, loss: 0.735842 val Loss: 0.797747\n",
      "step: 2690, loss: 0.794086 val Loss: 0.720568\n",
      "step: 2700, loss: 0.741085 val Loss: 0.762311\n",
      "step: 2710, loss: 0.766622 val Loss: 0.725158\n",
      "step: 2720, loss: 0.723695 val Loss: 0.785641\n",
      "step: 2730, loss: 0.760889 val Loss: 0.721759\n",
      "step: 2740, loss: 0.720037 val Loss: 0.784033\n",
      "step: 2750, loss: 0.706150 val Loss: 0.689183\n",
      "step: 2760, loss: 0.721341 val Loss: 0.783748\n",
      "step: 2770, loss: 0.782424 val Loss: 0.774836\n",
      "step: 2780, loss: 0.770919 val Loss: 0.784166\n",
      "step: 2790, loss: 0.753743 val Loss: 0.699181\n",
      "step: 2800, loss: 0.690621 val Loss: 0.792333\n",
      "step: 2810, loss: 0.824058 val Loss: 0.800578\n",
      "step: 2820, loss: 0.756714 val Loss: 0.770399\n",
      "step: 2830, loss: 0.754963 val Loss: 0.794618\n",
      "step: 2840, loss: 0.715455 val Loss: 0.684811\n",
      "step: 2850, loss: 0.781740 val Loss: 0.785589\n",
      "step: 2860, loss: 0.786754 val Loss: 0.698112\n",
      "step: 2870, loss: 0.699444 val Loss: 0.729689\n",
      "step: 2880, loss: 0.789343 val Loss: 0.761510\n",
      "step: 2890, loss: 0.802442 val Loss: 0.744180\n",
      "step: 2900, loss: 0.759909 val Loss: 0.687801\n",
      "step: 2910, loss: 0.725455 val Loss: 0.796624\n",
      "step: 2920, loss: 0.723279 val Loss: 0.822639\n",
      "step: 2930, loss: 0.776170 val Loss: 0.765588\n",
      "step: 2940, loss: 0.797209 val Loss: 0.805667\n",
      "step: 2950, loss: 0.769866 val Loss: 0.778953\n",
      "step: 2960, loss: 0.739073 val Loss: 0.751209\n",
      "step: 2970, loss: 0.750205 val Loss: 0.716387\n",
      "step: 2980, loss: 0.773195 val Loss: 0.746486\n",
      "step: 2990, loss: 0.805003 val Loss: 0.792982\n",
      "step: 3000, loss: 0.864607 val Loss: 0.755783\n",
      "step: 3010, loss: 0.691817 val Loss: 0.766527\n",
      "step: 3020, loss: 0.733082 val Loss: 0.796473\n",
      "step: 3030, loss: 0.773127 val Loss: 0.686579\n",
      "step: 3040, loss: 0.801368 val Loss: 0.738538\n",
      "step: 3050, loss: 0.762900 val Loss: 0.728907\n",
      "step: 3060, loss: 0.760226 val Loss: 0.761019\n",
      "step: 3070, loss: 0.757308 val Loss: 0.767767\n",
      "step: 3080, loss: 0.756669 val Loss: 0.692145\n",
      "step: 3090, loss: 0.722004 val Loss: 0.704645\n",
      "step: 3100, loss: 0.755249 val Loss: 0.735899\n",
      "step: 3110, loss: 0.747134 val Loss: 0.664263\n",
      "step: 3120, loss: 0.750921 val Loss: 0.754768\n",
      "step: 3130, loss: 0.775521 val Loss: 0.797631\n",
      "step: 3140, loss: 0.845313 val Loss: 0.779209\n",
      "step: 3150, loss: 0.768851 val Loss: 0.680376\n",
      "step: 3160, loss: 0.775156 val Loss: 0.728466\n",
      "step: 3170, loss: 0.711579 val Loss: 0.770854\n",
      "step: 3180, loss: 0.808428 val Loss: 0.693477\n",
      "step: 3190, loss: 0.752148 val Loss: 0.709901\n",
      "step: 3200, loss: 0.731736 val Loss: 0.766138\n",
      "step: 3210, loss: 0.708204 val Loss: 0.686389\n",
      "step: 3220, loss: 0.813663 val Loss: 0.734761\n",
      "step: 3230, loss: 0.728679 val Loss: 0.738173\n",
      "step: 3240, loss: 0.733625 val Loss: 0.831770\n",
      "step: 3250, loss: 0.786772 val Loss: 0.753969\n",
      "step: 3260, loss: 0.793995 val Loss: 0.763814\n",
      "step: 3270, loss: 0.657620 val Loss: 0.779049\n",
      "step: 3280, loss: 0.803424 val Loss: 0.754899\n",
      "step: 3290, loss: 0.779748 val Loss: 0.851741\n",
      "step: 3300, loss: 0.724982 val Loss: 0.671746\n",
      "step: 3310, loss: 0.796651 val Loss: 0.719976\n",
      "step: 3320, loss: 0.744917 val Loss: 0.668006\n",
      "step: 3330, loss: 0.802568 val Loss: 0.696331\n",
      "step: 3340, loss: 0.799850 val Loss: 0.745740\n",
      "step: 3350, loss: 0.704944 val Loss: 0.755050\n",
      "step: 3360, loss: 0.806752 val Loss: 0.741217\n",
      "step: 3370, loss: 0.783611 val Loss: 0.763976\n",
      "step: 3380, loss: 0.752690 val Loss: 0.734003\n",
      "step: 3390, loss: 0.801049 val Loss: 0.755949\n",
      "step: 3400, loss: 0.778033 val Loss: 0.660683\n",
      "Saving model with validation loss: 0.6606825590133667\n",
      "\n",
      "step: 3410, loss: 0.729736 val Loss: 0.764103\n",
      "step: 3420, loss: 0.745076 val Loss: 0.739047\n",
      "step: 3430, loss: 0.732942 val Loss: 0.717250\n",
      "step: 3440, loss: 0.728267 val Loss: 0.772992\n",
      "step: 3450, loss: 0.731675 val Loss: 0.749029\n",
      "step: 3460, loss: 0.739947 val Loss: 0.777752\n",
      "step: 3470, loss: 0.692112 val Loss: 0.776675\n",
      "step: 3480, loss: 0.807878 val Loss: 0.729869\n",
      "step: 3490, loss: 0.711168 val Loss: 0.762557\n",
      "step: 3500, loss: 0.867617 val Loss: 0.805420\n",
      "step: 3510, loss: 0.756973 val Loss: 0.739728\n",
      "step: 3520, loss: 0.781694 val Loss: 0.765621\n",
      "step: 3530, loss: 0.747070 val Loss: 0.670660\n",
      "step: 3540, loss: 0.759682 val Loss: 0.764072\n",
      "step: 3550, loss: 0.767715 val Loss: 0.784524\n",
      "step: 3560, loss: 0.746282 val Loss: 0.729255\n",
      "step: 3570, loss: 0.743887 val Loss: 0.780714\n",
      "step: 3580, loss: 0.771590 val Loss: 0.764026\n",
      "step: 3590, loss: 0.721866 val Loss: 0.808274\n",
      "step: 3600, loss: 0.841138 val Loss: 0.760676\n",
      "step: 3610, loss: 0.803958 val Loss: 0.754687\n",
      "step: 3620, loss: 0.811041 val Loss: 0.726455\n",
      "step: 3630, loss: 0.739512 val Loss: 0.738151\n",
      "step: 3640, loss: 0.790324 val Loss: 0.745644\n",
      "step: 3650, loss: 0.720146 val Loss: 0.804845\n",
      "step: 3660, loss: 0.799121 val Loss: 0.732075\n",
      "step: 3670, loss: 0.810312 val Loss: 0.685076\n",
      "step: 3680, loss: 0.671831 val Loss: 0.748762\n",
      "step: 3690, loss: 0.719223 val Loss: 0.773109\n",
      "step: 3700, loss: 0.743137 val Loss: 0.834374\n",
      "step: 3710, loss: 0.808763 val Loss: 0.664575\n",
      "step: 3720, loss: 0.722841 val Loss: 0.699461\n",
      "step: 3730, loss: 0.729156 val Loss: 0.850561\n",
      "step: 3740, loss: 0.743520 val Loss: 0.752971\n",
      "step: 3750, loss: 0.778398 val Loss: 0.749320\n",
      "step: 3760, loss: 0.805508 val Loss: 0.735808\n",
      "step: 3770, loss: 0.835217 val Loss: 0.757124\n",
      "step: 3780, loss: 0.775059 val Loss: 0.806806\n",
      "step: 3790, loss: 0.786998 val Loss: 0.751713\n",
      "step: 3800, loss: 0.724289 val Loss: 0.728755\n",
      "step: 3810, loss: 0.815323 val Loss: 0.729297\n",
      "step: 3820, loss: 0.679993 val Loss: 0.681011\n",
      "step: 3830, loss: 0.805213 val Loss: 0.798565\n",
      "step: 3840, loss: 0.746019 val Loss: 0.739706\n",
      "step: 3850, loss: 0.794822 val Loss: 0.767679\n",
      "step: 3860, loss: 0.735053 val Loss: 0.771911\n",
      "step: 3870, loss: 0.741393 val Loss: 0.764487\n",
      "step: 3880, loss: 0.722976 val Loss: 0.751521\n",
      "step: 3890, loss: 0.832991 val Loss: 0.751444\n",
      "step: 3900, loss: 0.716153 val Loss: 0.752571\n",
      "step: 3910, loss: 0.769567 val Loss: 0.792715\n",
      "step: 3920, loss: 0.674515 val Loss: 0.676257\n",
      "step: 3930, loss: 0.751488 val Loss: 0.791474\n",
      "step: 3940, loss: 0.858638 val Loss: 0.735384\n",
      "step: 3950, loss: 0.722486 val Loss: 0.651390\n",
      "Saving model with validation loss: 0.6513903141021729\n",
      "\n",
      "step: 3960, loss: 0.806719 val Loss: 0.708119\n",
      "step: 3970, loss: 0.813675 val Loss: 0.787103\n",
      "step: 3980, loss: 0.754422 val Loss: 0.728538\n",
      "step: 3990, loss: 0.733085 val Loss: 0.750681\n",
      "step: 4000, loss: 0.735467 val Loss: 0.734190\n",
      "step: 4010, loss: 0.725431 val Loss: 0.774911\n",
      "step: 4020, loss: 0.752649 val Loss: 0.805914\n",
      "step: 4030, loss: 0.778142 val Loss: 0.757619\n",
      "step: 4040, loss: 0.711117 val Loss: 0.677826\n",
      "step: 4050, loss: 0.756717 val Loss: 0.668254\n",
      "step: 4060, loss: 0.720347 val Loss: 0.781994\n",
      "step: 4070, loss: 0.781325 val Loss: 0.750417\n",
      "step: 4080, loss: 0.746030 val Loss: 0.779667\n",
      "step: 4090, loss: 0.739510 val Loss: 0.803470\n",
      "step: 4100, loss: 0.783982 val Loss: 0.766248\n",
      "step: 4110, loss: 0.728778 val Loss: 0.750446\n",
      "step: 4120, loss: 0.758229 val Loss: 0.738379\n",
      "step: 4130, loss: 0.688236 val Loss: 0.674249\n",
      "step: 4140, loss: 0.714617 val Loss: 0.725584\n",
      "step: 4150, loss: 0.704885 val Loss: 0.765688\n",
      "step: 4160, loss: 0.843759 val Loss: 0.744639\n",
      "step: 4170, loss: 0.802839 val Loss: 0.790075\n",
      "step: 4180, loss: 0.727123 val Loss: 0.734103\n",
      "step: 4190, loss: 0.691687 val Loss: 0.792662\n",
      "step: 4200, loss: 0.744488 val Loss: 0.734609\n",
      "step: 4210, loss: 0.760496 val Loss: 0.756306\n",
      "step: 4220, loss: 0.768715 val Loss: 0.671903\n",
      "step: 4230, loss: 0.761877 val Loss: 0.732831\n",
      "step: 4240, loss: 0.779187 val Loss: 0.659715\n",
      "step: 4250, loss: 0.716442 val Loss: 0.738047\n",
      "step: 4260, loss: 0.782096 val Loss: 0.793196\n",
      "step: 4270, loss: 0.775099 val Loss: 0.715879\n",
      "step: 4280, loss: 0.788750 val Loss: 0.743397\n",
      "step: 4290, loss: 0.714312 val Loss: 0.793042\n",
      "step: 4300, loss: 0.761788 val Loss: 0.748723\n",
      "step: 4310, loss: 0.754347 val Loss: 0.686585\n",
      "step: 4320, loss: 0.801142 val Loss: 0.720485\n",
      "step: 4330, loss: 0.819352 val Loss: 0.733661\n",
      "step: 4340, loss: 0.759608 val Loss: 0.778832\n",
      "step: 4350, loss: 0.734884 val Loss: 0.688038\n",
      "step: 4360, loss: 0.811042 val Loss: 0.751248\n",
      "step: 4370, loss: 0.692955 val Loss: 0.759146\n",
      "step: 4380, loss: 0.693427 val Loss: 0.771348\n",
      "step: 4390, loss: 0.732771 val Loss: 0.764822\n",
      "step: 4400, loss: 0.764752 val Loss: 0.732709\n",
      "step: 4410, loss: 0.755906 val Loss: 0.768128\n",
      "step: 4420, loss: 0.731201 val Loss: 0.796599\n",
      "step: 4430, loss: 0.795310 val Loss: 0.723503\n",
      "step: 4440, loss: 0.804710 val Loss: 0.747008\n",
      "step: 4450, loss: 0.797532 val Loss: 0.742164\n",
      "step: 4460, loss: 0.734248 val Loss: 0.781717\n",
      "step: 4470, loss: 0.799446 val Loss: 0.682594\n",
      "step: 4480, loss: 0.788527 val Loss: 0.798018\n",
      "step: 4490, loss: 0.731982 val Loss: 0.770943\n",
      "step: 4500, loss: 0.786950 val Loss: 0.756199\n",
      "step: 4510, loss: 0.711405 val Loss: 0.739738\n",
      "step: 4520, loss: 0.775125 val Loss: 0.702261\n",
      "step: 4530, loss: 0.765499 val Loss: 0.693095\n",
      "step: 4540, loss: 0.761326 val Loss: 0.777592\n",
      "step: 4550, loss: 0.748600 val Loss: 0.701964\n",
      "step: 4560, loss: 0.716851 val Loss: 0.777599\n",
      "step: 4570, loss: 0.778544 val Loss: 0.728406\n",
      "step: 4580, loss: 0.697451 val Loss: 0.692594\n",
      "step: 4590, loss: 0.717400 val Loss: 0.763462\n",
      "step: 4600, loss: 0.761646 val Loss: 0.780569\n",
      "step: 4610, loss: 0.709250 val Loss: 0.753416\n",
      "step: 4620, loss: 0.757583 val Loss: 0.720205\n",
      "step: 4630, loss: 0.709127 val Loss: 0.757116\n",
      "step: 4640, loss: 0.702464 val Loss: 0.776604\n",
      "step: 4650, loss: 0.785800 val Loss: 0.805052\n",
      "step: 4660, loss: 0.775016 val Loss: 0.762478\n",
      "step: 4670, loss: 0.799893 val Loss: 0.717146\n",
      "step: 4680, loss: 0.754291 val Loss: 0.785277\n",
      "step: 4690, loss: 0.789625 val Loss: 0.744947\n",
      "step: 4700, loss: 0.726085 val Loss: 0.717822\n",
      "step: 4710, loss: 0.713724 val Loss: 0.772093\n",
      "step: 4720, loss: 0.722215 val Loss: 0.758338\n",
      "step: 4730, loss: 0.710381 val Loss: 0.714623\n",
      "step: 4740, loss: 0.766103 val Loss: 0.689616\n",
      "step: 4750, loss: 0.746573 val Loss: 0.776380\n",
      "step: 4760, loss: 0.764347 val Loss: 0.786985\n",
      "step: 4770, loss: 0.752883 val Loss: 0.777187\n",
      "step: 4780, loss: 0.772174 val Loss: 0.708101\n",
      "step: 4790, loss: 0.752472 val Loss: 0.738865\n",
      "step: 4800, loss: 0.790173 val Loss: 0.755732\n",
      "step: 4810, loss: 0.845758 val Loss: 0.738420\n",
      "step: 4820, loss: 0.774657 val Loss: 0.680527\n",
      "step: 4830, loss: 0.843523 val Loss: 0.679240\n",
      "step: 4840, loss: 0.735789 val Loss: 0.770289\n",
      "step: 4850, loss: 0.719980 val Loss: 0.803788\n",
      "step: 4860, loss: 0.793959 val Loss: 0.761088\n",
      "step: 4870, loss: 0.722208 val Loss: 0.805370\n",
      "step: 4880, loss: 0.809059 val Loss: 0.771299\n",
      "step: 4890, loss: 0.702840 val Loss: 0.792140\n",
      "step: 4900, loss: 0.683299 val Loss: 0.729036\n",
      "step: 4910, loss: 0.726700 val Loss: 0.795051\n",
      "step: 4920, loss: 0.750026 val Loss: 0.740583\n",
      "step: 4930, loss: 0.714021 val Loss: 0.665390\n",
      "step: 4940, loss: 0.727729 val Loss: 0.769134\n",
      "step: 4950, loss: 0.823034 val Loss: 0.761063\n",
      "step: 4960, loss: 0.709658 val Loss: 0.756197\n",
      "step: 4970, loss: 0.799092 val Loss: 0.782884\n",
      "step: 4980, loss: 0.785572 val Loss: 0.790449\n",
      "step: 4990, loss: 0.750474 val Loss: 0.738900\n",
      "step: 5000, loss: 0.842494 val Loss: 0.694025\n",
      "step: 5010, loss: 0.757201 val Loss: 0.779826\n",
      "step: 5020, loss: 0.682544 val Loss: 0.792832\n",
      "step: 5030, loss: 0.708816 val Loss: 0.783956\n",
      "step: 5040, loss: 0.769407 val Loss: 0.670531\n",
      "step: 5050, loss: 0.705507 val Loss: 0.757100\n",
      "step: 5060, loss: 0.772237 val Loss: 0.816685\n",
      "step: 5070, loss: 0.717927 val Loss: 0.764724\n",
      "step: 5080, loss: 0.679756 val Loss: 0.813329\n",
      "step: 5090, loss: 0.735812 val Loss: 0.683541\n",
      "step: 5100, loss: 0.795061 val Loss: 0.812353\n",
      "step: 5110, loss: 0.819646 val Loss: 0.746185\n",
      "step: 5120, loss: 0.663128 val Loss: 0.750495\n",
      "step: 5130, loss: 0.687077 val Loss: 0.666105\n",
      "step: 5140, loss: 0.753847 val Loss: 0.765708\n",
      "step: 5150, loss: 0.784810 val Loss: 0.807019\n",
      "step: 5160, loss: 0.759249 val Loss: 0.695770\n",
      "step: 5170, loss: 0.728630 val Loss: 0.733028\n",
      "step: 5180, loss: 0.784055 val Loss: 0.741686\n",
      "step: 5190, loss: 0.819650 val Loss: 0.696012\n",
      "step: 5200, loss: 0.716201 val Loss: 0.722664\n",
      "step: 5210, loss: 0.742194 val Loss: 0.757416\n",
      "step: 5220, loss: 0.777714 val Loss: 0.752510\n",
      "step: 5230, loss: 0.815685 val Loss: 0.809078\n",
      "step: 5240, loss: 0.779870 val Loss: 0.831690\n",
      "step: 5250, loss: 0.714749 val Loss: 0.740818\n",
      "step: 5260, loss: 0.701927 val Loss: 0.731704\n",
      "step: 5270, loss: 0.757243 val Loss: 0.699284\n",
      "step: 5280, loss: 0.863441 val Loss: 0.731633\n",
      "step: 5290, loss: 0.761054 val Loss: 0.704192\n",
      "step: 5300, loss: 0.841940 val Loss: 0.803505\n",
      "step: 5310, loss: 0.756504 val Loss: 0.812583\n",
      "step: 5320, loss: 0.743039 val Loss: 0.728133\n",
      "step: 5330, loss: 0.722707 val Loss: 0.783325\n",
      "step: 5340, loss: 0.764902 val Loss: 0.793940\n",
      "step: 5350, loss: 0.793159 val Loss: 0.738522\n",
      "step: 5360, loss: 0.707612 val Loss: 0.743791\n",
      "step: 5370, loss: 0.695055 val Loss: 0.769087\n",
      "step: 5380, loss: 0.791581 val Loss: 0.784931\n",
      "step: 5390, loss: 0.719843 val Loss: 0.746824\n",
      "step: 5400, loss: 0.753987 val Loss: 0.704736\n",
      "step: 5410, loss: 0.808503 val Loss: 0.745362\n",
      "step: 5420, loss: 0.687320 val Loss: 0.684967\n",
      "step: 5430, loss: 0.810757 val Loss: 0.758675\n",
      "step: 5440, loss: 0.836984 val Loss: 0.754904\n",
      "step: 5450, loss: 0.749211 val Loss: 0.709290\n",
      "step: 5460, loss: 0.721624 val Loss: 0.768689\n",
      "step: 5470, loss: 0.774282 val Loss: 0.724578\n",
      "step: 5480, loss: 0.705039 val Loss: 0.761088\n",
      "step: 5490, loss: 0.779226 val Loss: 0.762046\n",
      "step: 5500, loss: 0.709645 val Loss: 0.764422\n",
      "step: 5510, loss: 0.750228 val Loss: 0.794795\n",
      "step: 5520, loss: 0.825588 val Loss: 0.757517\n",
      "step: 5530, loss: 0.761290 val Loss: 0.792050\n",
      "step: 5540, loss: 0.795431 val Loss: 0.699711\n",
      "step: 5550, loss: 0.807226 val Loss: 0.697604\n",
      "step: 5560, loss: 0.761209 val Loss: 0.779281\n",
      "step: 5570, loss: 0.745593 val Loss: 0.702383\n",
      "step: 5580, loss: 0.734240 val Loss: 0.801840\n",
      "step: 5590, loss: 0.731009 val Loss: 0.801045\n",
      "step: 5600, loss: 0.692145 val Loss: 0.743354\n",
      "step: 5610, loss: 0.836996 val Loss: 0.767136\n",
      "step: 5620, loss: 0.765723 val Loss: 0.747913\n",
      "step: 5630, loss: 0.805029 val Loss: 0.685218\n",
      "step: 5640, loss: 0.701283 val Loss: 0.729534\n",
      "step: 5650, loss: 0.758058 val Loss: 0.761267\n",
      "step: 5660, loss: 0.856269 val Loss: 0.742663\n",
      "step: 5670, loss: 0.752902 val Loss: 0.781316\n",
      "step: 5680, loss: 0.747718 val Loss: 0.803664\n",
      "step: 5690, loss: 0.689254 val Loss: 0.777185\n",
      "step: 5700, loss: 0.682864 val Loss: 0.750034\n",
      "step: 5710, loss: 0.784192 val Loss: 0.740607\n",
      "step: 5720, loss: 0.693615 val Loss: 0.778929\n",
      "step: 5730, loss: 0.716066 val Loss: 0.703623\n",
      "step: 5740, loss: 0.757293 val Loss: 0.777795\n",
      "step: 5750, loss: 0.725846 val Loss: 0.724247\n",
      "step: 5760, loss: 0.796797 val Loss: 0.812244\n",
      "step: 5770, loss: 0.744894 val Loss: 0.796956\n",
      "step: 5780, loss: 0.749074 val Loss: 0.699847\n",
      "step: 5790, loss: 0.722392 val Loss: 0.712492\n",
      "step: 5800, loss: 0.760315 val Loss: 0.757463\n",
      "step: 5810, loss: 0.794826 val Loss: 0.732092\n",
      "step: 5820, loss: 0.739976 val Loss: 0.743517\n",
      "step: 5830, loss: 0.765981 val Loss: 0.672823\n",
      "step: 5840, loss: 0.764498 val Loss: 0.714227\n",
      "step: 5850, loss: 0.752738 val Loss: 0.807149\n",
      "step: 5860, loss: 0.803582 val Loss: 0.802943\n",
      "step: 5870, loss: 0.746222 val Loss: 0.687356\n",
      "step: 5880, loss: 0.718345 val Loss: 0.754966\n",
      "step: 5890, loss: 0.720664 val Loss: 0.783949\n",
      "step: 5900, loss: 0.787438 val Loss: 0.755920\n",
      "step: 5910, loss: 0.772280 val Loss: 0.789906\n",
      "step: 5920, loss: 0.744419 val Loss: 0.739048\n",
      "step: 5930, loss: 0.801549 val Loss: 0.768931\n",
      "step: 5940, loss: 0.781846 val Loss: 0.742139\n",
      "step: 5950, loss: 0.753753 val Loss: 0.724882\n",
      "step: 5960, loss: 0.747746 val Loss: 0.751978\n",
      "step: 5970, loss: 0.748229 val Loss: 0.773182\n",
      "step: 5980, loss: 0.755624 val Loss: 0.812350\n",
      "step: 5990, loss: 0.784315 val Loss: 0.764316\n",
      "step: 6000, loss: 0.801647 val Loss: 0.676324\n",
      "step: 6010, loss: 0.776826 val Loss: 0.688531\n",
      "step: 6020, loss: 0.759521 val Loss: 0.742436\n",
      "step: 6030, loss: 0.744286 val Loss: 0.756326\n",
      "step: 6040, loss: 0.729183 val Loss: 0.759199\n",
      "step: 6050, loss: 0.703813 val Loss: 0.803864\n",
      "step: 6060, loss: 0.743523 val Loss: 0.649513\n",
      "Saving model with validation loss: 0.6495134234428406\n",
      "\n",
      "step: 6070, loss: 0.730762 val Loss: 0.750468\n",
      "step: 6080, loss: 0.728452 val Loss: 0.733239\n",
      "step: 6090, loss: 0.731658 val Loss: 0.702669\n",
      "step: 6100, loss: 0.794686 val Loss: 0.740268\n",
      "step: 6110, loss: 0.778408 val Loss: 0.810688\n",
      "step: 6120, loss: 0.756130 val Loss: 0.726869\n",
      "step: 6130, loss: 0.773868 val Loss: 0.736466\n",
      "step: 6140, loss: 0.837628 val Loss: 0.745312\n",
      "step: 6150, loss: 0.696284 val Loss: 0.687811\n",
      "step: 6160, loss: 0.821472 val Loss: 0.758381\n",
      "step: 6170, loss: 0.793359 val Loss: 0.675665\n",
      "step: 6180, loss: 0.783741 val Loss: 0.766845\n",
      "step: 6190, loss: 0.680923 val Loss: 0.754317\n",
      "step: 6200, loss: 0.805029 val Loss: 0.703623\n",
      "step: 6210, loss: 0.811181 val Loss: 0.742315\n",
      "step: 6220, loss: 0.813983 val Loss: 0.813542\n",
      "step: 6230, loss: 0.733576 val Loss: 0.777106\n",
      "step: 6240, loss: 0.721135 val Loss: 0.775082\n",
      "step: 6250, loss: 0.712786 val Loss: 0.733985\n",
      "step: 6260, loss: 0.732949 val Loss: 0.776033\n",
      "step: 6270, loss: 0.808955 val Loss: 0.778098\n",
      "step: 6280, loss: 0.838272 val Loss: 0.789256\n",
      "step: 6290, loss: 0.748968 val Loss: 0.665597\n",
      "step: 6300, loss: 0.829969 val Loss: 0.813632\n",
      "step: 6310, loss: 0.846510 val Loss: 0.780576\n",
      "step: 6320, loss: 0.681622 val Loss: 0.719051\n",
      "step: 6330, loss: 0.949476 val Loss: 0.788320\n",
      "step: 6340, loss: 0.816037 val Loss: 0.714901\n",
      "step: 6350, loss: 0.722586 val Loss: 0.714403\n",
      "step: 6360, loss: 0.716566 val Loss: 0.768362\n",
      "step: 6370, loss: 0.789318 val Loss: 0.680754\n",
      "step: 6380, loss: 0.724971 val Loss: 0.731762\n",
      "step: 6390, loss: 0.690167 val Loss: 0.746482\n",
      "step: 6400, loss: 0.792054 val Loss: 0.806453\n",
      "step: 6410, loss: 0.707862 val Loss: 0.730448\n",
      "step: 6420, loss: 0.733402 val Loss: 0.706643\n",
      "step: 6430, loss: 0.846713 val Loss: 0.776098\n",
      "step: 6440, loss: 0.677486 val Loss: 0.757881\n",
      "step: 6450, loss: 0.737011 val Loss: 0.737361\n",
      "step: 6460, loss: 0.799464 val Loss: 0.743734\n",
      "step: 6470, loss: 0.824134 val Loss: 0.823111\n",
      "step: 6480, loss: 0.838423 val Loss: 0.692560\n",
      "step: 6490, loss: 0.738639 val Loss: 0.675584\n",
      "step: 6500, loss: 0.851556 val Loss: 0.746130\n",
      "step: 6510, loss: 0.726340 val Loss: 0.739489\n",
      "step: 6520, loss: 0.792896 val Loss: 0.755735\n",
      "step: 6530, loss: 0.714433 val Loss: 0.810122\n",
      "step: 6540, loss: 0.780369 val Loss: 0.804227\n",
      "step: 6550, loss: 0.685023 val Loss: 0.748625\n",
      "step: 6560, loss: 0.726984 val Loss: 0.803029\n",
      "step: 6570, loss: 0.783661 val Loss: 0.763626\n",
      "step: 6580, loss: 0.731766 val Loss: 0.795616\n",
      "step: 6590, loss: 0.704419 val Loss: 0.816655\n",
      "step: 6600, loss: 0.724901 val Loss: 0.792160\n",
      "step: 6610, loss: 0.802168 val Loss: 0.817456\n",
      "step: 6620, loss: 0.764086 val Loss: 0.664431\n",
      "step: 6630, loss: 0.725337 val Loss: 0.668097\n",
      "step: 6640, loss: 0.822064 val Loss: 0.760525\n",
      "step: 6650, loss: 0.758042 val Loss: 0.751241\n",
      "step: 6660, loss: 0.815259 val Loss: 0.786818\n",
      "step: 6670, loss: 0.691289 val Loss: 0.737363\n",
      "step: 6680, loss: 0.745585 val Loss: 0.830396\n",
      "step: 6690, loss: 0.718461 val Loss: 0.653386\n",
      "step: 6700, loss: 0.786354 val Loss: 0.691805\n",
      "step: 6710, loss: 0.739206 val Loss: 0.655846\n",
      "step: 6720, loss: 0.783747 val Loss: 0.714634\n",
      "step: 6730, loss: 0.773228 val Loss: 0.782773\n",
      "step: 6740, loss: 0.715193 val Loss: 0.738564\n",
      "step: 6750, loss: 0.768386 val Loss: 0.668317\n",
      "step: 6760, loss: 0.826173 val Loss: 0.766059\n",
      "step: 6770, loss: 0.728709 val Loss: 0.800821\n",
      "step: 6780, loss: 0.751212 val Loss: 0.793039\n",
      "step: 6790, loss: 0.713086 val Loss: 0.743588\n",
      "step: 6800, loss: 0.850590 val Loss: 0.672628\n",
      "step: 6810, loss: 0.765122 val Loss: 0.753126\n",
      "step: 6820, loss: 0.829411 val Loss: 0.776795\n",
      "step: 6830, loss: 0.852709 val Loss: 0.792580\n",
      "step: 6840, loss: 0.688268 val Loss: 0.733498\n",
      "step: 6850, loss: 0.772100 val Loss: 0.748207\n",
      "step: 6860, loss: 0.766252 val Loss: 0.745689\n",
      "step: 6870, loss: 0.727498 val Loss: 0.765665\n",
      "step: 6880, loss: 0.745696 val Loss: 0.728703\n",
      "step: 6890, loss: 0.683545 val Loss: 0.679796\n",
      "step: 6900, loss: 0.772115 val Loss: 0.797235\n",
      "step: 6910, loss: 0.705512 val Loss: 0.848859\n",
      "step: 6920, loss: 0.748141 val Loss: 0.704937\n",
      "step: 6930, loss: 0.733745 val Loss: 0.748818\n",
      "step: 6940, loss: 0.756173 val Loss: 0.746622\n",
      "step: 6950, loss: 0.775861 val Loss: 0.805555\n",
      "step: 6960, loss: 0.789693 val Loss: 0.788234\n",
      "step: 6970, loss: 0.779796 val Loss: 0.777285\n",
      "step: 6980, loss: 0.779915 val Loss: 0.770340\n",
      "step: 6990, loss: 0.793645 val Loss: 0.777143\n",
      "step: 7000, loss: 0.806647 val Loss: 0.814765\n",
      "step: 7010, loss: 0.788928 val Loss: 0.740710\n",
      "step: 7020, loss: 0.768954 val Loss: 0.810293\n",
      "step: 7030, loss: 0.705422 val Loss: 0.739148\n",
      "step: 7040, loss: 0.751670 val Loss: 0.743924\n",
      "step: 7050, loss: 0.771702 val Loss: 0.789822\n",
      "step: 7060, loss: 0.772497 val Loss: 0.751789\n",
      "step: 7070, loss: 0.826894 val Loss: 0.759134\n",
      "step: 7080, loss: 0.734143 val Loss: 0.735616\n",
      "step: 7090, loss: 0.707531 val Loss: 0.853306\n",
      "step: 7100, loss: 0.748763 val Loss: 0.715730\n",
      "step: 7110, loss: 0.826184 val Loss: 0.769073\n",
      "step: 7120, loss: 0.809573 val Loss: 0.750547\n",
      "step: 7130, loss: 0.785572 val Loss: 0.803618\n",
      "step: 7140, loss: 0.760333 val Loss: 0.742874\n",
      "step: 7150, loss: 0.710597 val Loss: 0.781359\n",
      "step: 7160, loss: 0.745900 val Loss: 0.745728\n",
      "step: 7170, loss: 0.725236 val Loss: 0.697721\n",
      "step: 7180, loss: 0.752386 val Loss: 0.789704\n",
      "step: 7190, loss: 0.772886 val Loss: 0.772677\n",
      "step: 7200, loss: 0.690123 val Loss: 0.742164\n",
      "step: 7210, loss: 0.713903 val Loss: 0.723608\n",
      "step: 7220, loss: 0.750214 val Loss: 0.708195\n",
      "step: 7230, loss: 0.766932 val Loss: 0.789105\n",
      "step: 7240, loss: 0.694944 val Loss: 0.847564\n",
      "step: 7250, loss: 0.719404 val Loss: 0.721724\n",
      "step: 7260, loss: 0.735835 val Loss: 0.711707\n",
      "step: 7270, loss: 0.755505 val Loss: 0.648949\n",
      "Saving model with validation loss: 0.6489486694335938\n",
      "\n",
      "step: 7280, loss: 0.781667 val Loss: 0.775269\n",
      "step: 7290, loss: 0.743834 val Loss: 0.748873\n",
      "step: 7300, loss: 0.735052 val Loss: 0.758448\n",
      "step: 7310, loss: 0.841393 val Loss: 0.745948\n",
      "step: 7320, loss: 0.737282 val Loss: 0.778573\n",
      "step: 7330, loss: 0.774229 val Loss: 0.797495\n",
      "step: 7340, loss: 0.727068 val Loss: 0.739062\n",
      "step: 7350, loss: 0.828800 val Loss: 0.799353\n",
      "step: 7360, loss: 0.707355 val Loss: 0.695391\n",
      "step: 7370, loss: 0.744277 val Loss: 0.736858\n",
      "step: 7380, loss: 0.713651 val Loss: 0.679409\n",
      "step: 7390, loss: 0.821621 val Loss: 0.738359\n",
      "step: 7400, loss: 0.752842 val Loss: 0.736790\n",
      "step: 7410, loss: 0.739018 val Loss: 0.806269\n",
      "step: 7420, loss: 0.762222 val Loss: 0.755866\n",
      "step: 7430, loss: 0.734273 val Loss: 0.778750\n",
      "step: 7440, loss: 0.722330 val Loss: 0.738988\n",
      "step: 7450, loss: 0.783084 val Loss: 0.743493\n",
      "step: 7460, loss: 0.763076 val Loss: 0.741803\n",
      "step: 7470, loss: 0.773955 val Loss: 0.741514\n",
      "step: 7480, loss: 0.745010 val Loss: 0.665461\n",
      "step: 7490, loss: 0.763777 val Loss: 0.739825\n",
      "step: 7500, loss: 0.783798 val Loss: 0.775691\n",
      "step: 7510, loss: 0.760220 val Loss: 0.728612\n",
      "step: 7520, loss: 0.731611 val Loss: 0.729095\n",
      "step: 7530, loss: 0.721841 val Loss: 0.815826\n",
      "step: 7540, loss: 0.679762 val Loss: 0.679295\n",
      "step: 7550, loss: 0.727647 val Loss: 0.670468\n",
      "step: 7560, loss: 0.773013 val Loss: 0.738190\n",
      "step: 7570, loss: 0.736905 val Loss: 0.782003\n",
      "step: 7580, loss: 0.852445 val Loss: 0.794938\n",
      "step: 7590, loss: 0.764847 val Loss: 0.794781\n",
      "step: 7600, loss: 0.758890 val Loss: 0.689788\n",
      "step: 7610, loss: 0.771826 val Loss: 0.771188\n",
      "step: 7620, loss: 0.733608 val Loss: 0.701749\n",
      "step: 7630, loss: 0.716466 val Loss: 0.743422\n",
      "step: 7640, loss: 0.826770 val Loss: 0.742621\n",
      "step: 7650, loss: 0.797877 val Loss: 0.709955\n",
      "step: 7660, loss: 0.784949 val Loss: 0.728296\n",
      "step: 7670, loss: 0.753370 val Loss: 0.777532\n",
      "step: 7680, loss: 0.774794 val Loss: 0.675964\n",
      "step: 7690, loss: 0.702763 val Loss: 0.740712\n",
      "step: 7700, loss: 0.781135 val Loss: 0.741226\n",
      "step: 7710, loss: 0.823884 val Loss: 0.734647\n",
      "step: 7720, loss: 0.736000 val Loss: 0.738568\n",
      "step: 7730, loss: 0.704362 val Loss: 0.770928\n",
      "step: 7740, loss: 0.736414 val Loss: 0.820299\n",
      "step: 7750, loss: 0.718202 val Loss: 0.738164\n",
      "step: 7760, loss: 0.756671 val Loss: 0.730604\n",
      "step: 7770, loss: 0.840919 val Loss: 0.746965\n",
      "step: 7780, loss: 0.713340 val Loss: 0.766369\n",
      "step: 7790, loss: 0.752748 val Loss: 0.808820\n",
      "step: 7800, loss: 0.739147 val Loss: 0.732233\n",
      "step: 7810, loss: 0.766019 val Loss: 0.736046\n",
      "step: 7820, loss: 0.763160 val Loss: 0.725419\n",
      "step: 7830, loss: 0.876664 val Loss: 0.707907\n",
      "step: 7840, loss: 0.733294 val Loss: 0.777408\n",
      "step: 7850, loss: 0.770303 val Loss: 0.742259\n",
      "step: 7860, loss: 0.831246 val Loss: 0.651740\n",
      "step: 7870, loss: 0.723811 val Loss: 0.673907\n",
      "step: 7880, loss: 0.733082 val Loss: 0.738981\n",
      "step: 7890, loss: 0.701139 val Loss: 0.691535\n",
      "step: 7900, loss: 0.681526 val Loss: 0.761139\n",
      "step: 7910, loss: 0.744828 val Loss: 0.784090\n",
      "step: 7920, loss: 0.770357 val Loss: 0.801067\n",
      "step: 7930, loss: 0.702301 val Loss: 0.676569\n",
      "step: 7940, loss: 0.727186 val Loss: 0.804309\n",
      "step: 7950, loss: 0.769831 val Loss: 0.752318\n",
      "step: 7960, loss: 0.806426 val Loss: 0.681661\n",
      "step: 7970, loss: 0.852053 val Loss: 0.768332\n",
      "step: 7980, loss: 0.906605 val Loss: 0.797997\n",
      "step: 7990, loss: 0.734710 val Loss: 0.668459\n",
      "step: 8000, loss: 0.696968 val Loss: 0.685409\n",
      "step: 8010, loss: 0.782453 val Loss: 0.741255\n",
      "step: 8020, loss: 0.728447 val Loss: 0.791035\n",
      "step: 8030, loss: 0.755729 val Loss: 0.680887\n",
      "step: 8040, loss: 0.757485 val Loss: 0.782530\n",
      "step: 8050, loss: 0.671848 val Loss: 0.808467\n",
      "step: 8060, loss: 0.796529 val Loss: 0.794988\n",
      "step: 8070, loss: 0.794457 val Loss: 0.794697\n",
      "step: 8080, loss: 0.735324 val Loss: 0.790270\n",
      "step: 8090, loss: 0.764783 val Loss: 0.668239\n",
      "step: 8100, loss: 0.725353 val Loss: 0.747356\n",
      "step: 8110, loss: 0.869033 val Loss: 0.736600\n",
      "step: 8120, loss: 0.816685 val Loss: 0.727095\n",
      "step: 8130, loss: 0.677991 val Loss: 0.782213\n",
      "step: 8140, loss: 0.849786 val Loss: 0.740551\n",
      "step: 8150, loss: 0.839695 val Loss: 0.744371\n",
      "step: 8160, loss: 0.907543 val Loss: 0.732493\n",
      "step: 8170, loss: 0.668227 val Loss: 0.787843\n",
      "step: 8180, loss: 0.724050 val Loss: 0.845708\n",
      "step: 8190, loss: 0.753102 val Loss: 0.727130\n",
      "step: 8200, loss: 0.712504 val Loss: 0.766887\n",
      "step: 8210, loss: 0.678683 val Loss: 0.689995\n",
      "step: 8220, loss: 0.745262 val Loss: 0.737777\n",
      "step: 8230, loss: 0.777257 val Loss: 0.788861\n",
      "step: 8240, loss: 0.804981 val Loss: 0.760496\n",
      "step: 8250, loss: 0.757515 val Loss: 0.749551\n",
      "step: 8260, loss: 0.700027 val Loss: 0.753674\n",
      "step: 8270, loss: 0.855622 val Loss: 0.748869\n",
      "step: 8280, loss: 0.769494 val Loss: 0.690771\n",
      "step: 8290, loss: 0.784159 val Loss: 0.708942\n",
      "step: 8300, loss: 0.844634 val Loss: 0.739911\n",
      "step: 8310, loss: 0.717605 val Loss: 0.762558\n",
      "step: 8320, loss: 0.747745 val Loss: 0.808227\n",
      "step: 8330, loss: 0.753855 val Loss: 0.736820\n",
      "step: 8340, loss: 0.668290 val Loss: 0.739346\n",
      "step: 8350, loss: 0.726950 val Loss: 0.701078\n",
      "step: 8360, loss: 0.695156 val Loss: 0.718203\n",
      "step: 8370, loss: 0.758222 val Loss: 0.722287\n",
      "step: 8380, loss: 0.790434 val Loss: 0.794868\n",
      "step: 8390, loss: 0.863379 val Loss: 0.751669\n",
      "step: 8400, loss: 0.800371 val Loss: 0.778675\n",
      "step: 8410, loss: 0.741974 val Loss: 0.677323\n",
      "step: 8420, loss: 0.733302 val Loss: 0.715696\n",
      "step: 8430, loss: 0.781043 val Loss: 0.781132\n",
      "step: 8440, loss: 0.814028 val Loss: 0.747839\n",
      "step: 8450, loss: 0.776593 val Loss: 0.815729\n",
      "step: 8460, loss: 0.710223 val Loss: 0.725441\n",
      "step: 8470, loss: 0.800435 val Loss: 0.686699\n",
      "step: 8480, loss: 0.763981 val Loss: 0.798570\n",
      "step: 8490, loss: 0.796229 val Loss: 0.731745\n",
      "step: 8500, loss: 0.753103 val Loss: 0.754540\n",
      "step: 8510, loss: 0.802845 val Loss: 0.808537\n",
      "step: 8520, loss: 0.789517 val Loss: 0.688214\n",
      "step: 8530, loss: 0.759613 val Loss: 0.777277\n",
      "step: 8540, loss: 0.809282 val Loss: 0.759186\n",
      "step: 8550, loss: 0.709750 val Loss: 0.771983\n",
      "step: 8560, loss: 0.721552 val Loss: 0.781941\n",
      "step: 8570, loss: 0.771692 val Loss: 0.680650\n",
      "step: 8580, loss: 0.761670 val Loss: 0.761081\n",
      "step: 8590, loss: 0.817259 val Loss: 0.692280\n",
      "step: 8600, loss: 0.712218 val Loss: 0.690883\n",
      "step: 8610, loss: 0.775242 val Loss: 0.796235\n",
      "step: 8620, loss: 0.738523 val Loss: 0.749214\n",
      "step: 8630, loss: 0.814921 val Loss: 0.776339\n",
      "step: 8640, loss: 0.818426 val Loss: 0.805223\n",
      "step: 8650, loss: 0.753810 val Loss: 0.711210\n",
      "step: 8660, loss: 0.746334 val Loss: 0.787256\n",
      "step: 8670, loss: 0.723173 val Loss: 0.808589\n",
      "step: 8680, loss: 0.780760 val Loss: 0.758704\n",
      "step: 8690, loss: 0.725850 val Loss: 0.842950\n",
      "step: 8700, loss: 0.770003 val Loss: 0.805793\n",
      "step: 8710, loss: 0.713613 val Loss: 0.718336\n",
      "step: 8720, loss: 0.760748 val Loss: 0.678359\n",
      "step: 8730, loss: 0.737179 val Loss: 0.772924\n",
      "step: 8740, loss: 0.759690 val Loss: 0.804633\n",
      "step: 8750, loss: 0.681112 val Loss: 0.808823\n",
      "step: 8760, loss: 0.779509 val Loss: 0.714860\n",
      "step: 8770, loss: 0.782453 val Loss: 0.794208\n",
      "step: 8780, loss: 0.668579 val Loss: 0.779833\n",
      "step: 8790, loss: 0.741094 val Loss: 0.773301\n",
      "step: 8800, loss: 0.776848 val Loss: 0.742216\n",
      "step: 8810, loss: 0.726439 val Loss: 0.749534\n",
      "step: 8820, loss: 0.762388 val Loss: 0.677961\n",
      "step: 8830, loss: 0.775222 val Loss: 0.726419\n",
      "step: 8840, loss: 0.713222 val Loss: 0.782598\n",
      "step: 8850, loss: 0.811236 val Loss: 0.768276\n",
      "step: 8860, loss: 0.713397 val Loss: 0.770436\n",
      "step: 8870, loss: 0.775864 val Loss: 0.739978\n",
      "step: 8880, loss: 0.795441 val Loss: 0.738348\n",
      "step: 8890, loss: 0.744416 val Loss: 0.758455\n",
      "step: 8900, loss: 0.785013 val Loss: 0.781597\n",
      "step: 8910, loss: 0.813767 val Loss: 0.777526\n",
      "step: 8920, loss: 0.731753 val Loss: 0.772156\n",
      "step: 8930, loss: 0.724660 val Loss: 0.670785\n",
      "step: 8940, loss: 0.770380 val Loss: 0.748194\n",
      "step: 8950, loss: 0.761939 val Loss: 0.744093\n",
      "step: 8960, loss: 0.831658 val Loss: 0.715772\n",
      "step: 8970, loss: 0.790022 val Loss: 0.676920\n",
      "step: 8980, loss: 0.715853 val Loss: 0.761295\n",
      "step: 8990, loss: 0.817063 val Loss: 0.747606\n",
      "step: 9000, loss: 0.702166 val Loss: 0.789404\n",
      "step: 9010, loss: 0.746883 val Loss: 0.738324\n",
      "step: 9020, loss: 0.854189 val Loss: 0.687270\n",
      "step: 9030, loss: 0.758309 val Loss: 0.753272\n",
      "step: 9040, loss: 0.823308 val Loss: 0.742341\n",
      "step: 9050, loss: 0.735749 val Loss: 0.787444\n",
      "step: 9060, loss: 0.721022 val Loss: 0.758741\n",
      "step: 9070, loss: 0.743643 val Loss: 0.703621\n",
      "step: 9080, loss: 0.754241 val Loss: 0.774709\n",
      "step: 9090, loss: 0.717375 val Loss: 0.743778\n",
      "step: 9100, loss: 0.797055 val Loss: 0.799156\n",
      "step: 9110, loss: 0.718189 val Loss: 0.686656\n",
      "step: 9120, loss: 0.781233 val Loss: 0.740616\n",
      "step: 9130, loss: 0.727500 val Loss: 0.666026\n",
      "step: 9140, loss: 0.709675 val Loss: 0.736221\n",
      "step: 9150, loss: 0.816473 val Loss: 0.738984\n",
      "step: 9160, loss: 0.814195 val Loss: 0.799418\n",
      "step: 9170, loss: 0.760947 val Loss: 0.746619\n",
      "step: 9180, loss: 0.815289 val Loss: 0.758927\n",
      "step: 9190, loss: 0.777130 val Loss: 0.668736\n",
      "step: 9200, loss: 0.739121 val Loss: 0.764502\n",
      "step: 9210, loss: 0.741059 val Loss: 0.756565\n",
      "step: 9220, loss: 0.685623 val Loss: 0.825146\n",
      "step: 9230, loss: 0.738959 val Loss: 0.715307\n",
      "step: 9240, loss: 0.772674 val Loss: 0.760550\n",
      "step: 9250, loss: 0.692957 val Loss: 0.787695\n",
      "step: 9260, loss: 0.741177 val Loss: 0.690833\n",
      "step: 9270, loss: 0.783162 val Loss: 0.817362\n",
      "step: 9280, loss: 0.872602 val Loss: 0.755727\n",
      "step: 9290, loss: 0.737521 val Loss: 0.773857\n",
      "step: 9300, loss: 0.821332 val Loss: 0.809881\n",
      "step: 9310, loss: 0.852650 val Loss: 0.769565\n",
      "step: 9320, loss: 0.744318 val Loss: 0.752952\n",
      "step: 9330, loss: 0.785625 val Loss: 0.679767\n",
      "step: 9340, loss: 0.707847 val Loss: 0.671682\n",
      "step: 9350, loss: 0.807488 val Loss: 0.785548\n",
      "step: 9360, loss: 0.763774 val Loss: 0.712255\n",
      "step: 9370, loss: 0.694560 val Loss: 0.746805\n",
      "step: 9380, loss: 0.722379 val Loss: 0.740589\n",
      "step: 9390, loss: 0.794651 val Loss: 0.680672\n",
      "step: 9400, loss: 0.690641 val Loss: 0.707022\n",
      "step: 9410, loss: 0.761521 val Loss: 0.746102\n",
      "step: 9420, loss: 0.767405 val Loss: 0.773962\n",
      "step: 9430, loss: 0.754584 val Loss: 0.712837\n",
      "step: 9440, loss: 0.819407 val Loss: 0.797428\n",
      "step: 9450, loss: 0.844027 val Loss: 0.699240\n",
      "step: 9460, loss: 0.771954 val Loss: 0.686864\n",
      "step: 9470, loss: 0.772198 val Loss: 0.768186\n",
      "step: 9480, loss: 0.745416 val Loss: 0.845902\n",
      "step: 9490, loss: 0.760143 val Loss: 0.783275\n",
      "step: 9500, loss: 0.739154 val Loss: 0.758993\n",
      "step: 9510, loss: 0.734657 val Loss: 0.739624\n",
      "step: 9520, loss: 0.773887 val Loss: 0.700688\n",
      "step: 9530, loss: 0.756132 val Loss: 0.802926\n",
      "step: 9540, loss: 0.700192 val Loss: 0.716551\n",
      "step: 9550, loss: 0.757567 val Loss: 0.758857\n",
      "step: 9560, loss: 0.698729 val Loss: 0.766334\n",
      "step: 9570, loss: 0.783626 val Loss: 0.781887\n",
      "step: 9580, loss: 0.705736 val Loss: 0.775264\n",
      "step: 9590, loss: 0.691647 val Loss: 0.798401\n",
      "step: 9600, loss: 0.856438 val Loss: 0.756646\n",
      "step: 9610, loss: 0.703501 val Loss: 0.707800\n",
      "step: 9620, loss: 0.790581 val Loss: 0.798473\n",
      "step: 9630, loss: 0.697287 val Loss: 0.706380\n",
      "step: 9640, loss: 0.728465 val Loss: 0.758628\n",
      "step: 9650, loss: 0.799594 val Loss: 0.737003\n",
      "step: 9660, loss: 0.737982 val Loss: 0.759345\n",
      "step: 9670, loss: 0.743429 val Loss: 0.789275\n",
      "step: 9680, loss: 0.752658 val Loss: 0.790129\n",
      "step: 9690, loss: 0.721451 val Loss: 0.785127\n",
      "step: 9700, loss: 0.794243 val Loss: 0.785198\n",
      "step: 9710, loss: 0.739919 val Loss: 0.732465\n",
      "step: 9720, loss: 0.729955 val Loss: 0.690891\n",
      "step: 9730, loss: 0.778147 val Loss: 0.802662\n",
      "step: 9740, loss: 0.679550 val Loss: 0.784152\n",
      "step: 9750, loss: 0.754597 val Loss: 0.812281\n",
      "step: 9760, loss: 0.829305 val Loss: 0.721211\n",
      "step: 9770, loss: 0.764324 val Loss: 0.745073\n",
      "step: 9780, loss: 0.812546 val Loss: 0.722914\n",
      "step: 9790, loss: 0.755556 val Loss: 0.779648\n",
      "step: 9800, loss: 0.801674 val Loss: 0.797408\n",
      "step: 9810, loss: 0.732614 val Loss: 0.665314\n",
      "step: 9820, loss: 0.801767 val Loss: 0.743721\n",
      "step: 9830, loss: 0.797449 val Loss: 0.794539\n",
      "step: 9840, loss: 0.757485 val Loss: 0.733802\n",
      "step: 9850, loss: 0.766546 val Loss: 0.756053\n",
      "step: 9860, loss: 0.812612 val Loss: 0.760950\n",
      "step: 9870, loss: 0.682163 val Loss: 0.738523\n",
      "step: 9880, loss: 0.701526 val Loss: 0.665628\n",
      "step: 9890, loss: 0.742431 val Loss: 0.779206\n",
      "step: 9900, loss: 0.749901 val Loss: 0.708248\n",
      "step: 9910, loss: 0.729080 val Loss: 0.781900\n",
      "step: 9920, loss: 0.802074 val Loss: 0.726763\n",
      "step: 9930, loss: 0.738480 val Loss: 0.769609\n",
      "step: 9940, loss: 0.816439 val Loss: 0.754114\n",
      "step: 9950, loss: 0.732722 val Loss: 0.718002\n",
      "step: 9960, loss: 0.797608 val Loss: 0.761771\n",
      "step: 9970, loss: 0.725827 val Loss: 0.748716\n",
      "step: 9980, loss: 0.795751 val Loss: 0.742565\n",
      "step: 9990, loss: 0.726590 val Loss: 0.725040\n",
      "step: 10000, loss: 0.780042 val Loss: 0.746525\n",
      "step: 10010, loss: 0.688226 val Loss: 0.755806\n",
      "step: 10020, loss: 0.750387 val Loss: 0.771208\n",
      "step: 10030, loss: 0.759556 val Loss: 0.795650\n",
      "step: 10040, loss: 0.723779 val Loss: 0.715661\n",
      "step: 10050, loss: 0.778599 val Loss: 0.803237\n",
      "step: 10060, loss: 0.760852 val Loss: 0.639172\n",
      "Saving model with validation loss: 0.6391719579696655\n",
      "\n",
      "step: 10070, loss: 0.772408 val Loss: 0.777290\n",
      "step: 10080, loss: 0.735215 val Loss: 0.710907\n",
      "step: 10090, loss: 0.765402 val Loss: 0.727735\n",
      "step: 10100, loss: 0.829364 val Loss: 0.794308\n",
      "step: 10110, loss: 0.677759 val Loss: 0.719904\n",
      "step: 10120, loss: 0.742378 val Loss: 0.743705\n",
      "step: 10130, loss: 0.762692 val Loss: 0.697833\n",
      "step: 10140, loss: 0.832022 val Loss: 0.843347\n",
      "step: 10150, loss: 0.720088 val Loss: 0.738452\n",
      "step: 10160, loss: 0.862131 val Loss: 0.770413\n",
      "step: 10170, loss: 0.712389 val Loss: 0.766336\n",
      "step: 10180, loss: 0.765693 val Loss: 0.760469\n",
      "step: 10190, loss: 0.691562 val Loss: 0.723761\n",
      "step: 10200, loss: 0.729719 val Loss: 0.754212\n",
      "step: 10210, loss: 0.712499 val Loss: 0.775218\n",
      "step: 10220, loss: 0.702730 val Loss: 0.728678\n",
      "step: 10230, loss: 0.795773 val Loss: 0.775793\n",
      "step: 10240, loss: 0.776950 val Loss: 0.723298\n",
      "step: 10250, loss: 0.737467 val Loss: 0.800715\n",
      "step: 10260, loss: 0.815759 val Loss: 0.784690\n",
      "step: 10270, loss: 0.744255 val Loss: 0.752791\n",
      "step: 10280, loss: 0.763044 val Loss: 0.725153\n",
      "step: 10290, loss: 0.666879 val Loss: 0.784484\n",
      "step: 10300, loss: 0.780536 val Loss: 0.789902\n",
      "step: 10310, loss: 0.765635 val Loss: 0.728031\n",
      "step: 10320, loss: 0.778720 val Loss: 0.757841\n",
      "step: 10330, loss: 0.820780 val Loss: 0.792138\n",
      "step: 10340, loss: 0.714801 val Loss: 0.740634\n",
      "step: 10350, loss: 0.774594 val Loss: 0.790788\n",
      "step: 10360, loss: 0.755990 val Loss: 0.798478\n",
      "step: 10370, loss: 0.811210 val Loss: 0.733332\n",
      "step: 10380, loss: 0.748913 val Loss: 0.759469\n",
      "step: 10390, loss: 0.747806 val Loss: 0.736275\n",
      "step: 10400, loss: 0.712456 val Loss: 0.720543\n",
      "step: 10410, loss: 0.646358 val Loss: 0.758509\n",
      "step: 10420, loss: 0.714761 val Loss: 0.733990\n",
      "step: 10430, loss: 0.756968 val Loss: 0.769735\n",
      "step: 10440, loss: 0.826422 val Loss: 0.757257\n",
      "step: 10450, loss: 0.744876 val Loss: 0.682269\n",
      "step: 10460, loss: 0.706949 val Loss: 0.702212\n",
      "step: 10470, loss: 0.727445 val Loss: 0.765725\n",
      "step: 10480, loss: 0.782268 val Loss: 0.764441\n",
      "step: 10490, loss: 0.721642 val Loss: 0.742884\n",
      "step: 10500, loss: 0.749301 val Loss: 0.754506\n",
      "step: 10510, loss: 0.748843 val Loss: 0.729276\n",
      "step: 10520, loss: 0.744519 val Loss: 0.818003\n",
      "step: 10530, loss: 0.746249 val Loss: 0.680502\n",
      "step: 10540, loss: 0.766091 val Loss: 0.779459\n",
      "step: 10550, loss: 0.777915 val Loss: 0.767645\n",
      "step: 10560, loss: 0.748372 val Loss: 0.719477\n",
      "step: 10570, loss: 0.779698 val Loss: 0.677311\n",
      "step: 10580, loss: 0.710237 val Loss: 0.728466\n",
      "step: 10590, loss: 0.738429 val Loss: 0.731340\n",
      "step: 10600, loss: 0.807326 val Loss: 0.664779\n",
      "step: 10610, loss: 0.784981 val Loss: 0.722094\n",
      "step: 10620, loss: 0.711944 val Loss: 0.677905\n",
      "step: 10630, loss: 0.840589 val Loss: 0.743647\n",
      "step: 10640, loss: 0.766910 val Loss: 0.763140\n",
      "step: 10650, loss: 0.826038 val Loss: 0.763276\n",
      "step: 10660, loss: 0.854297 val Loss: 0.745708\n",
      "step: 10670, loss: 0.772043 val Loss: 0.782037\n",
      "step: 10680, loss: 0.797036 val Loss: 0.799314\n",
      "step: 10690, loss: 0.772612 val Loss: 0.785984\n",
      "step: 10700, loss: 0.765831 val Loss: 0.745728\n",
      "step: 10710, loss: 0.758658 val Loss: 0.687325\n",
      "step: 10720, loss: 0.737971 val Loss: 0.786820\n",
      "step: 10730, loss: 0.703037 val Loss: 0.748211\n",
      "step: 10740, loss: 0.751897 val Loss: 0.774863\n",
      "step: 10750, loss: 0.765985 val Loss: 0.806028\n",
      "step: 10760, loss: 0.751902 val Loss: 0.809956\n",
      "step: 10770, loss: 0.756779 val Loss: 0.781039\n",
      "step: 10780, loss: 0.772368 val Loss: 0.740267\n",
      "step: 10790, loss: 0.816430 val Loss: 0.729567\n",
      "step: 10800, loss: 0.766516 val Loss: 0.720543\n",
      "step: 10810, loss: 0.748596 val Loss: 0.817923\n",
      "step: 10820, loss: 0.763469 val Loss: 0.753704\n",
      "step: 10830, loss: 0.778368 val Loss: 0.712401\n",
      "step: 10840, loss: 0.783511 val Loss: 0.789463\n",
      "step: 10850, loss: 0.754099 val Loss: 0.732000\n",
      "step: 10860, loss: 0.699262 val Loss: 0.793000\n",
      "step: 10870, loss: 0.810564 val Loss: 0.811520\n",
      "step: 10880, loss: 0.802707 val Loss: 0.783832\n",
      "step: 10890, loss: 0.733570 val Loss: 0.750296\n",
      "step: 10900, loss: 0.736424 val Loss: 0.686830\n",
      "step: 10910, loss: 0.744739 val Loss: 0.786357\n",
      "step: 10920, loss: 0.744700 val Loss: 0.758078\n",
      "step: 10930, loss: 0.775895 val Loss: 0.769809\n",
      "step: 10940, loss: 0.872105 val Loss: 0.740595\n",
      "step: 10950, loss: 0.724939 val Loss: 0.709802\n",
      "step: 10960, loss: 0.729308 val Loss: 0.668777\n",
      "step: 10970, loss: 0.751274 val Loss: 0.803687\n",
      "step: 10980, loss: 0.738645 val Loss: 0.728142\n",
      "step: 10990, loss: 0.726869 val Loss: 0.771016\n",
      "step: 11000, loss: 0.694401 val Loss: 0.805731\n",
      "step: 11010, loss: 0.722713 val Loss: 0.728588\n",
      "step: 11020, loss: 0.701892 val Loss: 0.774320\n",
      "step: 11030, loss: 0.740275 val Loss: 0.750968\n",
      "step: 11040, loss: 0.788020 val Loss: 0.784949\n",
      "step: 11050, loss: 0.713747 val Loss: 0.736834\n",
      "step: 11060, loss: 0.702240 val Loss: 0.680767\n",
      "step: 11070, loss: 0.701522 val Loss: 0.748938\n",
      "step: 11080, loss: 0.692444 val Loss: 0.718798\n",
      "step: 11090, loss: 0.726790 val Loss: 0.742622\n",
      "step: 11100, loss: 0.825806 val Loss: 0.786751\n",
      "step: 11110, loss: 0.776680 val Loss: 0.757904\n",
      "step: 11120, loss: 0.700268 val Loss: 0.735987\n",
      "step: 11130, loss: 0.678149 val Loss: 0.765867\n",
      "step: 11140, loss: 0.748312 val Loss: 0.819826\n",
      "step: 11150, loss: 0.754339 val Loss: 0.774725\n",
      "step: 11160, loss: 0.808602 val Loss: 0.770070\n",
      "step: 11170, loss: 0.680664 val Loss: 0.692697\n",
      "step: 11180, loss: 0.703192 val Loss: 0.744398\n",
      "step: 11190, loss: 0.746971 val Loss: 0.813568\n",
      "step: 11200, loss: 0.769666 val Loss: 0.762331\n",
      "step: 11210, loss: 0.778918 val Loss: 0.735092\n",
      "step: 11220, loss: 0.651189 val Loss: 0.786313\n",
      "step: 11230, loss: 0.688733 val Loss: 0.731992\n",
      "step: 11240, loss: 0.752436 val Loss: 0.791561\n",
      "step: 11250, loss: 0.690485 val Loss: 0.817315\n",
      "step: 11260, loss: 0.785885 val Loss: 0.758747\n",
      "step: 11270, loss: 0.847322 val Loss: 0.733758\n",
      "step: 11280, loss: 0.735273 val Loss: 0.661926\n",
      "step: 11290, loss: 0.833681 val Loss: 0.741290\n",
      "step: 11300, loss: 0.771060 val Loss: 0.652255\n",
      "step: 11310, loss: 0.825425 val Loss: 0.765942\n",
      "step: 11320, loss: 0.821011 val Loss: 0.783129\n",
      "step: 11330, loss: 0.763201 val Loss: 0.771708\n",
      "step: 11340, loss: 0.755460 val Loss: 0.794425\n",
      "step: 11350, loss: 0.771272 val Loss: 0.754029\n",
      "step: 11360, loss: 0.695578 val Loss: 0.779611\n",
      "step: 11370, loss: 0.740448 val Loss: 0.752584\n",
      "step: 11380, loss: 0.701962 val Loss: 0.683697\n",
      "step: 11390, loss: 0.778823 val Loss: 0.715833\n",
      "step: 11400, loss: 0.757645 val Loss: 0.646589\n",
      "step: 11410, loss: 0.720827 val Loss: 0.803576\n",
      "step: 11420, loss: 0.765696 val Loss: 0.775937\n",
      "step: 11430, loss: 0.680697 val Loss: 0.747802\n",
      "step: 11440, loss: 0.800878 val Loss: 0.657641\n",
      "step: 11450, loss: 0.761843 val Loss: 0.799489\n",
      "step: 11460, loss: 0.788642 val Loss: 0.802155\n",
      "step: 11470, loss: 0.762068 val Loss: 0.770870\n",
      "step: 11480, loss: 0.718945 val Loss: 0.745722\n",
      "step: 11490, loss: 0.837478 val Loss: 0.708695\n",
      "step: 11500, loss: 0.709702 val Loss: 0.776530\n",
      "step: 11510, loss: 0.770173 val Loss: 0.670554\n",
      "step: 11520, loss: 0.744900 val Loss: 0.780391\n",
      "step: 11530, loss: 0.748383 val Loss: 0.756999\n",
      "step: 11540, loss: 0.789669 val Loss: 0.756359\n",
      "step: 11550, loss: 0.820936 val Loss: 0.773077\n",
      "step: 11560, loss: 0.768048 val Loss: 0.758221\n",
      "step: 11570, loss: 0.792448 val Loss: 0.755372\n",
      "step: 11580, loss: 0.787021 val Loss: 0.733340\n",
      "step: 11590, loss: 0.767723 val Loss: 0.673519\n",
      "step: 11600, loss: 0.687515 val Loss: 0.744148\n",
      "step: 11610, loss: 0.735415 val Loss: 0.797604\n",
      "step: 11620, loss: 0.757110 val Loss: 0.734764\n",
      "step: 11630, loss: 0.832956 val Loss: 0.748121\n",
      "step: 11640, loss: 0.771425 val Loss: 0.686961\n",
      "step: 11650, loss: 0.806554 val Loss: 0.701681\n",
      "step: 11660, loss: 0.758167 val Loss: 0.753790\n",
      "step: 11670, loss: 0.755070 val Loss: 0.668250\n",
      "step: 11680, loss: 0.809774 val Loss: 0.791424\n",
      "step: 11690, loss: 0.733411 val Loss: 0.712883\n",
      "step: 11700, loss: 0.758340 val Loss: 0.789112\n",
      "step: 11710, loss: 0.775275 val Loss: 0.752222\n",
      "step: 11720, loss: 0.719906 val Loss: 0.738528\n",
      "step: 11730, loss: 0.721148 val Loss: 0.778395\n",
      "step: 11740, loss: 0.714512 val Loss: 0.679242\n",
      "step: 11750, loss: 0.738029 val Loss: 0.758217\n",
      "step: 11760, loss: 0.812215 val Loss: 0.739579\n",
      "step: 11770, loss: 0.830684 val Loss: 0.711776\n",
      "step: 11780, loss: 0.722030 val Loss: 0.754990\n",
      "step: 11790, loss: 0.737553 val Loss: 0.660577\n",
      "step: 11800, loss: 0.662817 val Loss: 0.835403\n",
      "step: 11810, loss: 0.728883 val Loss: 0.803106\n",
      "step: 11820, loss: 0.739142 val Loss: 0.763135\n",
      "step: 11830, loss: 0.746080 val Loss: 0.733491\n",
      "step: 11840, loss: 0.786763 val Loss: 0.669993\n",
      "step: 11850, loss: 0.770126 val Loss: 0.788981\n",
      "step: 11860, loss: 0.701268 val Loss: 0.746480\n",
      "step: 11870, loss: 0.797244 val Loss: 0.751451\n",
      "step: 11880, loss: 0.808901 val Loss: 0.772196\n",
      "step: 11890, loss: 0.686110 val Loss: 0.746351\n",
      "step: 11900, loss: 0.745141 val Loss: 0.710340\n",
      "step: 11910, loss: 0.723154 val Loss: 0.758908\n",
      "step: 11920, loss: 0.727985 val Loss: 0.787008\n",
      "step: 11930, loss: 0.791361 val Loss: 0.706796\n",
      "step: 11940, loss: 0.758388 val Loss: 0.672988\n",
      "step: 11950, loss: 0.784674 val Loss: 0.729559\n",
      "step: 11960, loss: 0.642691 val Loss: 0.711494\n",
      "step: 11970, loss: 0.849095 val Loss: 0.727568\n",
      "step: 11980, loss: 0.683797 val Loss: 0.747667\n",
      "step: 11990, loss: 0.776214 val Loss: 0.747617\n",
      "step: 12000, loss: 0.719964 val Loss: 0.783321\n",
      "step: 12010, loss: 0.833218 val Loss: 0.717418\n",
      "step: 12020, loss: 0.767906 val Loss: 0.718840\n",
      "step: 12030, loss: 0.681236 val Loss: 0.771760\n",
      "step: 12040, loss: 0.658544 val Loss: 0.734747\n",
      "step: 12050, loss: 0.784745 val Loss: 0.823509\n",
      "step: 12060, loss: 0.729589 val Loss: 0.712458\n",
      "step: 12070, loss: 0.771405 val Loss: 0.731227\n",
      "step: 12080, loss: 0.756632 val Loss: 0.781861\n",
      "step: 12090, loss: 0.735463 val Loss: 0.757549\n",
      "step: 12100, loss: 0.813937 val Loss: 0.784507\n",
      "step: 12110, loss: 0.756525 val Loss: 0.677420\n",
      "step: 12120, loss: 0.672972 val Loss: 0.750640\n",
      "step: 12130, loss: 0.762304 val Loss: 0.786412\n",
      "step: 12140, loss: 0.765718 val Loss: 0.747716\n",
      "step: 12150, loss: 0.763162 val Loss: 0.761831\n",
      "step: 12160, loss: 0.776407 val Loss: 0.731092\n",
      "step: 12170, loss: 0.791314 val Loss: 0.787063\n",
      "step: 12180, loss: 0.696030 val Loss: 0.786558\n",
      "step: 12190, loss: 0.764422 val Loss: 0.768841\n",
      "step: 12200, loss: 0.724797 val Loss: 0.715588\n",
      "step: 12210, loss: 0.745478 val Loss: 0.766123\n",
      "step: 12220, loss: 0.780194 val Loss: 0.851791\n",
      "step: 12230, loss: 0.777393 val Loss: 0.693258\n",
      "step: 12240, loss: 0.729007 val Loss: 0.749412\n",
      "step: 12250, loss: 0.802889 val Loss: 0.718703\n",
      "step: 12260, loss: 0.797286 val Loss: 0.763776\n",
      "step: 12270, loss: 0.707247 val Loss: 0.733755\n",
      "step: 12280, loss: 0.810604 val Loss: 0.797073\n",
      "step: 12290, loss: 0.745764 val Loss: 0.660896\n",
      "step: 12300, loss: 0.853178 val Loss: 0.744595\n",
      "step: 12310, loss: 0.737219 val Loss: 0.766479\n",
      "step: 12320, loss: 0.711124 val Loss: 0.774392\n",
      "step: 12330, loss: 0.760799 val Loss: 0.664743\n",
      "step: 12340, loss: 0.762831 val Loss: 0.756387\n",
      "step: 12350, loss: 0.719635 val Loss: 0.757407\n",
      "step: 12360, loss: 0.688495 val Loss: 0.719661\n",
      "step: 12370, loss: 0.735540 val Loss: 0.829431\n",
      "step: 12380, loss: 0.756980 val Loss: 0.776051\n",
      "step: 12390, loss: 0.717360 val Loss: 0.746265\n",
      "step: 12400, loss: 0.701225 val Loss: 0.664639\n",
      "step: 12410, loss: 0.694143 val Loss: 0.672116\n",
      "step: 12420, loss: 0.708194 val Loss: 0.762253\n",
      "step: 12430, loss: 0.767027 val Loss: 0.799163\n",
      "step: 12440, loss: 0.707709 val Loss: 0.688815\n",
      "step: 12450, loss: 0.752842 val Loss: 0.758482\n",
      "step: 12460, loss: 0.724176 val Loss: 0.746659\n",
      "step: 12470, loss: 0.739086 val Loss: 0.799602\n",
      "step: 12480, loss: 0.838393 val Loss: 0.780208\n",
      "step: 12490, loss: 0.828444 val Loss: 0.714070\n",
      "step: 12500, loss: 0.753862 val Loss: 0.796346\n",
      "step: 12510, loss: 0.829333 val Loss: 0.749904\n",
      "step: 12520, loss: 0.780697 val Loss: 0.686603\n",
      "step: 12530, loss: 0.694140 val Loss: 0.774682\n",
      "step: 12540, loss: 0.808002 val Loss: 0.738991\n",
      "step: 12550, loss: 0.755096 val Loss: 0.703885\n",
      "step: 12560, loss: 0.808391 val Loss: 0.756867\n",
      "step: 12570, loss: 0.669625 val Loss: 0.740126\n",
      "step: 12580, loss: 0.743393 val Loss: 0.744046\n",
      "step: 12590, loss: 0.826343 val Loss: 0.716658\n",
      "step: 12600, loss: 0.814175 val Loss: 0.768505\n",
      "step: 12610, loss: 0.776958 val Loss: 0.720554\n",
      "step: 12620, loss: 0.753492 val Loss: 0.803633\n",
      "step: 12630, loss: 0.793845 val Loss: 0.721740\n",
      "step: 12640, loss: 0.785357 val Loss: 0.773057\n",
      "step: 12650, loss: 0.833915 val Loss: 0.754273\n",
      "step: 12660, loss: 0.828521 val Loss: 0.764428\n",
      "step: 12670, loss: 0.663950 val Loss: 0.757566\n",
      "step: 12680, loss: 0.688012 val Loss: 0.693306\n",
      "step: 12690, loss: 0.805655 val Loss: 0.760842\n",
      "step: 12700, loss: 0.701204 val Loss: 0.764553\n",
      "step: 12710, loss: 0.811674 val Loss: 0.769002\n",
      "step: 12720, loss: 0.694375 val Loss: 0.697396\n",
      "step: 12730, loss: 0.779839 val Loss: 0.766349\n",
      "step: 12740, loss: 0.835514 val Loss: 0.735602\n",
      "step: 12750, loss: 0.779956 val Loss: 0.746117\n",
      "step: 12760, loss: 0.727783 val Loss: 0.747930\n",
      "step: 12770, loss: 0.812671 val Loss: 0.763398\n",
      "step: 12780, loss: 0.778013 val Loss: 0.748256\n",
      "step: 12790, loss: 0.805825 val Loss: 0.790070\n",
      "step: 12800, loss: 0.740607 val Loss: 0.709834\n",
      "step: 12810, loss: 0.749768 val Loss: 0.742993\n",
      "step: 12820, loss: 0.773284 val Loss: 0.768297\n",
      "step: 12830, loss: 0.721538 val Loss: 0.757017\n",
      "step: 12840, loss: 0.772508 val Loss: 0.772090\n",
      "step: 12850, loss: 0.783905 val Loss: 0.756814\n",
      "step: 12860, loss: 0.809857 val Loss: 0.735301\n",
      "step: 12870, loss: 0.736785 val Loss: 0.840501\n",
      "step: 12880, loss: 0.793475 val Loss: 0.771558\n",
      "step: 12890, loss: 0.752573 val Loss: 0.752793\n",
      "step: 12900, loss: 0.722476 val Loss: 0.760804\n",
      "step: 12910, loss: 0.701787 val Loss: 0.760763\n",
      "step: 12920, loss: 0.719542 val Loss: 0.840441\n",
      "step: 12930, loss: 0.792730 val Loss: 0.679376\n",
      "step: 12940, loss: 0.750246 val Loss: 0.733239\n",
      "step: 12950, loss: 0.762063 val Loss: 0.763594\n",
      "step: 12960, loss: 0.778761 val Loss: 0.768437\n",
      "step: 12970, loss: 0.706193 val Loss: 0.722927\n",
      "step: 12980, loss: 0.758700 val Loss: 0.706491\n",
      "step: 12990, loss: 0.849624 val Loss: 0.756896\n",
      "step: 13000, loss: 0.769632 val Loss: 0.763469\n",
      "step: 13010, loss: 0.752309 val Loss: 0.784956\n",
      "step: 13020, loss: 0.654056 val Loss: 0.791869\n",
      "step: 13030, loss: 0.744508 val Loss: 0.668434\n",
      "step: 13040, loss: 0.759509 val Loss: 0.798268\n",
      "step: 13050, loss: 0.701790 val Loss: 0.741921\n",
      "step: 13060, loss: 0.780493 val Loss: 0.751757\n",
      "step: 13070, loss: 0.728342 val Loss: 0.723450\n",
      "step: 13080, loss: 0.727715 val Loss: 0.782650\n",
      "step: 13090, loss: 0.708329 val Loss: 0.669186\n",
      "step: 13100, loss: 0.801837 val Loss: 0.771126\n",
      "step: 13110, loss: 0.708056 val Loss: 0.748813\n",
      "step: 13120, loss: 0.787744 val Loss: 0.657838\n",
      "step: 13130, loss: 0.770551 val Loss: 0.694992\n",
      "step: 13140, loss: 0.714292 val Loss: 0.762495\n",
      "step: 13150, loss: 0.780914 val Loss: 0.733114\n",
      "step: 13160, loss: 0.825348 val Loss: 0.761464\n",
      "step: 13170, loss: 0.691660 val Loss: 0.776092\n",
      "step: 13180, loss: 0.733695 val Loss: 0.764851\n",
      "step: 13190, loss: 0.754732 val Loss: 0.717030\n",
      "step: 13200, loss: 0.722869 val Loss: 0.748608\n",
      "step: 13210, loss: 0.768916 val Loss: 0.804064\n",
      "step: 13220, loss: 0.800618 val Loss: 0.735944\n",
      "step: 13230, loss: 0.816162 val Loss: 0.812206\n",
      "step: 13240, loss: 0.729992 val Loss: 0.682092\n",
      "step: 13250, loss: 0.708940 val Loss: 0.763151\n",
      "step: 13260, loss: 0.754798 val Loss: 0.686938\n",
      "step: 13270, loss: 0.723739 val Loss: 0.795503\n",
      "step: 13280, loss: 0.807403 val Loss: 0.740629\n",
      "step: 13290, loss: 0.789892 val Loss: 0.743736\n",
      "step: 13300, loss: 0.736445 val Loss: 0.677188\n",
      "step: 13310, loss: 0.745708 val Loss: 0.759673\n",
      "step: 13320, loss: 0.763861 val Loss: 0.687955\n",
      "step: 13330, loss: 0.802564 val Loss: 0.710392\n",
      "step: 13340, loss: 0.725830 val Loss: 0.761724\n",
      "step: 13350, loss: 0.706763 val Loss: 0.754780\n",
      "step: 13360, loss: 0.794524 val Loss: 0.689514\n",
      "step: 13370, loss: 0.766195 val Loss: 0.841293\n",
      "step: 13380, loss: 0.791454 val Loss: 0.700915\n",
      "step: 13390, loss: 0.799937 val Loss: 0.841214\n",
      "step: 13400, loss: 0.831261 val Loss: 0.753211\n",
      "step: 13410, loss: 0.692752 val Loss: 0.776366\n",
      "step: 13420, loss: 0.727757 val Loss: 0.763207\n",
      "step: 13430, loss: 0.795629 val Loss: 0.811402\n",
      "step: 13440, loss: 0.751012 val Loss: 0.732733\n",
      "step: 13450, loss: 0.668524 val Loss: 0.714998\n",
      "step: 13460, loss: 0.787214 val Loss: 0.758134\n",
      "step: 13470, loss: 0.752903 val Loss: 0.729614\n",
      "step: 13480, loss: 0.774035 val Loss: 0.770434\n",
      "step: 13490, loss: 0.805715 val Loss: 0.721492\n",
      "step: 13500, loss: 0.769405 val Loss: 0.679752\n",
      "step: 13510, loss: 0.769917 val Loss: 0.739020\n",
      "step: 13520, loss: 0.758288 val Loss: 0.745168\n",
      "step: 13530, loss: 0.703028 val Loss: 0.690152\n",
      "step: 13540, loss: 0.681280 val Loss: 0.726939\n",
      "step: 13550, loss: 0.784895 val Loss: 0.753957\n",
      "step: 13560, loss: 0.748313 val Loss: 0.673623\n",
      "step: 13570, loss: 0.758925 val Loss: 0.766052\n",
      "step: 13580, loss: 0.717990 val Loss: 0.785527\n",
      "step: 13590, loss: 0.742921 val Loss: 0.778087\n",
      "step: 13600, loss: 0.775363 val Loss: 0.711838\n",
      "step: 13610, loss: 0.753052 val Loss: 0.749551\n",
      "step: 13620, loss: 0.687100 val Loss: 0.732548\n",
      "step: 13630, loss: 0.733493 val Loss: 0.694216\n",
      "step: 13640, loss: 0.771059 val Loss: 0.787432\n",
      "step: 13650, loss: 0.801116 val Loss: 0.794410\n",
      "step: 13660, loss: 0.790062 val Loss: 0.790946\n",
      "step: 13670, loss: 0.805026 val Loss: 0.672085\n",
      "step: 13680, loss: 0.782511 val Loss: 0.810098\n",
      "step: 13690, loss: 0.796868 val Loss: 0.765338\n",
      "step: 13700, loss: 0.781218 val Loss: 0.705363\n",
      "step: 13710, loss: 0.730789 val Loss: 0.738158\n",
      "step: 13720, loss: 0.722813 val Loss: 0.738467\n",
      "step: 13730, loss: 0.711684 val Loss: 0.685667\n",
      "step: 13740, loss: 0.784680 val Loss: 0.756101\n",
      "step: 13750, loss: 0.779137 val Loss: 0.753765\n",
      "step: 13760, loss: 0.808478 val Loss: 0.805106\n",
      "step: 13770, loss: 0.846055 val Loss: 0.798843\n",
      "step: 13780, loss: 0.713809 val Loss: 0.805888\n",
      "step: 13790, loss: 0.751585 val Loss: 0.777696\n",
      "step: 13800, loss: 0.822344 val Loss: 0.730755\n",
      "step: 13810, loss: 0.742073 val Loss: 0.773420\n",
      "step: 13820, loss: 0.781453 val Loss: 0.743979\n",
      "step: 13830, loss: 0.726603 val Loss: 0.793178\n",
      "step: 13840, loss: 0.771242 val Loss: 0.709635\n",
      "step: 13850, loss: 0.781365 val Loss: 0.762423\n",
      "step: 13860, loss: 0.738990 val Loss: 0.707351\n",
      "step: 13870, loss: 0.793201 val Loss: 0.734644\n",
      "step: 13880, loss: 0.845797 val Loss: 0.665558\n",
      "step: 13890, loss: 0.693274 val Loss: 0.757375\n",
      "step: 13900, loss: 0.700071 val Loss: 0.749331\n",
      "step: 13910, loss: 0.747203 val Loss: 0.758944\n",
      "step: 13920, loss: 0.761411 val Loss: 0.747437\n",
      "step: 13930, loss: 0.736856 val Loss: 0.724705\n",
      "step: 13940, loss: 0.710770 val Loss: 0.704723\n",
      "step: 13950, loss: 0.746089 val Loss: 0.751785\n",
      "step: 13960, loss: 0.832252 val Loss: 0.799310\n",
      "step: 13970, loss: 0.769520 val Loss: 0.784524\n",
      "step: 13980, loss: 0.726304 val Loss: 0.807262\n",
      "step: 13990, loss: 0.734549 val Loss: 0.758307\n",
      "step: 14000, loss: 0.688661 val Loss: 0.798568\n",
      "step: 14010, loss: 0.771233 val Loss: 0.711998\n",
      "step: 14020, loss: 0.729761 val Loss: 0.773433\n",
      "step: 14030, loss: 0.762279 val Loss: 0.783297\n",
      "step: 14040, loss: 0.783565 val Loss: 0.751037\n",
      "step: 14050, loss: 0.751076 val Loss: 0.732064\n",
      "step: 14060, loss: 0.761411 val Loss: 0.720930\n",
      "step: 14070, loss: 0.731812 val Loss: 0.781854\n",
      "step: 14080, loss: 0.774756 val Loss: 0.726436\n",
      "step: 14090, loss: 0.764881 val Loss: 0.786873\n",
      "step: 14100, loss: 0.790422 val Loss: 0.758877\n",
      "step: 14110, loss: 0.737943 val Loss: 0.752927\n",
      "step: 14120, loss: 0.742252 val Loss: 0.773857\n",
      "step: 14130, loss: 0.748287 val Loss: 0.762737\n",
      "step: 14140, loss: 0.722589 val Loss: 0.706582\n",
      "step: 14150, loss: 0.781995 val Loss: 0.711082\n",
      "step: 14160, loss: 0.786399 val Loss: 0.751244\n",
      "step: 14170, loss: 0.696041 val Loss: 0.794840\n",
      "step: 14180, loss: 0.727911 val Loss: 0.677562\n",
      "step: 14190, loss: 0.715513 val Loss: 0.767301\n",
      "step: 14200, loss: 0.711588 val Loss: 0.776172\n",
      "step: 14210, loss: 0.717169 val Loss: 0.704617\n",
      "step: 14220, loss: 0.769951 val Loss: 0.679403\n",
      "step: 14230, loss: 0.749946 val Loss: 0.794731\n",
      "step: 14240, loss: 0.705266 val Loss: 0.742512\n",
      "step: 14250, loss: 0.709655 val Loss: 0.764234\n",
      "step: 14260, loss: 0.748037 val Loss: 0.735751\n",
      "step: 14270, loss: 0.746142 val Loss: 0.742639\n",
      "step: 14280, loss: 0.754620 val Loss: 0.649239\n",
      "step: 14290, loss: 0.794855 val Loss: 0.750753\n",
      "step: 14300, loss: 0.726407 val Loss: 0.765795\n",
      "step: 14310, loss: 0.790993 val Loss: 0.689430\n",
      "step: 14320, loss: 0.770050 val Loss: 0.751928\n",
      "step: 14330, loss: 0.781537 val Loss: 0.702746\n",
      "step: 14340, loss: 0.763964 val Loss: 0.750689\n",
      "step: 14350, loss: 0.787670 val Loss: 0.741148\n",
      "step: 14360, loss: 0.724043 val Loss: 0.779767\n",
      "step: 14370, loss: 0.757911 val Loss: 0.708872\n",
      "step: 14380, loss: 0.767655 val Loss: 0.782006\n",
      "step: 14390, loss: 0.834002 val Loss: 0.807819\n",
      "step: 14400, loss: 0.732159 val Loss: 0.660136\n",
      "step: 14410, loss: 0.733332 val Loss: 0.799584\n",
      "step: 14420, loss: 0.783831 val Loss: 0.660612\n",
      "step: 14430, loss: 0.754628 val Loss: 0.694592\n",
      "step: 14440, loss: 0.734418 val Loss: 0.750437\n",
      "step: 14450, loss: 0.837942 val Loss: 0.800716\n",
      "step: 14460, loss: 0.809410 val Loss: 0.776086\n",
      "step: 14470, loss: 0.730267 val Loss: 0.764135\n",
      "step: 14480, loss: 0.756738 val Loss: 0.744646\n",
      "step: 14490, loss: 0.803849 val Loss: 0.764161\n",
      "step: 14500, loss: 0.774795 val Loss: 0.756948\n",
      "step: 14510, loss: 0.760462 val Loss: 0.809201\n",
      "step: 14520, loss: 0.657723 val Loss: 0.758214\n",
      "step: 14530, loss: 0.739218 val Loss: 0.785671\n",
      "step: 14540, loss: 0.716671 val Loss: 0.783942\n",
      "step: 14550, loss: 0.738650 val Loss: 0.766845\n",
      "step: 14560, loss: 0.686988 val Loss: 0.776638\n",
      "step: 14570, loss: 0.761299 val Loss: 0.799119\n",
      "step: 14580, loss: 0.752052 val Loss: 0.753045\n",
      "step: 14590, loss: 0.738159 val Loss: 0.735458\n",
      "step: 14600, loss: 0.803226 val Loss: 0.800772\n",
      "step: 14610, loss: 0.766886 val Loss: 0.810036\n",
      "step: 14620, loss: 0.771083 val Loss: 0.750491\n",
      "step: 14630, loss: 0.751812 val Loss: 0.745635\n",
      "step: 14640, loss: 0.738242 val Loss: 0.764876\n",
      "step: 14650, loss: 0.785072 val Loss: 0.675426\n",
      "step: 14660, loss: 0.824296 val Loss: 0.754214\n",
      "step: 14670, loss: 0.726518 val Loss: 0.729269\n",
      "step: 14680, loss: 0.703352 val Loss: 0.684912\n",
      "step: 14690, loss: 0.776686 val Loss: 0.702505\n",
      "step: 14700, loss: 0.742018 val Loss: 0.780491\n",
      "step: 14710, loss: 0.748567 val Loss: 0.798548\n",
      "step: 14720, loss: 0.738920 val Loss: 0.733504\n",
      "step: 14730, loss: 0.742410 val Loss: 0.766279\n",
      "step: 14740, loss: 0.737496 val Loss: 0.744625\n",
      "step: 14750, loss: 0.794609 val Loss: 0.819088\n",
      "step: 14760, loss: 0.763763 val Loss: 0.782981\n",
      "step: 14770, loss: 0.909919 val Loss: 0.758191\n",
      "step: 14780, loss: 0.755304 val Loss: 0.794289\n",
      "step: 14790, loss: 0.716794 val Loss: 0.737005\n",
      "step: 14800, loss: 0.809503 val Loss: 0.786196\n",
      "step: 14810, loss: 0.688851 val Loss: 0.834165\n",
      "step: 14820, loss: 0.780198 val Loss: 0.749527\n",
      "step: 14830, loss: 0.823577 val Loss: 0.792270\n",
      "step: 14840, loss: 0.777221 val Loss: 0.728748\n",
      "step: 14850, loss: 0.808018 val Loss: 0.772393\n",
      "step: 14860, loss: 0.704899 val Loss: 0.739846\n",
      "step: 14870, loss: 0.720704 val Loss: 0.676908\n",
      "step: 14880, loss: 0.791176 val Loss: 0.671487\n",
      "step: 14890, loss: 0.763723 val Loss: 0.782580\n",
      "step: 14900, loss: 0.779230 val Loss: 0.722313\n",
      "step: 14910, loss: 0.800870 val Loss: 0.792880\n",
      "step: 14920, loss: 0.746385 val Loss: 0.804716\n",
      "step: 14930, loss: 0.749720 val Loss: 0.710189\n",
      "step: 14940, loss: 0.835869 val Loss: 0.726644\n",
      "step: 14950, loss: 0.745833 val Loss: 0.770967\n",
      "step: 14960, loss: 0.809611 val Loss: 0.767857\n",
      "step: 14970, loss: 0.821683 val Loss: 0.777611\n",
      "step: 14980, loss: 0.746834 val Loss: 0.795886\n",
      "step: 14990, loss: 0.801832 val Loss: 0.721460\n",
      "step: 15000, loss: 0.717521 val Loss: 0.799161\n",
      "step: 15010, loss: 0.794723 val Loss: 0.673412\n",
      "step: 15020, loss: 0.764714 val Loss: 0.786714\n",
      "step: 15030, loss: 0.731961 val Loss: 0.817880\n",
      "step: 15040, loss: 0.765649 val Loss: 0.760376\n",
      "step: 15050, loss: 0.737382 val Loss: 0.733146\n",
      "step: 15060, loss: 0.792271 val Loss: 0.761982\n",
      "step: 15070, loss: 0.748767 val Loss: 0.676132\n",
      "step: 15080, loss: 0.771669 val Loss: 0.735640\n",
      "step: 15090, loss: 0.757471 val Loss: 0.768972\n",
      "step: 15100, loss: 0.826535 val Loss: 0.792686\n",
      "step: 15110, loss: 0.727384 val Loss: 0.733013\n",
      "step: 15120, loss: 0.726387 val Loss: 0.737516\n",
      "step: 15130, loss: 0.723619 val Loss: 0.725174\n",
      "step: 15140, loss: 0.678353 val Loss: 0.724725\n",
      "step: 15150, loss: 0.744173 val Loss: 0.689444\n",
      "step: 15160, loss: 0.797383 val Loss: 0.686716\n",
      "step: 15170, loss: 0.831076 val Loss: 0.735570\n",
      "step: 15180, loss: 0.723889 val Loss: 0.788964\n",
      "step: 15190, loss: 0.782134 val Loss: 0.742206\n",
      "step: 15200, loss: 0.796582 val Loss: 0.735489\n",
      "step: 15210, loss: 0.787849 val Loss: 0.668304\n",
      "step: 15220, loss: 0.694714 val Loss: 0.773926\n",
      "step: 15230, loss: 0.762023 val Loss: 0.742943\n",
      "step: 15240, loss: 0.783772 val Loss: 0.746806\n",
      "step: 15250, loss: 0.768360 val Loss: 0.786993\n",
      "step: 15260, loss: 0.794121 val Loss: 0.746416\n",
      "step: 15270, loss: 0.733770 val Loss: 0.757161\n",
      "step: 15280, loss: 0.688140 val Loss: 0.706606\n",
      "step: 15290, loss: 0.738553 val Loss: 0.666747\n",
      "step: 15300, loss: 0.800809 val Loss: 0.753153\n",
      "step: 15310, loss: 0.811118 val Loss: 0.748978\n",
      "step: 15320, loss: 0.828737 val Loss: 0.768423\n",
      "step: 15330, loss: 0.801497 val Loss: 0.762482\n",
      "step: 15340, loss: 0.806744 val Loss: 0.708044\n",
      "step: 15350, loss: 0.752755 val Loss: 0.735669\n",
      "step: 15360, loss: 0.719236 val Loss: 0.774679\n",
      "step: 15370, loss: 0.796010 val Loss: 0.734089\n",
      "step: 15380, loss: 0.710890 val Loss: 0.760845\n",
      "step: 15390, loss: 0.691240 val Loss: 0.770591\n",
      "step: 15400, loss: 0.726628 val Loss: 0.778006\n",
      "step: 15410, loss: 0.749738 val Loss: 0.813629\n",
      "step: 15420, loss: 0.741826 val Loss: 0.793645\n",
      "step: 15430, loss: 0.741953 val Loss: 0.725122\n",
      "step: 15440, loss: 0.798563 val Loss: 0.788820\n",
      "step: 15450, loss: 0.718002 val Loss: 0.758882\n",
      "step: 15460, loss: 0.797632 val Loss: 0.728232\n",
      "step: 15470, loss: 0.750863 val Loss: 0.780353\n",
      "step: 15480, loss: 0.786225 val Loss: 0.765635\n",
      "step: 15490, loss: 0.761928 val Loss: 0.782666\n",
      "step: 15500, loss: 0.751839 val Loss: 0.767729\n",
      "step: 15510, loss: 0.774313 val Loss: 0.758332\n",
      "step: 15520, loss: 0.782382 val Loss: 0.711400\n",
      "step: 15530, loss: 0.740270 val Loss: 0.785179\n",
      "step: 15540, loss: 0.765999 val Loss: 0.755884\n",
      "step: 15550, loss: 0.715239 val Loss: 0.783856\n",
      "step: 15560, loss: 0.706397 val Loss: 0.817857\n",
      "step: 15570, loss: 0.809038 val Loss: 0.758889\n",
      "step: 15580, loss: 0.783091 val Loss: 0.793036\n",
      "step: 15590, loss: 0.823313 val Loss: 0.809142\n",
      "step: 15600, loss: 0.692084 val Loss: 0.765315\n",
      "step: 15610, loss: 0.774893 val Loss: 0.707265\n",
      "step: 15620, loss: 0.702759 val Loss: 0.778225\n",
      "step: 15630, loss: 0.813079 val Loss: 0.802124\n",
      "step: 15640, loss: 0.755977 val Loss: 0.695024\n",
      "step: 15650, loss: 0.804156 val Loss: 0.708289\n",
      "step: 15660, loss: 0.668982 val Loss: 0.717475\n",
      "step: 15670, loss: 0.766593 val Loss: 0.671197\n",
      "step: 15680, loss: 0.754855 val Loss: 0.783012\n",
      "step: 15690, loss: 0.750877 val Loss: 0.723551\n",
      "step: 15700, loss: 0.743758 val Loss: 0.748617\n",
      "step: 15710, loss: 0.767818 val Loss: 0.791593\n",
      "step: 15720, loss: 0.682765 val Loss: 0.770100\n",
      "step: 15730, loss: 0.827421 val Loss: 0.754699\n",
      "step: 15740, loss: 0.775571 val Loss: 0.741762\n",
      "step: 15750, loss: 0.763575 val Loss: 0.723441\n",
      "step: 15760, loss: 0.791368 val Loss: 0.798365\n",
      "step: 15770, loss: 0.779214 val Loss: 0.674103\n",
      "step: 15780, loss: 0.788156 val Loss: 0.747049\n",
      "step: 15790, loss: 0.835187 val Loss: 0.753482\n",
      "step: 15800, loss: 0.781055 val Loss: 0.807935\n",
      "step: 15810, loss: 0.807066 val Loss: 0.687932\n",
      "step: 15820, loss: 0.785719 val Loss: 0.669030\n",
      "step: 15830, loss: 0.720267 val Loss: 0.785113\n",
      "step: 15840, loss: 0.844662 val Loss: 0.767253\n",
      "step: 15850, loss: 0.754213 val Loss: 0.779759\n",
      "step: 15860, loss: 0.719417 val Loss: 0.778122\n",
      "step: 15870, loss: 0.706903 val Loss: 0.799256\n",
      "step: 15880, loss: 0.803339 val Loss: 0.757972\n",
      "step: 15890, loss: 0.742936 val Loss: 0.734651\n",
      "step: 15900, loss: 0.689116 val Loss: 0.799783\n",
      "step: 15910, loss: 0.739752 val Loss: 0.746725\n",
      "step: 15920, loss: 0.834034 val Loss: 0.740368\n",
      "step: 15930, loss: 0.786034 val Loss: 0.757936\n",
      "step: 15940, loss: 0.799422 val Loss: 0.746525\n",
      "step: 15950, loss: 0.715633 val Loss: 0.793078\n",
      "step: 15960, loss: 0.847790 val Loss: 0.681134\n",
      "step: 15970, loss: 0.843659 val Loss: 0.770310\n",
      "step: 15980, loss: 0.854603 val Loss: 0.782182\n",
      "step: 15990, loss: 0.807615 val Loss: 0.692636\n",
      "step: 16000, loss: 0.751857 val Loss: 0.795250\n",
      "step: 16010, loss: 0.769852 val Loss: 0.772850\n",
      "step: 16020, loss: 0.802638 val Loss: 0.750740\n",
      "step: 16030, loss: 0.700923 val Loss: 0.772163\n",
      "step: 16040, loss: 0.740828 val Loss: 0.716980\n",
      "step: 16050, loss: 0.770482 val Loss: 0.723033\n",
      "step: 16060, loss: 0.644281 val Loss: 0.706582\n",
      "step: 16070, loss: 0.800187 val Loss: 0.686003\n",
      "step: 16080, loss: 0.716087 val Loss: 0.813610\n",
      "step: 16090, loss: 0.763664 val Loss: 0.736287\n",
      "step: 16100, loss: 0.716434 val Loss: 0.777547\n",
      "step: 16110, loss: 0.739344 val Loss: 0.805389\n",
      "step: 16120, loss: 0.654954 val Loss: 0.725145\n",
      "step: 16130, loss: 0.855865 val Loss: 0.741564\n",
      "step: 16140, loss: 0.808294 val Loss: 0.749200\n",
      "step: 16150, loss: 0.844560 val Loss: 0.795868\n",
      "step: 16160, loss: 0.720121 val Loss: 0.768561\n",
      "step: 16170, loss: 0.811100 val Loss: 0.701841\n",
      "step: 16180, loss: 0.742110 val Loss: 0.782196\n",
      "step: 16190, loss: 0.681201 val Loss: 0.793387\n",
      "step: 16200, loss: 0.754118 val Loss: 0.699204\n",
      "step: 16210, loss: 0.801704 val Loss: 0.810896\n",
      "step: 16220, loss: 0.718598 val Loss: 0.806559\n",
      "step: 16230, loss: 0.787529 val Loss: 0.731634\n",
      "step: 16240, loss: 0.769160 val Loss: 0.683316\n",
      "step: 16250, loss: 0.862812 val Loss: 0.733146\n",
      "step: 16260, loss: 0.741298 val Loss: 0.694603\n",
      "step: 16270, loss: 0.790789 val Loss: 0.798812\n",
      "step: 16280, loss: 0.775427 val Loss: 0.690290\n",
      "step: 16290, loss: 0.723278 val Loss: 0.745508\n",
      "step: 16300, loss: 0.775066 val Loss: 0.801491\n",
      "step: 16310, loss: 0.849355 val Loss: 0.745701\n",
      "step: 16320, loss: 0.760510 val Loss: 0.799059\n",
      "step: 16330, loss: 0.794036 val Loss: 0.794575\n",
      "step: 16340, loss: 0.784547 val Loss: 0.802336\n",
      "step: 16350, loss: 0.749036 val Loss: 0.833902\n",
      "step: 16360, loss: 0.729043 val Loss: 0.721573\n",
      "step: 16370, loss: 0.764013 val Loss: 0.765033\n",
      "step: 16380, loss: 0.796048 val Loss: 0.798445\n",
      "step: 16390, loss: 0.734285 val Loss: 0.725730\n",
      "step: 16400, loss: 0.755157 val Loss: 0.760278\n",
      "step: 16410, loss: 0.715749 val Loss: 0.746216\n",
      "step: 16420, loss: 0.801331 val Loss: 0.803892\n",
      "step: 16430, loss: 0.768356 val Loss: 0.764391\n",
      "step: 16440, loss: 0.715907 val Loss: 0.679491\n",
      "step: 16450, loss: 0.732947 val Loss: 0.738589\n",
      "step: 16460, loss: 0.853900 val Loss: 0.731672\n",
      "step: 16470, loss: 0.785193 val Loss: 0.713458\n",
      "step: 16480, loss: 0.747017 val Loss: 0.699639\n",
      "step: 16490, loss: 0.827595 val Loss: 0.770127\n",
      "step: 16500, loss: 0.727403 val Loss: 0.706830\n",
      "step: 16510, loss: 0.734221 val Loss: 0.748382\n",
      "step: 16520, loss: 0.802714 val Loss: 0.822429\n",
      "step: 16530, loss: 0.725192 val Loss: 0.688026\n",
      "step: 16540, loss: 0.763075 val Loss: 0.742040\n",
      "step: 16550, loss: 0.680047 val Loss: 0.742137\n",
      "step: 16560, loss: 0.704195 val Loss: 0.789301\n",
      "step: 16570, loss: 0.726231 val Loss: 0.778185\n",
      "step: 16580, loss: 0.764500 val Loss: 0.804463\n",
      "step: 16590, loss: 0.791794 val Loss: 0.769781\n",
      "step: 16600, loss: 0.811239 val Loss: 0.753425\n",
      "step: 16610, loss: 0.758721 val Loss: 0.701173\n",
      "step: 16620, loss: 0.726821 val Loss: 0.790714\n",
      "step: 16630, loss: 0.750930 val Loss: 0.731954\n",
      "step: 16640, loss: 0.686867 val Loss: 0.771464\n",
      "step: 16650, loss: 0.797512 val Loss: 0.777472\n",
      "step: 16660, loss: 0.718563 val Loss: 0.736961\n",
      "step: 16670, loss: 0.756298 val Loss: 0.742230\n",
      "step: 16680, loss: 0.725714 val Loss: 0.763021\n",
      "step: 16690, loss: 0.688975 val Loss: 0.742757\n",
      "step: 16700, loss: 0.829011 val Loss: 0.668285\n",
      "step: 16710, loss: 0.758771 val Loss: 0.687948\n",
      "step: 16720, loss: 0.742815 val Loss: 0.670416\n",
      "step: 16730, loss: 0.801727 val Loss: 0.774275\n",
      "step: 16740, loss: 0.753798 val Loss: 0.775294\n",
      "step: 16750, loss: 0.745316 val Loss: 0.766378\n",
      "step: 16760, loss: 0.800027 val Loss: 0.734276\n",
      "step: 16770, loss: 0.739406 val Loss: 0.705651\n",
      "step: 16780, loss: 0.757102 val Loss: 0.763074\n",
      "step: 16790, loss: 0.725356 val Loss: 0.788359\n",
      "step: 16800, loss: 0.772081 val Loss: 0.755788\n",
      "step: 16810, loss: 0.784140 val Loss: 0.755456\n",
      "step: 16820, loss: 0.729035 val Loss: 0.768580\n",
      "step: 16830, loss: 0.737708 val Loss: 0.773673\n",
      "step: 16840, loss: 0.784652 val Loss: 0.796222\n",
      "step: 16850, loss: 0.770644 val Loss: 0.777826\n",
      "step: 16860, loss: 0.692222 val Loss: 0.792226\n",
      "step: 16870, loss: 0.784387 val Loss: 0.820345\n",
      "step: 16880, loss: 0.801129 val Loss: 0.755976\n",
      "step: 16890, loss: 0.866679 val Loss: 0.802772\n",
      "step: 16900, loss: 0.762160 val Loss: 0.803085\n",
      "step: 16910, loss: 0.675232 val Loss: 0.790523\n",
      "step: 16920, loss: 0.780038 val Loss: 0.761931\n",
      "step: 16930, loss: 0.777644 val Loss: 0.748240\n",
      "step: 16940, loss: 0.744283 val Loss: 0.677930\n",
      "step: 16950, loss: 0.728639 val Loss: 0.743659\n",
      "step: 16960, loss: 0.727727 val Loss: 0.764432\n",
      "step: 16970, loss: 0.724634 val Loss: 0.746588\n",
      "step: 16980, loss: 0.807791 val Loss: 0.788810\n",
      "step: 16990, loss: 0.756632 val Loss: 0.765119\n",
      "step: 17000, loss: 0.788714 val Loss: 0.774527\n",
      "step: 17010, loss: 0.725542 val Loss: 0.736778\n",
      "step: 17020, loss: 0.767128 val Loss: 0.810795\n",
      "step: 17030, loss: 0.718572 val Loss: 0.763319\n",
      "step: 17040, loss: 0.785108 val Loss: 0.740679\n",
      "step: 17050, loss: 0.741749 val Loss: 0.674863\n",
      "step: 17060, loss: 0.802360 val Loss: 0.757234\n",
      "step: 17070, loss: 0.647365 val Loss: 0.706397\n",
      "step: 17080, loss: 0.727957 val Loss: 0.679315\n",
      "step: 17090, loss: 0.721203 val Loss: 0.664694\n",
      "step: 17100, loss: 0.789918 val Loss: 0.846505\n",
      "step: 17110, loss: 0.695623 val Loss: 0.757005\n",
      "step: 17120, loss: 0.729135 val Loss: 0.701146\n",
      "step: 17130, loss: 0.795958 val Loss: 0.704021\n",
      "step: 17140, loss: 0.709265 val Loss: 0.724669\n",
      "step: 17150, loss: 0.808165 val Loss: 0.706143\n",
      "step: 17160, loss: 0.757208 val Loss: 0.774479\n",
      "step: 17170, loss: 0.698799 val Loss: 0.830696\n",
      "step: 17180, loss: 0.728172 val Loss: 0.762162\n",
      "step: 17190, loss: 0.733894 val Loss: 0.670641\n",
      "step: 17200, loss: 0.745961 val Loss: 0.728735\n",
      "step: 17210, loss: 0.741662 val Loss: 0.821004\n",
      "step: 17220, loss: 0.728259 val Loss: 0.766503\n",
      "step: 17230, loss: 0.755405 val Loss: 0.679158\n",
      "step: 17240, loss: 0.705449 val Loss: 0.735698\n",
      "step: 17250, loss: 0.744636 val Loss: 0.711231\n",
      "step: 17260, loss: 0.737108 val Loss: 0.667648\n",
      "step: 17270, loss: 0.776633 val Loss: 0.795204\n",
      "step: 17280, loss: 0.836572 val Loss: 0.749431\n",
      "step: 17290, loss: 0.772753 val Loss: 0.759849\n",
      "step: 17300, loss: 0.667751 val Loss: 0.787748\n",
      "step: 17310, loss: 0.768559 val Loss: 0.792852\n",
      "step: 17320, loss: 0.794893 val Loss: 0.689818\n",
      "step: 17330, loss: 0.707248 val Loss: 0.763426\n",
      "step: 17340, loss: 0.665803 val Loss: 0.707015\n",
      "step: 17350, loss: 0.737842 val Loss: 0.740893\n",
      "step: 17360, loss: 0.707863 val Loss: 0.682746\n",
      "step: 17370, loss: 0.773681 val Loss: 0.815523\n",
      "step: 17380, loss: 0.766243 val Loss: 0.780867\n",
      "step: 17390, loss: 0.811615 val Loss: 0.776659\n",
      "step: 17400, loss: 0.772163 val Loss: 0.772884\n",
      "step: 17410, loss: 0.727961 val Loss: 0.751657\n",
      "step: 17420, loss: 0.793337 val Loss: 0.758150\n",
      "step: 17430, loss: 0.758343 val Loss: 0.778471\n",
      "step: 17440, loss: 0.767283 val Loss: 0.678197\n",
      "step: 17450, loss: 0.767872 val Loss: 0.736572\n",
      "step: 17460, loss: 0.751822 val Loss: 0.781142\n",
      "step: 17470, loss: 0.777933 val Loss: 0.746106\n",
      "step: 17480, loss: 0.777187 val Loss: 0.726120\n",
      "step: 17490, loss: 0.864664 val Loss: 0.750601\n",
      "step: 17500, loss: 0.745223 val Loss: 0.795146\n",
      "step: 17510, loss: 0.682895 val Loss: 0.727228\n",
      "step: 17520, loss: 0.713585 val Loss: 0.771478\n",
      "step: 17530, loss: 0.687418 val Loss: 0.774728\n",
      "step: 17540, loss: 0.762177 val Loss: 0.742631\n",
      "step: 17550, loss: 0.747928 val Loss: 0.720872\n",
      "step: 17560, loss: 0.739013 val Loss: 0.721888\n",
      "step: 17570, loss: 0.716202 val Loss: 0.733674\n",
      "step: 17580, loss: 0.753618 val Loss: 0.717200\n",
      "step: 17590, loss: 0.758119 val Loss: 0.686767\n",
      "step: 17600, loss: 0.762223 val Loss: 0.714274\n",
      "step: 17610, loss: 0.783030 val Loss: 0.756395\n",
      "step: 17620, loss: 0.793619 val Loss: 0.726477\n",
      "step: 17630, loss: 0.756250 val Loss: 0.723776\n",
      "step: 17640, loss: 0.727122 val Loss: 0.756159\n",
      "step: 17650, loss: 0.755970 val Loss: 0.679588\n",
      "step: 17660, loss: 0.706618 val Loss: 0.797096\n",
      "step: 17670, loss: 0.684701 val Loss: 0.740957\n",
      "step: 17680, loss: 0.765634 val Loss: 0.733473\n",
      "step: 17690, loss: 0.740952 val Loss: 0.669956\n",
      "step: 17700, loss: 0.832652 val Loss: 0.840334\n",
      "step: 17710, loss: 0.698228 val Loss: 0.727718\n",
      "step: 17720, loss: 0.768137 val Loss: 0.768700\n",
      "step: 17730, loss: 0.733809 val Loss: 0.755276\n",
      "step: 17740, loss: 0.706828 val Loss: 0.808544\n",
      "step: 17750, loss: 0.827287 val Loss: 0.774198\n",
      "step: 17760, loss: 0.820445 val Loss: 0.747758\n",
      "step: 17770, loss: 0.713524 val Loss: 0.660343\n",
      "step: 17780, loss: 0.784770 val Loss: 0.746837\n",
      "step: 17790, loss: 0.806987 val Loss: 0.805682\n",
      "step: 17800, loss: 0.710565 val Loss: 0.771267\n",
      "step: 17810, loss: 0.739504 val Loss: 0.680463\n",
      "step: 17820, loss: 0.813376 val Loss: 0.665810\n",
      "step: 17830, loss: 0.691133 val Loss: 0.788487\n",
      "step: 17840, loss: 0.761092 val Loss: 0.742591\n",
      "step: 17850, loss: 0.762043 val Loss: 0.732007\n",
      "step: 17860, loss: 0.781060 val Loss: 0.727410\n",
      "step: 17870, loss: 0.716959 val Loss: 0.785640\n",
      "step: 17880, loss: 0.697469 val Loss: 0.671435\n",
      "step: 17890, loss: 0.801599 val Loss: 0.797017\n",
      "step: 17900, loss: 0.767017 val Loss: 0.760870\n",
      "step: 17910, loss: 0.786380 val Loss: 0.705433\n",
      "step: 17920, loss: 0.661599 val Loss: 0.766635\n",
      "step: 17930, loss: 0.794070 val Loss: 0.749549\n",
      "step: 17940, loss: 0.714592 val Loss: 0.691608\n",
      "step: 17950, loss: 0.683603 val Loss: 0.750656\n",
      "step: 17960, loss: 0.768320 val Loss: 0.718703\n",
      "step: 17970, loss: 0.785556 val Loss: 0.756874\n",
      "step: 17980, loss: 0.844269 val Loss: 0.722862\n",
      "step: 17990, loss: 0.792118 val Loss: 0.683438\n",
      "step: 18000, loss: 0.798908 val Loss: 0.739688\n",
      "step: 18010, loss: 0.765329 val Loss: 0.741620\n",
      "step: 18020, loss: 0.713740 val Loss: 0.673551\n",
      "step: 18030, loss: 0.720619 val Loss: 0.753904\n",
      "step: 18040, loss: 0.759016 val Loss: 0.715718\n",
      "step: 18050, loss: 0.735640 val Loss: 0.762376\n",
      "step: 18060, loss: 0.753759 val Loss: 0.764718\n",
      "step: 18070, loss: 0.674838 val Loss: 0.816717\n",
      "step: 18080, loss: 0.822177 val Loss: 0.672217\n",
      "step: 18090, loss: 0.778543 val Loss: 0.728025\n",
      "step: 18100, loss: 0.749490 val Loss: 0.738288\n",
      "step: 18110, loss: 0.726391 val Loss: 0.747768\n",
      "step: 18120, loss: 0.791546 val Loss: 0.706379\n",
      "step: 18130, loss: 0.814330 val Loss: 0.748751\n",
      "step: 18140, loss: 0.714310 val Loss: 0.751370\n",
      "step: 18150, loss: 0.796392 val Loss: 0.686780\n",
      "step: 18160, loss: 0.710312 val Loss: 0.768797\n",
      "step: 18170, loss: 0.682842 val Loss: 0.672005\n",
      "step: 18180, loss: 0.815122 val Loss: 0.708129\n",
      "step: 18190, loss: 0.803291 val Loss: 0.796698\n",
      "step: 18200, loss: 0.780940 val Loss: 0.726026\n",
      "step: 18210, loss: 0.791727 val Loss: 0.787122\n",
      "step: 18220, loss: 0.744233 val Loss: 0.760538\n",
      "step: 18230, loss: 0.700535 val Loss: 0.846178\n",
      "step: 18240, loss: 0.696333 val Loss: 0.780085\n",
      "step: 18250, loss: 0.664726 val Loss: 0.767058\n",
      "step: 18260, loss: 0.807351 val Loss: 0.740244\n",
      "step: 18270, loss: 0.804225 val Loss: 0.690180\n",
      "step: 18280, loss: 0.700002 val Loss: 0.661991\n",
      "step: 18290, loss: 0.760688 val Loss: 0.753487\n",
      "step: 18300, loss: 0.822960 val Loss: 0.823259\n",
      "step: 18310, loss: 0.709317 val Loss: 0.802002\n",
      "step: 18320, loss: 0.749210 val Loss: 0.722104\n",
      "step: 18330, loss: 0.700643 val Loss: 0.702264\n",
      "step: 18340, loss: 0.785728 val Loss: 0.714611\n",
      "step: 18350, loss: 0.777314 val Loss: 0.760796\n",
      "step: 18360, loss: 0.760534 val Loss: 0.818881\n",
      "step: 18370, loss: 0.745047 val Loss: 0.757463\n",
      "step: 18380, loss: 0.796594 val Loss: 0.750992\n",
      "step: 18390, loss: 0.786466 val Loss: 0.777059\n",
      "step: 18400, loss: 0.751920 val Loss: 0.699237\n",
      "step: 18410, loss: 0.696448 val Loss: 0.705407\n",
      "step: 18420, loss: 0.699764 val Loss: 0.751413\n",
      "step: 18430, loss: 0.771905 val Loss: 0.683457\n",
      "step: 18440, loss: 0.733030 val Loss: 0.677735\n",
      "step: 18450, loss: 0.754686 val Loss: 0.811519\n",
      "step: 18460, loss: 0.863652 val Loss: 0.726108\n",
      "step: 18470, loss: 0.846445 val Loss: 0.752352\n",
      "step: 18480, loss: 0.767012 val Loss: 0.733955\n",
      "step: 18490, loss: 0.796342 val Loss: 0.763524\n",
      "step: 18500, loss: 0.760929 val Loss: 0.759248\n",
      "step: 18510, loss: 0.778020 val Loss: 0.773840\n",
      "step: 18520, loss: 0.733682 val Loss: 0.673732\n",
      "step: 18530, loss: 0.727454 val Loss: 0.747627\n",
      "step: 18540, loss: 0.720383 val Loss: 0.774879\n",
      "step: 18550, loss: 0.764478 val Loss: 0.740376\n",
      "step: 18560, loss: 0.704827 val Loss: 0.748141\n",
      "step: 18570, loss: 0.737003 val Loss: 0.672106\n",
      "step: 18580, loss: 0.713864 val Loss: 0.816139\n",
      "step: 18590, loss: 0.756949 val Loss: 0.715116\n",
      "step: 18600, loss: 0.784867 val Loss: 0.749420\n",
      "step: 18610, loss: 0.855397 val Loss: 0.764180\n",
      "step: 18620, loss: 0.734002 val Loss: 0.664157\n",
      "step: 18630, loss: 0.783464 val Loss: 0.764477\n",
      "step: 18640, loss: 0.810029 val Loss: 0.773971\n",
      "step: 18650, loss: 0.724960 val Loss: 0.694214\n",
      "step: 18660, loss: 0.764982 val Loss: 0.671447\n",
      "step: 18670, loss: 0.754320 val Loss: 0.804570\n",
      "step: 18680, loss: 0.716200 val Loss: 0.746978\n",
      "step: 18690, loss: 0.770157 val Loss: 0.798599\n",
      "step: 18700, loss: 0.783735 val Loss: 0.799493\n",
      "step: 18710, loss: 0.721567 val Loss: 0.772839\n",
      "step: 18720, loss: 0.689897 val Loss: 0.780982\n",
      "step: 18730, loss: 0.699005 val Loss: 0.825326\n",
      "step: 18740, loss: 0.770079 val Loss: 0.687140\n",
      "step: 18750, loss: 0.815671 val Loss: 0.677004\n",
      "step: 18760, loss: 0.809252 val Loss: 0.681424\n",
      "step: 18770, loss: 0.736531 val Loss: 0.761253\n",
      "step: 18780, loss: 0.792218 val Loss: 0.821887\n",
      "step: 18790, loss: 0.828912 val Loss: 0.750270\n",
      "step: 18800, loss: 0.772001 val Loss: 0.666505\n",
      "step: 18810, loss: 0.762610 val Loss: 0.744441\n",
      "step: 18820, loss: 0.847359 val Loss: 0.740836\n",
      "step: 18830, loss: 0.755212 val Loss: 0.756638\n",
      "step: 18840, loss: 0.752057 val Loss: 0.815656\n",
      "step: 18850, loss: 0.753597 val Loss: 0.850195\n",
      "step: 18860, loss: 0.777411 val Loss: 0.703885\n",
      "step: 18870, loss: 0.775793 val Loss: 0.732742\n",
      "step: 18880, loss: 0.756585 val Loss: 0.757971\n",
      "step: 18890, loss: 0.810826 val Loss: 0.691109\n",
      "step: 18900, loss: 0.674790 val Loss: 0.816705\n",
      "step: 18910, loss: 0.749200 val Loss: 0.736359\n",
      "step: 18920, loss: 0.748128 val Loss: 0.749756\n",
      "step: 18930, loss: 0.787865 val Loss: 0.785260\n",
      "step: 18940, loss: 0.731262 val Loss: 0.791646\n",
      "step: 18950, loss: 0.784614 val Loss: 0.793151\n",
      "step: 18960, loss: 0.753146 val Loss: 0.793100\n",
      "step: 18970, loss: 0.737829 val Loss: 0.733384\n",
      "step: 18980, loss: 0.765565 val Loss: 0.730092\n",
      "step: 18990, loss: 0.747091 val Loss: 0.741465\n",
      "step: 19000, loss: 0.748294 val Loss: 0.679790\n",
      "step: 19010, loss: 0.764665 val Loss: 0.765130\n",
      "step: 19020, loss: 0.776045 val Loss: 0.800323\n",
      "step: 19030, loss: 0.743099 val Loss: 0.736080\n",
      "step: 19040, loss: 0.724707 val Loss: 0.696319\n",
      "step: 19050, loss: 0.707849 val Loss: 0.780471\n",
      "step: 19060, loss: 0.776367 val Loss: 0.644559\n",
      "step: 19070, loss: 0.715383 val Loss: 0.754790\n",
      "step: 19080, loss: 0.755009 val Loss: 0.787056\n",
      "step: 19090, loss: 0.756273 val Loss: 0.788381\n",
      "step: 19100, loss: 0.811733 val Loss: 0.785729\n",
      "step: 19110, loss: 0.799703 val Loss: 0.757015\n",
      "step: 19120, loss: 0.704852 val Loss: 0.741763\n",
      "step: 19130, loss: 0.782699 val Loss: 0.764220\n",
      "step: 19140, loss: 0.849476 val Loss: 0.724637\n",
      "step: 19150, loss: 0.811902 val Loss: 0.754684\n",
      "step: 19160, loss: 0.832858 val Loss: 0.792805\n",
      "step: 19170, loss: 0.782973 val Loss: 0.776311\n",
      "step: 19180, loss: 0.760988 val Loss: 0.789583\n",
      "step: 19190, loss: 0.778353 val Loss: 0.688070\n",
      "step: 19200, loss: 0.801000 val Loss: 0.786299\n",
      "step: 19210, loss: 0.778864 val Loss: 0.730482\n",
      "step: 19220, loss: 0.718745 val Loss: 0.753473\n",
      "step: 19230, loss: 0.735950 val Loss: 0.814831\n",
      "step: 19240, loss: 0.792583 val Loss: 0.703931\n",
      "step: 19250, loss: 0.839382 val Loss: 0.737339\n",
      "step: 19260, loss: 0.802378 val Loss: 0.680218\n",
      "step: 19270, loss: 0.745264 val Loss: 0.758638\n",
      "step: 19280, loss: 0.793092 val Loss: 0.736891\n",
      "step: 19290, loss: 0.712678 val Loss: 0.778590\n",
      "step: 19300, loss: 0.721423 val Loss: 0.755784\n",
      "step: 19310, loss: 0.735329 val Loss: 0.752905\n",
      "step: 19320, loss: 0.761149 val Loss: 0.750422\n",
      "step: 19330, loss: 0.738834 val Loss: 0.764926\n",
      "step: 19340, loss: 0.722156 val Loss: 0.742688\n",
      "step: 19350, loss: 0.784294 val Loss: 0.789581\n",
      "step: 19360, loss: 0.777232 val Loss: 0.733560\n",
      "step: 19370, loss: 0.760091 val Loss: 0.755377\n",
      "step: 19380, loss: 0.752341 val Loss: 0.747253\n",
      "step: 19390, loss: 0.872612 val Loss: 0.737153\n",
      "step: 19400, loss: 0.742707 val Loss: 0.708025\n",
      "step: 19410, loss: 0.770018 val Loss: 0.793282\n",
      "step: 19420, loss: 0.773889 val Loss: 0.719975\n",
      "step: 19430, loss: 0.821497 val Loss: 0.769541\n",
      "step: 19440, loss: 0.777116 val Loss: 0.732984\n",
      "step: 19450, loss: 0.730752 val Loss: 0.673179\n",
      "step: 19460, loss: 0.752636 val Loss: 0.787370\n",
      "step: 19470, loss: 0.744250 val Loss: 0.739445\n",
      "step: 19480, loss: 0.816828 val Loss: 0.796814\n",
      "step: 19490, loss: 0.812572 val Loss: 0.736852\n",
      "step: 19500, loss: 0.709106 val Loss: 0.754639\n",
      "step: 19510, loss: 0.804676 val Loss: 0.804510\n",
      "step: 19520, loss: 0.671474 val Loss: 0.761151\n",
      "step: 19530, loss: 0.817679 val Loss: 0.759077\n",
      "step: 19540, loss: 0.709361 val Loss: 0.730907\n",
      "step: 19550, loss: 0.756647 val Loss: 0.717757\n",
      "step: 19560, loss: 0.792740 val Loss: 0.824123\n",
      "step: 19570, loss: 0.705430 val Loss: 0.734751\n",
      "step: 19580, loss: 0.798957 val Loss: 0.803655\n",
      "step: 19590, loss: 0.779051 val Loss: 0.811498\n",
      "step: 19600, loss: 0.889761 val Loss: 0.801097\n",
      "step: 19610, loss: 0.840433 val Loss: 0.793022\n",
      "step: 19620, loss: 0.683519 val Loss: 0.695675\n",
      "step: 19630, loss: 0.773440 val Loss: 0.727528\n",
      "step: 19640, loss: 0.758812 val Loss: 0.720468\n",
      "step: 19650, loss: 0.838986 val Loss: 0.743792\n",
      "step: 19660, loss: 0.739002 val Loss: 0.741402\n",
      "step: 19670, loss: 0.692047 val Loss: 0.784811\n",
      "step: 19680, loss: 0.689396 val Loss: 0.784563\n",
      "step: 19690, loss: 0.737809 val Loss: 0.710785\n",
      "step: 19700, loss: 0.742102 val Loss: 0.798449\n",
      "step: 19710, loss: 0.781192 val Loss: 0.772592\n",
      "step: 19720, loss: 0.708199 val Loss: 0.800600\n",
      "step: 19730, loss: 0.740930 val Loss: 0.767040\n",
      "step: 19740, loss: 0.700633 val Loss: 0.705033\n",
      "step: 19750, loss: 0.713971 val Loss: 0.764407\n",
      "step: 19760, loss: 0.805872 val Loss: 0.768416\n",
      "step: 19770, loss: 0.772979 val Loss: 0.694578\n",
      "step: 19780, loss: 0.750770 val Loss: 0.659604\n",
      "step: 19790, loss: 0.790345 val Loss: 0.791426\n",
      "step: 19800, loss: 0.767929 val Loss: 0.792269\n",
      "step: 19810, loss: 0.864554 val Loss: 0.808601\n",
      "step: 19820, loss: 0.812601 val Loss: 0.795874\n",
      "step: 19830, loss: 0.722143 val Loss: 0.751888\n",
      "step: 19840, loss: 0.682322 val Loss: 0.742207\n",
      "step: 19850, loss: 0.756043 val Loss: 0.750957\n",
      "step: 19860, loss: 0.776002 val Loss: 0.660835\n",
      "step: 19870, loss: 0.741520 val Loss: 0.742767\n",
      "step: 19880, loss: 0.704607 val Loss: 0.734284\n",
      "step: 19890, loss: 0.773059 val Loss: 0.799954\n",
      "step: 19900, loss: 0.714955 val Loss: 0.729522\n",
      "step: 19910, loss: 0.723077 val Loss: 0.809962\n",
      "step: 19920, loss: 0.683826 val Loss: 0.775684\n",
      "step: 19930, loss: 0.771046 val Loss: 0.746134\n",
      "step: 19940, loss: 0.771401 val Loss: 0.673077\n",
      "step: 19950, loss: 0.832985 val Loss: 0.780922\n",
      "step: 19960, loss: 0.794186 val Loss: 0.803449\n",
      "step: 19970, loss: 0.749845 val Loss: 0.753964\n",
      "step: 19980, loss: 0.735233 val Loss: 0.707609\n",
      "step: 19990, loss: 0.695135 val Loss: 0.747109\n",
      "step: 20000, loss: 0.717867 val Loss: 0.797768\n"
     ]
    }
   ],
   "source": [
    "from net_work_def import  MtlNetwork_head, MtlNetwork_body\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "with tf.device('gpu:0'):\n",
    "    n1, n2 = train_personalized_network(1, 20000, frames_used = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1_data, sub1_ppg = sd.get_sub1_data(1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelhmorton/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mtl_body =  MtlNetwork_body()\n",
    "head1 =  MtlNetwork_head(num_classes)\n",
    "head2 = MtlNetwork_head(num_classes)\n",
    "\n",
    "neural_net1 =  tf.keras.Sequential([mtl_body, head1])\n",
    "neural_net1.load_weights('personalized_weights/subject_1/model_weights')\n",
    "\n",
    "i = 0\n",
    "tX = np.reshape(sub1_data[i: i + 40,:,:], [40,100,100])\n",
    "tX = np.array(tX, dtype= np.float64)\n",
    "tX = np.moveaxis(tX, 0,-1) # very important line in axis changeing \n",
    "\n",
    "p_point = np.int(np.round(i*64/30))\n",
    "gt = sub1_ppg[p_point: p_point+85, 0]\n",
    "gt = (gt-gt.min())/(gt.max()-gt.min())\n",
    "\n",
    "tX1 = np.reshape(tX, [-1, 100,100,40])\n",
    "plt.plot(gt*2-1)\n",
    "tX1 = (tX1 - tX1.min())/(tX1.max() - tX1.min())\n",
    "predd = neural_net1(tX1) \n",
    "plt.plot(predd[0])\n",
    "\n",
    "plt.legend([\"Ground Truth\", \"Predicted\"])\n",
    "plt.xlabel('time sample \\n (60 samples = 1 second)', fontsize =12)\n",
    "plt.ylabel('magnitude \\n (Normalized voltage)', fontsize = 12)\n",
    "from matplotlib import rcParams\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['lines.color'] = 'r'\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

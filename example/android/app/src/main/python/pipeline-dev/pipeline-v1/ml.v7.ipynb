{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.signal import resample\n",
    "from sdtw import SoftDTW\n",
    "from sdtw.distance import SquaredEuclidean\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [18, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from truth import IeeeGroundTruth\n",
    "from wavelet import apply_wavelet\n",
    "from peaks import get_peaks_v2\n",
    "from signal_pross import (\n",
    "    normalize_signal,\n",
    "    detrend_w_poly,\n",
    "    normalize_amplitude_to_1,\n",
    "    n_moving_avg,\n",
    "    min_max_scale,\n",
    "    bandpass,\n",
    "    get_hr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFactory:\n",
    "\n",
    "    def __init__(self, split_size, loss_type = 'mse', gamma = 1.0, mse_weight = None, dtw_weight = None):\n",
    "        \n",
    "        if loss_type not in ['mse', 'dtw', 'combined']:\n",
    "            raise ValueError(f'Loss type [{loss_type}] not supported')\n",
    "        \n",
    "        self.split_size = split_size\n",
    "        self.gamma = gamma\n",
    "        self.mse_weight = mse_weight\n",
    "        self.dtw_weight = dtw_weight\n",
    "\n",
    "        if loss_type == 'mse':\n",
    "            self.loss_function = self.mse_loss\n",
    "        elif loss_type == 'dtw':\n",
    "            self.loss_function = self.soft_dtw_loss\n",
    "        elif loss_type == 'combined':\n",
    "            self.loss_function = self.combined_loss\n",
    "        \n",
    "    def __call__(self, y_pred, data):\n",
    "        return self.loss_function(y_pred, data)\n",
    "\n",
    "    def mse_loss(self, y_pred, data):\n",
    "        \n",
    "        y_true = data.get_label()\n",
    "        num_batches = int(len(y_pred) / self.split_size)\n",
    "        errs = np.zeros_like(y_true)\n",
    "\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            y_true_curr = y_true[i * self.split_size: (i + 1) * self.split_size]\n",
    "            y_pred_curr = y_pred[i * self.split_size: (i + 1) * self.split_size]\n",
    "            \n",
    "            err = y_true_curr - y_pred_curr\n",
    "            errs[i * self.split_size: (i + 1) * self.split_size] = err\n",
    "\n",
    "        grad = -2 * errs\n",
    "        hess = 2 * np.ones_like(y_true)\n",
    "        return grad, hess\n",
    "\n",
    "    # def soft_dtw_loss(self, y_pred, data):\n",
    "\n",
    "    #     y_true = data.get_label()\n",
    "    #     num_batches = int(len(y_pred) / self.split_size)\n",
    "    #     grads = np.zeros_like(y_true)\n",
    "    #     hesses = np.zeros_like(y_true)\n",
    "\n",
    "    #     for i in range(num_batches):\n",
    "\n",
    "    #         y_true_curr = y_true[i * self.split_size: (i + 1) * self.split_size]\n",
    "    #         y_pred_curr = y_pred[i * self.split_size: (i + 1) * self.split_size]\n",
    "            \n",
    "    #         grad_curr, hess_curr = self.soft_dtw_loss_helper(y_true_curr, y_pred_curr)\n",
    "    #         grad_curr = grad_curr.flatten()\n",
    "    #         hess_curr = hess_curr.flatten()\n",
    "\n",
    "    #         grads[i * self.split_size: (i + 1) * self.split_size] = grad_curr\n",
    "    #         hesses[i * self.split_size: (i + 1) * self.split_size] = hess_curr\n",
    "\n",
    "    #     return grads, hesses\n",
    "    def soft_dtw_loss(self, y_pred, data):\n",
    "\n",
    "        def batch_loss_helper(i, y_true, y_pred, split_size):\n",
    "            \n",
    "            y_true_curr = y_true[i * split_size: (i + 1) * split_size]\n",
    "            y_pred_curr = y_pred[i * split_size: (i + 1) * split_size]\n",
    "\n",
    "            grad_curr, hess_curr = self.soft_dtw_loss_helper(y_true_curr, y_pred_curr)\n",
    "            grad_curr = grad_curr.flatten()\n",
    "            hess_curr = hess_curr.flatten()\n",
    "\n",
    "            return grad_curr, hess_curr\n",
    "\n",
    "        y_true = data.get_label()\n",
    "        num_batches = int(len(y_pred) / self.split_size)\n",
    "        grads = np.zeros_like(y_true)\n",
    "        hesses = np.zeros_like(y_true)\n",
    "\n",
    "        results = Parallel(n_jobs = -1)(\n",
    "            delayed(batch_loss_helper)(\n",
    "                i, y_true, y_pred, self.split_size\n",
    "            ) for i in range(num_batches)\n",
    "        )\n",
    "\n",
    "        for i, (grad_curr, hess_curr) in enumerate(results):\n",
    "            grads[i * self.split_size: (i + 1) * self.split_size] = grad_curr\n",
    "            hesses[i * self.split_size: (i + 1) * self.split_size] = hess_curr\n",
    "\n",
    "        return grads, hesses\n",
    "    \n",
    "    def soft_dtw_loss_helper(self, y_true, y_pred):\n",
    "        x = y_true.reshape(-1, 1)\n",
    "        y = y_pred.reshape(-1, 1)\n",
    "        D = SquaredEuclidean(x, y)\n",
    "        sdtw = SoftDTW(D, gamma = self.gamma)\n",
    "        sdtw.compute()\n",
    "        E = sdtw.grad()\n",
    "        G = D.jacobian_product(E)\n",
    "        return G, np.ones(len(G))\n",
    "    \n",
    "    def combined_loss(self, y_pred, data):\n",
    "\n",
    "        if self.mse_weight is None or self.dtw_weight is None:\n",
    "            raise ValueError('mse_weight and dtw_weight must be set before calling combined_loss')\n",
    "\n",
    "        mse_grads, mse_hesses = self.mse_loss(y_pred, data)\n",
    "        dtw_grads, dtw_hesses = self.soft_dtw_loss(y_pred, data)\n",
    "\n",
    "        combined_grad = self.mse_weight * mse_grads + self.dtw_weight * dtw_grads\n",
    "        combined_hess = self.mse_weight * mse_hesses + self.dtw_weight * dtw_hesses\n",
    "\n",
    "        return combined_grad, combined_hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LonePineGBM:\n",
    "    \n",
    "    def __init__(self, truths, label_col = 'bvp', subject_col = 'subject',\n",
    "                # model customization\n",
    "                model_type = 'gbdt', random_state = None, loss_type = 'mse', excluded_subject = None,\n",
    "                # hyperparameters\n",
    "                n_estimators = 100, split_size = 1280, learning_rate = 0.1, test_size = 0.3, early_stopping_rounds = 50,\n",
    "                mse_weight = None, dtw_weight = None, data_beg = 1000, data_end = 10000, batches = 1, finetune = True,\n",
    "                min_bandpass_freq = 0.67, max_bandpass_freq = 3.0, bandpass_order = 4,\n",
    "                predicted_peaks_prominence = 0.22, true_peaks_prominence = 0.15,\n",
    "                # hyperparams from LightGBM docs\n",
    "                max_depth = 7, num_leaves = 75, max_bin = 255,\n",
    "                num_feats_per_channel = 3, skip_amount = 15):\n",
    "        \n",
    "        if model_type not in ['gbdt', 'rf']:\n",
    "            raise ValueError(f'Model type [{model_type}] not supported')\n",
    "        \n",
    "        self.label_col = label_col\n",
    "        self.subject_col = subject_col\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.random_state = random_state\n",
    "        self.excluded_subject = excluded_subject\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.split_size = split_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.test_size = test_size\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self.data_beg = data_beg\n",
    "        self.data_end = data_end\n",
    "        self.finetune = finetune\n",
    "\n",
    "        self.min_bandpass_freq = min_bandpass_freq\n",
    "        self.max_bandpass_freq = max_bandpass_freq\n",
    "        self.bandpass_order = bandpass_order\n",
    "        self.predicted_peaks_prominence = predicted_peaks_prominence\n",
    "        self.true_peaks_prominence = true_peaks_prominence\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.num_leaves = num_leaves\n",
    "        self.max_bin = max_bin\n",
    "\n",
    "        self.num_feats_per_channel = num_feats_per_channel\n",
    "        self.skip_amount = skip_amount\n",
    "\n",
    "        self.gbm = None\n",
    "        self.training_loss = None\n",
    "        self.test_loss = None\n",
    "\n",
    "        self.given_data = self.prepare_dataset_from_subjects(truths, data_beg = data_beg, data_end = data_end)\n",
    "        self.features = list(self.given_data.drop(columns = [self.label_col, self.subject_col]).columns)\n",
    "        if self.excluded_subject is not None:\n",
    "            self.given_data = self.given_data[self.given_data[self.subject_col] != self.excluded_subject]\n",
    "\n",
    "        if self.random_state is not None:\n",
    "            random.seed(self.random_state)\n",
    "        splits = self.split_data()\n",
    "        self.train_split_indices = random.sample(range(len(splits)), int(len(splits) * (1 - self.test_size)))\n",
    "        self.train_splits = [splits[i] for i in self.train_split_indices]\n",
    "        self.test_splits = [splits[i] for i in range(len(splits)) if i not in self.train_split_indices]\n",
    "        \n",
    "        self.train_data = []\n",
    "        batch_size = len(self.train_splits) // batches\n",
    "        print(f'Rows per batch: {batch_size * self.split_size}')\n",
    "        for batch_num in range(batches):\n",
    "            batch_split_idxs = random.sample(range(len(self.train_splits)), batch_size)\n",
    "            batch_splits = [self.train_splits[i] for i in batch_split_idxs]\n",
    "            self.train_splits = [self.train_splits[i] for i in range(len(self.train_splits)) if i not in batch_split_idxs]\n",
    "\n",
    "            train_indices = [idx for split in batch_splits for idx in split]\n",
    "            training_rows = self.given_data.iloc[train_indices].drop(columns = [self.subject_col])\n",
    "            train_X = training_rows.drop(columns = [self.label_col]).to_numpy()\n",
    "            train_y = training_rows[self.label_col].to_numpy()\n",
    "\n",
    "            batch_data = lgb.Dataset(train_X, train_y, free_raw_data = False)\n",
    "            self.train_data.append(batch_data)\n",
    "\n",
    "        test_indices = [idx for split in self.test_splits for idx in split]\n",
    "        test_rows = self.given_data.iloc[test_indices].drop(columns = [self.subject_col])\n",
    "        test_X = test_rows.drop(columns = [self.label_col]).to_numpy()\n",
    "        test_y = test_rows[self.label_col].to_numpy()\n",
    "        self.test_data = lgb.Dataset(test_X, test_y, free_raw_data = False)\n",
    "\n",
    "        self.loss = LossFactory(self.split_size, loss_type = loss_type, mse_weight = mse_weight, dtw_weight = dtw_weight)\n",
    "    \n",
    "    def split_data(self, to_exclude = None):\n",
    "        \n",
    "        data_in_use = self.given_data if to_exclude is None else self.given_data[~self.given_data[self.subject_col].isin(to_exclude)]\n",
    "\n",
    "        subject_indices = data_in_use.groupby(self.subject_col).indices\n",
    "        splits = []\n",
    "        for _, indices in subject_indices.items():\n",
    "            \n",
    "            n_splits = len(indices) // self.split_size\n",
    "            if n_splits > 0:\n",
    "\n",
    "                subject_splits = []\n",
    "                for i in range(n_splits):\n",
    "                    split_start = i * self.split_size\n",
    "                    split_end = (i + 1) * self.split_size\n",
    "                    subject_split = indices[split_start: split_end]\n",
    "                    subject_splits.append(subject_split)\n",
    "                \n",
    "                splits.extend(subject_splits)\n",
    "        \n",
    "        return splits\n",
    "\n",
    "    def fit(self):\n",
    "        t1 = datetime.today()\n",
    "        \n",
    "        params = {\n",
    "            'metric': 'None',\n",
    "            'verbosity': -1,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'objective': 'regression',\n",
    "            'boosting': self.model_type,\n",
    "            'max_depth': self.max_depth,\n",
    "            'num_leaves': self.num_leaves,\n",
    "            'max_bin': self.max_bin,\n",
    "        }\n",
    "    \n",
    "        if self.model_type == 'rf':\n",
    "            params['bagging_freq'] = 1\n",
    "            params['bagging_fraction'] = 0.8\n",
    "\n",
    "\n",
    "        training_loss_key = 'hr_err'\n",
    "        feval = self.hr_error_eval_metric\n",
    "        \n",
    "        training_meta = {}\n",
    "\n",
    "        for train_data in self.train_data:\n",
    "            \n",
    "            if self.model_type == 'gbdt':\n",
    "                self.gbm = lgb.train(\n",
    "                    params,\n",
    "                    train_data,\n",
    "                    valid_sets = [train_data, self.test_data],\n",
    "                    valid_names=['train', 'test'],\n",
    "                    fobj = self.loss,\n",
    "                    num_boost_round = self.n_estimators,\n",
    "                    feval=feval,\n",
    "                    callbacks=[\n",
    "                        early_stopping(stopping_rounds = self.early_stopping_rounds),\n",
    "                        log_evaluation(period=5)\n",
    "                    ],\n",
    "                    evals_result = training_meta,\n",
    "                    init_model = self.gbm\n",
    "                )\n",
    "            else:\n",
    "                self.gbm = lgb.train(\n",
    "                    params,\n",
    "                    train_data,\n",
    "                    valid_sets = [train_data, self.test_data],\n",
    "                    valid_names=['train', 'test'],\n",
    "                    num_boost_round = self.n_estimators,\n",
    "                    feval=feval,\n",
    "                    callbacks=[\n",
    "                        early_stopping(stopping_rounds = self.early_stopping_rounds),\n",
    "                        log_evaluation(period=5)\n",
    "                    ],\n",
    "                    evals_result = training_meta,\n",
    "                )\n",
    "\n",
    "            mse, hr_err, hr_err_sq = self.eval()\n",
    "            print(f'Before fine-tuning: MSE = {mse}, HR error = {hr_err}, HR error (squared) = {hr_err_sq}')\n",
    "\n",
    "            if self.model_type == 'gbdt' and self.finetune:\n",
    "                \n",
    "                print('\\n\\nFine-tuning...')\n",
    "                gbm_copy = copy.deepcopy(self.gbm)\n",
    "                pred = gbm_copy.predict(train_data.get_data())\n",
    "                \n",
    "                # new_targ = train_data.get_label() - pred\n",
    "                new_targ = np.ones(len(pred))\n",
    "                nsplits = len(pred) // self.split_size\n",
    "                labels = train_data.get_label()\n",
    "                for i in range(nsplits):\n",
    "                    pred_curr = pred[i * self.split_size: (i + 1) * self.split_size]\n",
    "                    label_curr = labels[i * self.split_size: (i + 1) * self.split_size]\n",
    "                    hr_err = self.get_hr_error(pred_curr, label_curr, square = True)\n",
    "                    new_targ[i * self.split_size: (i + 1) * self.split_size] = hr_err\n",
    "                \n",
    "                new_train_data = lgb.Dataset(train_data.get_data(), label = new_targ)\n",
    "\n",
    "                self.gbm = lgb.train(\n",
    "                    params,\n",
    "                    new_train_data,\n",
    "                    valid_sets = [new_train_data, self.test_data],\n",
    "                    valid_names=['train', 'test'],\n",
    "                    fobj = self.loss,\n",
    "                    num_boost_round = self.n_estimators // 2,\n",
    "                    feval=feval,\n",
    "                    callbacks=[\n",
    "                        early_stopping(stopping_rounds = self.early_stopping_rounds // 2),\n",
    "                        log_evaluation(period=5)\n",
    "                    ],\n",
    "                    evals_result = training_meta,\n",
    "                    init_model = gbm_copy\n",
    "                )\n",
    "\n",
    "            \n",
    "\n",
    "        self.training_loss = training_meta['train'][training_loss_key]\n",
    "        self.test_loss = training_meta['test'][training_loss_key]\n",
    "        print(f'Finished training in {datetime.today() - t1}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.gbm.predict(X)\n",
    "\n",
    "    def eval(self):\n",
    "        \n",
    "        test_X = self.test_data.get_data()\n",
    "        test_y = self.test_data.get_label()\n",
    "        nsplits = int(len(test_X) / self.split_size)\n",
    "        errs = []\n",
    "        mses = np.zeros(len(test_X))\n",
    "        \n",
    "        for i in range(nsplits):\n",
    "\n",
    "            curr_pred = self.predict(test_X[i * self.split_size: (i + 1) * self.split_size, :])\n",
    "            curr_true = test_y[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true, curr_pred = self.process_signal(curr_true, curr_pred, smoothing_window = 5, use_bandpass = True)\n",
    "            \n",
    "            mses[i * self.split_size: (i + 1) * self.split_size] = curr_true - curr_pred\n",
    "            hr_err = self.get_hr_error(curr_true, curr_pred, square = False)\n",
    "            errs.append(hr_err)\n",
    "        \n",
    "        return np.mean(np.square(mses)), np.mean(errs), np.mean(np.square(errs))\n",
    "\n",
    "    def plot_loss(self):\n",
    "        if self.training_loss is not None and self.test_loss is not None:\n",
    "            training_loss_normed = min_max_scale(self.training_loss)\n",
    "            test_loss_normed = min_max_scale(self.test_loss)\n",
    "            plt.plot(training_loss_normed, label = 'training loss')\n",
    "            plt.plot(test_loss_normed, label = 'test loss')\n",
    "            plt.legend()\n",
    "        \n",
    "    def get_model_stats(self):\n",
    "\n",
    "        model_info = self.gbm.dump_model()\n",
    "        tree_depths = []\n",
    "\n",
    "        for tree_info in model_info['tree_info']:\n",
    "            tree_structure = tree_info['tree_structure']\n",
    "            \n",
    "            # Recursive function to compute the depth of a tree\n",
    "            def calculate_depth(node, current_depth=0):\n",
    "                if 'leaf_value' in node:\n",
    "                    return current_depth\n",
    "                else:\n",
    "                    left_depth = calculate_depth(node['left_child'], current_depth + 1)\n",
    "                    right_depth = calculate_depth(node['right_child'], current_depth + 1)\n",
    "                    return max(left_depth, right_depth)\n",
    "\n",
    "            tree_depth = calculate_depth(tree_structure)\n",
    "            tree_depths.append(tree_depth)\n",
    "        \n",
    "\n",
    "        print(f'Best test loss: {min(self.test_loss)}\\n')\n",
    "        print('Tree depth stats:')\n",
    "        print('Min tree depth:', min(tree_depths))\n",
    "        print('Max tree depth:', max(tree_depths))\n",
    "        print('Avg tree depth:', np.mean(tree_depths))\n",
    "        print('\\nFeature importances:')\n",
    "        display(self.get_feature_importances())\n",
    "    \n",
    "    def get_feature_importances(self):\n",
    "        importances = self.gbm.feature_importance(importance_type='gain')\n",
    "        feature_importances = pd.DataFrame({'feature': self.features, 'importance': importances})\n",
    "        feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "        return feature_importances\n",
    "    \n",
    "    def hr_error_eval_metric(self, y_pred, eval_data):\n",
    "        y_true = eval_data.get_label()\n",
    "        nsplits = int(len(y_pred) / self.split_size)\n",
    "        hr_err = []\n",
    "        for i in range(nsplits):\n",
    "            curr_pred = y_pred[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true = y_true[i * self.split_size: (i + 1) * self.split_size]\n",
    "            curr_true, curr_pred = self.process_signal(curr_true, curr_pred, smoothing_window = 10, use_bandpass = True)\n",
    "            hr_err.append(self.get_hr_error(curr_true, curr_pred, square = False))\n",
    "        return 'hr_err', np.mean(hr_err), False\n",
    "    \n",
    "    def get_hr_error(self, y_true, y_pred, square = True):\n",
    "\n",
    "        true_peaks, _ = self.get_true_peaks(y_true)\n",
    "        pred_peaks, _ = self.get_predicted_peaks(y_pred)\n",
    "\n",
    "        if len(true_peaks) >= 2:\n",
    "            true_ibis = np.diff(true_peaks) / 64\n",
    "            true_hr = 60 / np.mean(true_ibis)\n",
    "        else:\n",
    "            true_hr = 0\n",
    "\n",
    "        if len(pred_peaks) >= 2:\n",
    "            pred_ibis = np.diff(pred_peaks) / 64\n",
    "            pred_hr = 60 / np.mean(pred_ibis)\n",
    "        else:\n",
    "            pred_hr = 0\n",
    "        \n",
    "        if square:\n",
    "            return np.power(true_hr - pred_hr, 2)\n",
    "        return abs(true_hr - pred_hr)\n",
    "    \n",
    "    def process_signal(self, y_true, y_pred, smoothing_window = 10, use_bandpass = False):\n",
    "    \n",
    "        orig_len = len(y_pred)\n",
    "        y_pred = n_moving_avg(y_pred, smoothing_window)\n",
    "        y_pred = resample(y_pred, orig_len)\n",
    "        if use_bandpass:\n",
    "            y_pred = bandpass(y_pred, 64, [self.min_bandpass_freq, self.max_bandpass_freq], self.bandpass_order)\n",
    "        y_pred = min_max_scale(y_pred)\n",
    "        \n",
    "        y_true = n_moving_avg(y_true, 20)\n",
    "        y_true = resample(y_true, orig_len)\n",
    "        if use_bandpass:\n",
    "            y_true = bandpass(y_true, 64, [self.min_bandpass_freq, self.max_bandpass_freq], self.bandpass_order)\n",
    "        y_true = min_max_scale(y_true)\n",
    "        \n",
    "        return y_true, y_pred\n",
    "    \n",
    "    def get_predicted_peaks(self, signal):\n",
    "        return get_peaks_v2(signal, 64, 3.0, -1, prominence = self.predicted_peaks_prominence, with_min_dist = True, with_valleys = False)\n",
    "    def get_true_peaks(self, signal):\n",
    "        return get_peaks_v2(signal, 64, 3.0, -1, prominence = self.true_peaks_prominence, with_min_dist = True, with_valleys = False)\n",
    "\n",
    "    def prepare_dataset_from_subjects(self, truths, data_beg = 1000, data_end = 2000):\n",
    "        data_arr = []\n",
    "        for i in range(len(truths)):    \n",
    "            truth = truths[i]\n",
    "            data = truth.prepare_data_for_ml(self.num_feats_per_channel, self.skip_amount)\n",
    "            data = data.iloc[data_beg: data_end, :]\n",
    "            data['subject'] = i + 1\n",
    "            data_arr.append(data)\n",
    "        return pd.concat(data_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectwise_kfold(truths, model_type = 'gbdt', random_state = None, loss_type = 'mse',\n",
    "                    n_estimators = 100, split_size = 1280, learning_rate = 0.1, test_size = 0.3, early_stopping_rounds = 50,\n",
    "                    mse_weight = None, dtw_weight = None, data_beg = 1000, data_end = 10000, batches = 1, finetune = True, \n",
    "                    min_bandpass_freq = 0.67, max_bandpass_freq = 3.0, bandpass_order = 4,\n",
    "                    predicted_peaks_prominence = 0.22, true_peaks_prominence = 0.15,\n",
    "                    max_depth = 7, num_leaves = 75, max_bin = 255, num_feats_per_channel = 3, skip_amount = 15,\n",
    "                    rounds_per_model = 1):\n",
    "        \n",
    "        models = {}\n",
    "        for subj_idx in range(len(truths)):\n",
    "            models[subj_idx + 1] = []\n",
    "            for i in range(rounds_per_model):\n",
    "                \n",
    "                print(f'\\n\\nTraining excluding subject {subj_idx + 1}...\\n')\n",
    "                mod = LonePineGBM(\n",
    "                    truths = truths,\n",
    "                    model_type = model_type,\n",
    "                    random_state = random_state,\n",
    "                    loss_type = loss_type,\n",
    "                    n_estimators = n_estimators,\n",
    "                    split_size = split_size,\n",
    "                    learning_rate = learning_rate,\n",
    "                    test_size = test_size,\n",
    "                    early_stopping_rounds = early_stopping_rounds,\n",
    "                    mse_weight = mse_weight,\n",
    "                    dtw_weight = dtw_weight,\n",
    "                    data_beg = data_beg,\n",
    "                    data_end = data_end,\n",
    "                    batches = batches,\n",
    "                    finetune = finetune,\n",
    "                    min_bandpass_freq = min_bandpass_freq,\n",
    "                    max_bandpass_freq = max_bandpass_freq,\n",
    "                    bandpass_order = bandpass_order,\n",
    "                    predicted_peaks_prominence = predicted_peaks_prominence,\n",
    "                    true_peaks_prominence = true_peaks_prominence,\n",
    "                    max_depth = max_depth,\n",
    "                    num_leaves = num_leaves,\n",
    "                    max_bin = max_bin,\n",
    "                    num_feats_per_channel = num_feats_per_channel,\n",
    "                    skip_amount = skip_amount,\n",
    "                    excluded_subject = subj_idx + 1\n",
    "                )\n",
    "                mod.fit()\n",
    "                models[subj_idx + 1].append(mod)\n",
    "        \n",
    "        model_performances = {}\n",
    "        for subj_idx in models:\n",
    "            model_performances[subj_idx] = []\n",
    "            for i in range(rounds_per_model):\n",
    "                mod = models[subj_idx][i]\n",
    "                model_performances[subj_idx].append(mod.eval())\n",
    "        \n",
    "        mean_hr_score = np.mean([model_performances[subj_idx][i][1] for subj_idx in model_performances for i in range(rounds_per_model)])\n",
    "        return mean_hr_score, models, model_performances\n",
    "    \n",
    "\n",
    "class LonePineOptimizer:\n",
    "\n",
    "    def __init__(self, truths):\n",
    "        self.truths = truths\n",
    "    \n",
    "    def objective(self, n_estimators = 100, split_size = 1280, learning_rate = 0.1, test_size = 0.3, early_stopping_rounds = 50,\n",
    "                    mse_weight = None, dtw_weight = None, data_beg = 1000, data_end = 2000, batches = 1, finetune = True, \n",
    "                    min_bandpass_freq = 0.67, max_bandpass_freq = 3.0, bandpass_order = 4,\n",
    "                    predicted_peaks_prominence = 0.22, true_peaks_prominence = 0.15,\n",
    "                    max_depth = 7, num_leaves = 75, max_bin = 255, num_feats_per_channel = 3, skip_amount = 15):\n",
    "\n",
    "        hr_score, _, _ = subjectwise_kfold(\n",
    "            self.truths,\n",
    "            model_type = 'gbdt',\n",
    "            random_state = None,\n",
    "            loss_type = 'combined',\n",
    "            n_estimators = n_estimators,\n",
    "            split_size = split_size,\n",
    "            learning_rate = learning_rate,\n",
    "            early_stopping_rounds = 50,\n",
    "            mse_weight = mse_weight,\n",
    "            dtw_weight = dtw_weight,\n",
    "            data_beg = data_beg,\n",
    "            data_end = data_end,\n",
    "            batches = batches,\n",
    "            finetune = finetune,\n",
    "            min_bandpass_freq = min_bandpass_freq,\n",
    "            max_bandpass_freq = max_bandpass_freq,\n",
    "            bandpass_order = bandpass_order,\n",
    "            predicted_peaks_prominence = predicted_peaks_prominence,\n",
    "            true_peaks_prominence = true_peaks_prominence,\n",
    "            max_depth = max_depth,\n",
    "            num_leaves = num_leaves,\n",
    "            max_bin = max_bin,\n",
    "            num_feats_per_channel = num_feats_per_channel,\n",
    "            skip_amount = skip_amount,\n",
    "        )\n",
    "        return hr_score\n",
    "    \n",
    "    def optimize(self, n_calls = 50):\n",
    "\n",
    "        space = [\n",
    "            Integer(50, 300, name = \"n_estimators\"),\n",
    "            Integer(640, 1280, name = \"split_size\"),\n",
    "            Real(0.002, 0.5, name = \"learning_rate\"),\n",
    "            Integer(10, 100, name = \"early_stopping_rounds\"),\n",
    "            Real(0.0, 1.0, name = \"mse_weight\"),\n",
    "            Real(0.0, 1.0, name = \"dtw_weight\"),\n",
    "            Integer(1000, 4000, name = \"data_beg\"),\n",
    "            Integer(6000, 10000, name = \"data_end\"),\n",
    "            Integer(1, 8, name = \"batches\"),\n",
    "            Real(0.4, 1.0, name = \"min_bandpass_freq\"),\n",
    "            Real(2.5, 4.0, name = \"max_bandpass_freq\"),\n",
    "            Integer(2, 6, name = \"bandpass_order\"),\n",
    "            Real(0.1, 0.75, name = \"predicted_peaks_prominence\"),\n",
    "            Real(0.1, 0.5, name = \"true_peaks_prominence\"),\n",
    "            Integer(3, 10, name = \"max_depth\"),\n",
    "            Integer(30, 140, name = \"num_leaves\"),\n",
    "            Integer(100, 300, name = \"max_bin\"),\n",
    "            Integer(3, 10, name = \"num_feats_per_channel\"),\n",
    "            Integer(5, 25, name = \"skip_amount\"),\n",
    "        ]\n",
    "\n",
    "        @use_named_args(space)\n",
    "        def wrapped_objective(**params):\n",
    "            return self.objective(**params)\n",
    "        \n",
    "        result = gp_minimize(\n",
    "            wrapped_objective, space, n_calls=n_calls, random_state=42, verbose=1\n",
    "        )\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = LonePineOptimizer(truths)\n",
    "# result = optimizer.optimize(n_calls = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space = [\n",
    "#             Integer(50, 300, name = \"n_estimators\"),\n",
    "#             Integer(640, 1280, name = \"split_size\"),\n",
    "#             Real(0.002, 0.5, name = \"learning_rate\"),\n",
    "#             Integer(10, 100, name = \"early_stopping_rounds\"),\n",
    "#             Real(0.0, 1.0, name = \"mse_weight\"),\n",
    "#             Real(0.0, 1.0, name = \"dtw_weight\"),\n",
    "#             Integer(1000, 4000, name = \"data_beg\"),\n",
    "#             Integer(6000, 10000, name = \"data_end\"),\n",
    "#             Integer(1, 8, name = \"batches\"),\n",
    "#             Real(0.4, 1.0, name = \"min_bandpass_freq\"),\n",
    "#             Real(2.5, 4.0, name = \"max_bandpass_freq\"),\n",
    "#             Integer(2, 6, name = \"bandpass_order\"),\n",
    "#             Real(0.1, 0.75, name = \"predicted_peaks_prominence\"),\n",
    "#             Real(0.1, 0.5, name = \"true_peaks_prominence\"),\n",
    "#             Integer(3, 10, name = \"max_depth\"),\n",
    "#             Integer(30, 140, name = \"num_leaves\"),\n",
    "#             Integer(100, 300, name = \"max_bin\"),\n",
    "#             Integer(3, 10, name = \"num_feats_per_channel\"),\n",
    "#             Integer(5, 25, name = \"skip_amount\"),\n",
    "#         ]\n",
    "\n",
    "optimization_res = [188,\n",
    " 993,\n",
    " 0.22844583483861589,\n",
    " 16,\n",
    " 0.913467044583398,\n",
    " 1.0,\n",
    " 3909,\n",
    " 7474,\n",
    " 5,\n",
    " 0.9421772128909808,\n",
    " 3.6351090994830813,\n",
    " 4,\n",
    " 0.17087564911262462,\n",
    " 0.322741927274642,\n",
    " 6,\n",
    " 34,\n",
    " 235,\n",
    " 8,\n",
    " 12]\n",
    "\n",
    "# result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean([model_performances[sub][i][1] for sub in model_performances for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\n",
    "    {\n",
    "        'minmax': False,\n",
    "        'use_wavelet': True,\n",
    "        'use_bandpass': False\n",
    "    },\n",
    "    {\n",
    "        'minmax': False,\n",
    "        'use_wavelet': False,\n",
    "        'use_bandpass': True\n",
    "    },\n",
    "    {\n",
    "        'minmax': False,\n",
    "        'use_wavelet': True,\n",
    "        'use_bandpass': True\n",
    "    },\n",
    "]\n",
    "rpm = 10\n",
    "\n",
    "scores = []\n",
    "\n",
    "for option in options:\n",
    "    \n",
    "    truths = []\n",
    "    for subject in range(1, 8):\n",
    "\n",
    "        truth = IeeeGroundTruth(subject, 1, directory = 'channel_data3')\n",
    "        truth.align_rgb_bvp()\n",
    "        truth.fill_nans()\n",
    "        truth.process_rgb(\n",
    "            minmax = option['minmax'],\n",
    "            use_wavelet = option['use_wavelet'],\n",
    "            use_bandpass = option['use_bandpass']\n",
    "        )\n",
    "        truth.process_bvp()\n",
    "        truths.append(truth)\n",
    "\n",
    "\n",
    "    hr_score, models, model_performances = subjectwise_kfold(\n",
    "        truths, model_type = 'gbdt', random_state = None, loss_type = 'combined', rounds_per_model = rpm,\n",
    "        n_estimators = 188, split_size = 960, learning_rate = 0.01,\n",
    "        early_stopping_rounds = 16, mse_weight = 0.1, dtw_weight = 0.9, data_beg = 4000, data_end = 7500,\n",
    "        batches = 5, min_bandpass_freq = 0.9421772128909808, max_bandpass_freq = 3.6351090994830813, bandpass_order = 4,\n",
    "        predicted_peaks_prominence = 0.17087564911262462, true_peaks_prominence = 0.322741927274642, max_depth = 6,\n",
    "        num_leaves = 34, max_bin = 235, num_feats_per_channel = 8, skip_amount = 12\n",
    "    )\n",
    "\n",
    "    scores.append(hr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\n\n",
      "Score: 17.581146877558954\n",
      "Score: 24.72481695669135\n",
      "Score: 23.133821941921045\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n\\\\n')\n",
    "for s in scores:\n",
    "    print(f'Score: {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_subject = 7\n",
    "# test_subject_truth = truths[test_subject - 1]\n",
    "\n",
    "# mod = LonePineGBM(\n",
    "#     truths, n_estimators = 188, loss_type = 'combined', split_size = 960, learning_rate = 0.05,#learning_rate = 0.22844583483861589,\n",
    "#     early_stopping_rounds = 16, mse_weight = 0.1, dtw_weight = 0.9, data_beg = 4000, data_end = 7500,\n",
    "#     batches = 5, min_bandpass_freq = 0.9421772128909808, max_bandpass_freq = 3.6351090994830813, bandpass_order = 4,\n",
    "#     predicted_peaks_prominence = 0.17087564911262462, true_peaks_prominence = 0.322741927274642, max_depth = 6,\n",
    "#     num_leaves = 34, max_bin = 235, num_feats_per_channel = 8, skip_amount = 12,\n",
    "#     excluded_subject = test_subject, model_type = 'gbdt', finetune = True\n",
    "# )\n",
    "# mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, hr_err, hr_err_sq = mod.eval()\n",
    "\n",
    "print(f'\\n\\nMSE: {mse}')\n",
    "print(f'HR error: {hr_err}')\n",
    "print(f'HR error squared: {hr_err_sq}\\n\\n')\n",
    "\n",
    "mod.get_model_stats()\n",
    "mod.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg = 2000\n",
    "end = beg + 640\n",
    "\n",
    "model = mod\n",
    "\n",
    "data = truths[2].prepare_data_for_ml(8, 12)\n",
    "x = data.drop(columns = ['bvp']).to_numpy()\n",
    "y = data['bvp'].to_numpy()\n",
    "\n",
    "def measure_code_block():\n",
    "    targ = y[beg: end]\n",
    "    pred = model.predict(x)\n",
    "    pred = pred[beg: end]\n",
    "    targ, pred = mod.process_signal(targ, pred, use_bandpass=True)\n",
    "    return targ, pred\n",
    "\n",
    "%memit targ, pred = measure_code_block()\n",
    "\n",
    "pred_peaks, _ = mod.get_predicted_peaks(pred)\n",
    "true_peaks, _ = mod.get_true_peaks(targ)\n",
    "\n",
    "plt.plot(targ)\n",
    "plt.plot(pred)\n",
    "plt.scatter(pred_peaks, pred[pred_peaks], c='r')\n",
    "plt.scatter(true_peaks, targ[true_peaks], c='g')\n",
    "\n",
    "pred_ibis = np.diff(pred_peaks) / 64\n",
    "true_ibis = np.diff(true_peaks) / 64\n",
    "pred_hr = get_hr(pred_ibis)\n",
    "true_hr = get_hr(true_ibis)\n",
    "print(f'True HR: {true_hr}; Pred HR: {pred_hr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
